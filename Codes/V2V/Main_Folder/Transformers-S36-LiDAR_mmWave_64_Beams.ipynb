{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a50fa-e309-468d-8f83-89fb579e7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 36 - 64 Beams - LiDAR! Transformers!\n",
    "\n",
    "# Average Top-1 accuracy 0.18911290322580646\n",
    "# Average Top-3 accuracy 0.29758064516129035\n",
    "# Average Top-5 accuracy 0.3588709677419355\n",
    "# Average Top-7 accuracy 0.42016129032258065\n",
    "# Average Top-9 accuracy 0.4600806451612903\n",
    "# Average Top-11 accuracy 0.5016129032258064\n",
    "# Average Top-13 accuracy 0.5423387096774194\n",
    "# Average Top-15 accuracy 0.5810483870967742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f7c21-c010-4efb-b277-f9fbe5867096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plyfile plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d700d29-4334-4095-bec3-124514ad1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from torchsummary import summary\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plyfile import PlyData, PlyElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa35ae-a6cb-4401-ba14-611a6a39f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'scenario36/unit1/lidar1/lidar_frame_11-46-32.474528.csv')\n",
    "\n",
    "# Extract the columns of interest\n",
    "x = df[' X (mm)'] / 1000  # Convert to meters\n",
    "y = df[' Y (mm)'] / 1000\n",
    "z = df[' Z (mm)'] / 1000\n",
    "reflectivity = df[' REFLECTIVITY']\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the point cloud data with color based on reflectivity\n",
    "sc = ax.scatter(x, y, z, c=reflectivity, cmap='viridis', s=1)\n",
    "\n",
    "# Add labels and a colorbar\n",
    "ax.set_xlabel('X (m)')\n",
    "ax.set_ylabel('Y (m)')\n",
    "ax.set_zlabel('Z (m)')\n",
    "cbar = fig.colorbar(sc, ax=ax)\n",
    "cbar.set_label('Reflectivity')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17c1351-9295-4d00-8f59-6fb972dfd65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-08-2025\n",
      "22_36\n",
      "C:\\Users\\Baqer\\Desktop\\V2X_CNN_All\\Scenario36_64-Beams\\Main_Folder//saved_folder//01-08-2025_22_36\n"
     ]
    }
   ],
   "source": [
    "# Save directory\n",
    "# year month day\n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds\n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)\n",
    "\n",
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime\n",
    "print(pwd)\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)\n",
    "\n",
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "    isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3275dbe5-8bb6-48f5-8fa9-ed19725bd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Feeding: Create data sample list\n",
    "def create_samples(root, shuffle=False, nat_sort=False):\n",
    "    f = pd.read_csv(root)\n",
    "    data_samples = []\n",
    "    pred_val = []\n",
    "    for idx, row in f.iterrows():\n",
    "        img_paths = row.values[1:3]\n",
    "        data_samples.append(img_paths)\n",
    "    return data_samples\n",
    "\n",
    "class LidarDataFeed(Dataset):\n",
    "    '''\n",
    "    A class retrieving a tuple of (image,label). It can handle the case\n",
    "    of empty classes (empty folders).\n",
    "    '''\n",
    "    def __init__(self,root_dir, num_points=15000, nat_sort = False, init_shuflle = True):\n",
    "        self.num_points = num_points\n",
    "        self.root = root_dir\n",
    "        self.samples = create_samples(self.root,shuffle=init_shuflle,nat_sort=nat_sort)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.samples )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        # print(sample)\n",
    "        df = pd.read_csv(sample[0])\n",
    "        # Extract the columns of interest\n",
    "        x = df[' X (mm)'] / 1000  # Convert to meters\n",
    "        y = df[' Y (mm)'] / 1000\n",
    "        z = df[' Z (mm)'] / 1000\n",
    "        points = np.column_stack((x.values, y.values, z.values))\n",
    "\n",
    "\n",
    "        if points.shape[0] < self.num_points:\n",
    "            points = np.pad(points, ((0, self.num_points - points.shape[0]), (0, 0)), mode='constant')\n",
    "        elif points.shape[0] > self.num_points:\n",
    "            indices = np.random.choice(points.shape[0], self.num_points, replace=False)\n",
    "            points = points[indices]\n",
    "\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.float32)\n",
    "        label = sample[1]\n",
    "        return (points.T, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0ed49d-0148-4754-8bb9-4d1c30dc0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0481165-aee7-4a2b-b0d0-3b9a6877fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "class InputLayer(nn.Module):\n",
    "    \"\"\"Input Layer to accept input point cloud data.\"\"\"\n",
    "    def __init__(self, k=3):\n",
    "        super(InputLayer, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure the input shape is (batch_size, num_points, channels)\n",
    "        return x.transpose(1, 2)  # Change shape to (batch_size, channels, num_points)\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"Embedding Layer for dimensionality reduction.\"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be in shape (batch_size, channels, num_points)\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, channels, num_points)\n",
    "        return F.relu(self.conv(x))  # Applies a 1x1 convolution\n",
    "\n",
    "\n",
    "class EncoderStage(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.3):\n",
    "        super(EncoderStage, self).__init__()\n",
    "        self.cpe = nn.Conv1d(in_channels, out_channels, kernel_size=1)  # CPE\n",
    "        self.norm = nn.BatchNorm1d(out_channels)  # Normalization\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=out_channels, num_heads=8)  # Attention\n",
    "        self.dropout_att = nn.Dropout(dropout_rate)  # Dropout after attention\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels, out_channels),  # MLP layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout after first MLP layer\n",
    "            nn.Linear(out_channels, out_channels)   # Another MLP layer\n",
    "        )\n",
    "        self.dropout_mlp = nn.Dropout(dropout_rate)  # Dropout after second MLP layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply CPE\n",
    "        identity = x  # Save the input for residual connection\n",
    "        x = F.relu(self.cpe(x))\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Prepare for Attention mechanism\n",
    "        x_att = x.transpose(2, 1)  # Shape change for attention\n",
    "        x_att, _ = self.attention(x_att, x_att, x_att)  # Attention output\n",
    "        x_att = self.dropout_att(x_att)  # Apply dropout\n",
    "\n",
    "        x_att = x_att.transpose(2, 1)  # Transpose back\n",
    "        x = x + x_att  # Skip connection\n",
    "\n",
    "        # MLP\n",
    "        b, c, p = x.size()\n",
    "        x = x.view(b * p, c)  # Flatten for MLP\n",
    "        x = self.mlp(x)  # Apply MLP\n",
    "        x = self.dropout_mlp(x)  # Apply dropout after MLP\n",
    "\n",
    "        # Reshape back to (B, out_channels, num_points)\n",
    "        output_channels = x.size(1)\n",
    "        x = x.view(b, output_channels, p)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SerializedPoolingLayer(nn.Module):\n",
    "    \"\"\"Serialized Pooling Layer for dimensionality reduction.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SerializedPoolingLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool1d(x, kernel_size=2)  # Average pooling\n",
    "\n",
    "\n",
    "class PointTransformerV3(nn.Module):\n",
    "    \"\"\"Main Point Transformer V3 for Classification.\"\"\"\n",
    "    def __init__(self, num_classes= num_classes, num_points=15000, enc_depths=[64, 128, 256], dropout_rate=0.3):\n",
    "        super(PointTransformerV3, self).__init__()\n",
    "\n",
    "        # Input and embedding\n",
    "        self.input_layer = InputLayer()\n",
    "        self.embedding = EmbeddingLayer(input_channels=3, output_channels=enc_depths[0])  # Initial embedding layer\n",
    "\n",
    "        # Encoder stages\n",
    "        self.encoders = nn.ModuleList()\n",
    "        in_channels = enc_depths[0]\n",
    "        for out_channels in enc_depths:\n",
    "            self.encoders.append(EncoderStage(in_channels, out_channels, dropout_rate=dropout_rate))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Serialized pooling\n",
    "        self.serialized_pooling = SerializedPoolingLayer()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(enc_depths[-1], 128)  # Input size from enc_depths[-1]\n",
    "        self.fc2 = nn.Linear(128, 64)  # Intermediate layer\n",
    "        self.fc3 = nn.Linear(64, num_classes)  # Output layer for 65 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pass through encoder stages\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "\n",
    "        # Serialized pooling\n",
    "        x = self.serialized_pooling(x)\n",
    "\n",
    "        # Global feature extraction\n",
    "        x = torch.max(x, dim=2)[0]  # Global max pooling across points\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7e3888a-ab70-4932-a631-ff61a4ed8bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─InputLayer: 1-1                        [-1, 15000, 3]            --\n",
      "├─EmbeddingLayer: 1-2                    [-1, 64, 15000]           --\n",
      "|    └─Conv1d: 2-1                       [-1, 64, 15000]           256\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─EncoderStage: 2-2                 [-1, 64, 15000]           --\n",
      "|    |    └─Conv1d: 3-1                  [-1, 64, 15000]           4,160\n",
      "|    |    └─BatchNorm1d: 3-2             [-1, 64, 15000]           128\n",
      "|    |    └─MultiheadAttention: 3-3      [-1, 15000, 64]           16,640\n",
      "|    |    └─Dropout: 3-4                 [-1, 15000, 64]           --\n",
      "|    |    └─Sequential: 3-5              [-1, 64]                  8,320\n",
      "|    |    └─Dropout: 3-6                 [-1, 64]                  --\n",
      "|    └─EncoderStage: 2-3                 [-1, 128, 15000]          --\n",
      "|    |    └─Conv1d: 3-7                  [-1, 128, 15000]          8,320\n",
      "|    |    └─BatchNorm1d: 3-8             [-1, 128, 15000]          256\n",
      "|    |    └─MultiheadAttention: 3-9      [-1, 15000, 128]          66,048\n",
      "|    |    └─Dropout: 3-10                [-1, 15000, 128]          --\n",
      "|    |    └─Sequential: 3-11             [-1, 128]                 33,024\n",
      "|    |    └─Dropout: 3-12                [-1, 128]                 --\n",
      "|    └─EncoderStage: 2-4                 [-1, 256, 15000]          --\n",
      "|    |    └─Conv1d: 3-13                 [-1, 256, 15000]          33,024\n",
      "|    |    └─BatchNorm1d: 3-14            [-1, 256, 15000]          512\n",
      "|    |    └─MultiheadAttention: 3-15     [-1, 15000, 256]          263,168\n",
      "|    |    └─Dropout: 3-16                [-1, 15000, 256]          --\n",
      "|    |    └─Sequential: 3-17             [-1, 256]                 131,584\n",
      "|    |    └─Dropout: 3-18                [-1, 256]                 --\n",
      "├─SerializedPoolingLayer: 1-3            [-1, 256, 7500]           --\n",
      "├─Linear: 1-4                            [-1, 128]                 32,896\n",
      "├─Linear: 1-5                            [-1, 64]                  8,256\n",
      "├─Linear: 1-6                            [-1, 65]                  4,225\n",
      "==========================================================================================\n",
      "Total params: 610,817\n",
      "Trainable params: 610,817\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 680.02\n",
      "==========================================================================================\n",
      "Input size (MB): 0.17\n",
      "Forward/backward pass size (MB): 109.87\n",
      "Params size (MB): 2.33\n",
      "Estimated Total Size (MB): 112.37\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─InputLayer: 1-1                        [-1, 15000, 3]            --\n",
       "├─EmbeddingLayer: 1-2                    [-1, 64, 15000]           --\n",
       "|    └─Conv1d: 2-1                       [-1, 64, 15000]           256\n",
       "├─ModuleList: 1                          []                        --\n",
       "|    └─EncoderStage: 2-2                 [-1, 64, 15000]           --\n",
       "|    |    └─Conv1d: 3-1                  [-1, 64, 15000]           4,160\n",
       "|    |    └─BatchNorm1d: 3-2             [-1, 64, 15000]           128\n",
       "|    |    └─MultiheadAttention: 3-3      [-1, 15000, 64]           16,640\n",
       "|    |    └─Dropout: 3-4                 [-1, 15000, 64]           --\n",
       "|    |    └─Sequential: 3-5              [-1, 64]                  8,320\n",
       "|    |    └─Dropout: 3-6                 [-1, 64]                  --\n",
       "|    └─EncoderStage: 2-3                 [-1, 128, 15000]          --\n",
       "|    |    └─Conv1d: 3-7                  [-1, 128, 15000]          8,320\n",
       "|    |    └─BatchNorm1d: 3-8             [-1, 128, 15000]          256\n",
       "|    |    └─MultiheadAttention: 3-9      [-1, 15000, 128]          66,048\n",
       "|    |    └─Dropout: 3-10                [-1, 15000, 128]          --\n",
       "|    |    └─Sequential: 3-11             [-1, 128]                 33,024\n",
       "|    |    └─Dropout: 3-12                [-1, 128]                 --\n",
       "|    └─EncoderStage: 2-4                 [-1, 256, 15000]          --\n",
       "|    |    └─Conv1d: 3-13                 [-1, 256, 15000]          33,024\n",
       "|    |    └─BatchNorm1d: 3-14            [-1, 256, 15000]          512\n",
       "|    |    └─MultiheadAttention: 3-15     [-1, 15000, 256]          263,168\n",
       "|    |    └─Dropout: 3-16                [-1, 15000, 256]          --\n",
       "|    |    └─Sequential: 3-17             [-1, 256]                 131,584\n",
       "|    |    └─Dropout: 3-18                [-1, 256]                 --\n",
       "├─SerializedPoolingLayer: 1-3            [-1, 256, 7500]           --\n",
       "├─Linear: 1-4                            [-1, 128]                 32,896\n",
       "├─Linear: 1-5                            [-1, 64]                  8,256\n",
       "├─Linear: 1-6                            [-1, 65]                  4,225\n",
       "==========================================================================================\n",
       "Total params: 610,817\n",
       "Trainable params: 610,817\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 680.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.17\n",
       "Forward/backward pass size (MB): 109.87\n",
       "Params size (MB): 2.33\n",
       "Estimated Total Size (MB): 112.37\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes=65\n",
    "# Create an instance of the model\n",
    "model = PointTransformerV3(num_classes)\n",
    "# Move model to the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "summary(model, (3, 15000))  # Example input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee07e05-355c-4e2c-be6e-f041ace2fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "val_batch_size = 1\n",
    "lr = 1e-2\n",
    "decay = 1e-3\n",
    "num_epochs = 20 # After 1st epochs, the accuracies remain same!\n",
    "train_size = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08482b01-779c-452e-86e7-51c4ad39d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'scenario36_64_lidar_beam_train.csv'\n",
    "val_dir = 'scenario36_64_lidar_beam_val.csv'\n",
    "\n",
    "train_loader = DataLoader(LidarDataFeed(train_dir),\n",
    "                          batch_size=batch_size,\n",
    "                          #num_workers=8,\n",
    "                          shuffle=False)\n",
    "val_loader = DataLoader(LidarDataFeed(val_dir),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43019207-379b-46bc-9ccc-f68b4b5f3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```````````````````````````````````````````````````````\n",
      "Training size is 1\n",
      "Epoch No. 1\n",
      "Training-Batch No.10\n",
      "Loss = 3.9942381381988525\n",
      "Training-Batch No.20\n",
      "Loss = 3.2030186653137207\n",
      "Training-Batch No.30\n",
      "Loss = 4.01035737991333\n",
      "Training-Batch No.40\n",
      "Loss = 4.403517246246338\n",
      "Training-Batch No.50\n",
      "Loss = 3.208278179168701\n",
      "Training-Batch No.60\n",
      "Loss = 4.405714511871338\n",
      "Training-Batch No.70\n",
      "Loss = 4.078480243682861\n",
      "Training-Batch No.80\n",
      "Loss = 3.9374423027038574\n",
      "Training-Batch No.90\n",
      "Loss = 4.047043800354004\n",
      "Training-Batch No.100\n",
      "Loss = 3.8013501167297363\n",
      "Training-Batch No.110\n",
      "Loss = 3.9299325942993164\n",
      "Training-Batch No.120\n",
      "Loss = 3.915048360824585\n",
      "Training-Batch No.130\n",
      "Loss = 3.828789710998535\n",
      "Training-Batch No.140\n",
      "Loss = 3.8577306270599365\n",
      "Training-Batch No.150\n",
      "Loss = 4.033524036407471\n",
      "Training-Batch No.160\n",
      "Loss = 3.587904930114746\n",
      "Training-Batch No.170\n",
      "Loss = 3.413902759552002\n",
      "Training-Batch No.180\n",
      "Loss = 4.153591632843018\n",
      "Training-Batch No.190\n",
      "Loss = 3.8213536739349365\n",
      "Training-Batch No.200\n",
      "Loss = 3.6126818656921387\n",
      "Training-Batch No.210\n",
      "Loss = 3.965130090713501\n",
      "Training-Batch No.220\n",
      "Loss = 3.5185701847076416\n",
      "Training-Batch No.230\n",
      "Loss = 3.9568593502044678\n",
      "Training-Batch No.240\n",
      "Loss = 4.239731788635254\n",
      "Training-Batch No.250\n",
      "Loss = 3.5508174896240234\n",
      "Training-Batch No.260\n",
      "Loss = 3.1628758907318115\n",
      "Training-Batch No.270\n",
      "Loss = 3.818545341491699\n",
      "Training-Batch No.280\n",
      "Loss = 3.708099365234375\n",
      "Training-Batch No.290\n",
      "Loss = 3.453665018081665\n",
      "Training-Batch No.300\n",
      "Loss = 3.2324182987213135\n",
      "Training-Batch No.310\n",
      "Loss = 3.8593125343322754\n",
      "Training-Batch No.320\n",
      "Loss = 3.8326053619384766\n",
      "Training-Batch No.330\n",
      "Loss = 3.9219858646392822\n",
      "Training-Batch No.340\n",
      "Loss = 3.2407150268554688\n",
      "Training-Batch No.350\n",
      "Loss = 3.5404610633850098\n",
      "Training-Batch No.360\n",
      "Loss = 3.9240331649780273\n",
      "Training-Batch No.370\n",
      "Loss = 3.9248549938201904\n",
      "Training-Batch No.380\n",
      "Loss = 3.799570083618164\n",
      "Training-Batch No.390\n",
      "Loss = 3.644937753677368\n",
      "Training-Batch No.400\n",
      "Loss = 3.8547260761260986\n",
      "Training-Batch No.410\n",
      "Loss = 3.4128305912017822\n",
      "Training-Batch No.420\n",
      "Loss = 3.0686326026916504\n",
      "Training-Batch No.430\n",
      "Loss = 3.7065958976745605\n",
      "Training-Batch No.440\n",
      "Loss = 3.9591526985168457\n",
      "Training-Batch No.450\n",
      "Loss = 3.737887144088745\n",
      "Training-Batch No.460\n",
      "Loss = 2.789335250854492\n",
      "Training-Batch No.470\n",
      "Loss = 3.956937074661255\n",
      "Training-Batch No.480\n",
      "Loss = 3.2857413291931152\n",
      "Training-Batch No.490\n",
      "Loss = 3.1295576095581055\n",
      "Training-Batch No.500\n",
      "Loss = 3.047152519226074\n",
      "Training-Batch No.510\n",
      "Loss = 3.8025715351104736\n",
      "Training-Batch No.520\n",
      "Loss = 3.6342039108276367\n",
      "Training-Batch No.530\n",
      "Loss = 3.759167194366455\n",
      "Training-Batch No.540\n",
      "Loss = 3.3752188682556152\n",
      "Training-Batch No.550\n",
      "Loss = 3.3937649726867676\n",
      "Training-Batch No.560\n",
      "Loss = 3.8233096599578857\n",
      "Training-Batch No.570\n",
      "Loss = 3.153421401977539\n",
      "Training-Batch No.580\n",
      "Loss = 3.94358229637146\n",
      "Training-Batch No.590\n",
      "Loss = 3.701948881149292\n",
      "Training-Batch No.600\n",
      "Loss = 3.7668280601501465\n",
      "Training-Batch No.610\n",
      "Loss = 3.87505841255188\n",
      "Training-Batch No.620\n",
      "Loss = 3.3647658824920654\n",
      "Training-Batch No.630\n",
      "Loss = 3.751248359680176\n",
      "Training-Batch No.640\n",
      "Loss = 3.688462257385254\n",
      "Training-Batch No.650\n",
      "Loss = 3.643796443939209\n",
      "Training-Batch No.660\n",
      "Loss = 3.620640277862549\n",
      "Training-Batch No.670\n",
      "Loss = 3.5278103351593018\n",
      "Training-Batch No.680\n",
      "Loss = 3.722726821899414\n",
      "Training-Batch No.690\n",
      "Loss = 3.509719133377075\n",
      "Training-Batch No.700\n",
      "Loss = 3.2175240516662598\n",
      "Training-Batch No.710\n",
      "Loss = 3.6992547512054443\n",
      "Training-Batch No.720\n",
      "Loss = 3.24682879447937\n",
      "Training-Batch No.730\n",
      "Loss = 3.747837543487549\n",
      "Training-Batch No.740\n",
      "Loss = 4.132212162017822\n",
      "Training-Batch No.750\n",
      "Loss = 3.532512903213501\n",
      "Training-Batch No.760\n",
      "Loss = 3.6396265029907227\n",
      "Training-Batch No.770\n",
      "Loss = 4.359778881072998\n",
      "Training-Batch No.780\n",
      "Loss = 3.2438297271728516\n",
      "Training-Batch No.790\n",
      "Loss = 3.1760306358337402\n",
      "Training-Batch No.800\n",
      "Loss = 3.2380423545837402\n",
      "Training-Batch No.810\n",
      "Loss = 3.3410391807556152\n",
      "Training-Batch No.820\n",
      "Loss = 3.8277945518493652\n",
      "Training-Batch No.830\n",
      "Loss = 3.7003073692321777\n",
      "Training-Batch No.840\n",
      "Loss = 3.7964675426483154\n",
      "Training-Batch No.850\n",
      "Loss = 3.540006399154663\n",
      "Training-Batch No.860\n",
      "Loss = 3.5576024055480957\n",
      "Training-Batch No.870\n",
      "Loss = 3.2213523387908936\n",
      "Training-Batch No.880\n",
      "Loss = 2.9976797103881836\n",
      "Training-Batch No.890\n",
      "Loss = 3.4192440509796143\n",
      "Training-Batch No.900\n",
      "Loss = 3.5870964527130127\n",
      "Training-Batch No.910\n",
      "Loss = 3.5034470558166504\n",
      "Training-Batch No.920\n",
      "Loss = 3.5434207916259766\n",
      "Training-Batch No.930\n",
      "Loss = 3.753023624420166\n",
      "Epoch 1 Training Loss: 3.7090\n",
      "Start Validating\n",
      "Epoch 1 Validation Loss: 3.6517\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.35591397849462364\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.46102150537634407\n",
      "Average Top-11 accuracy 0.5021505376344086\n",
      "Average Top-13 accuracy 0.5466397849462366\n",
      "Average Top-15 accuracy 0.5806451612903226\n",
      "current acc 0.18508064516129033\n",
      "best acc 0\n",
      "Saving the best model\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 2\n",
      "Training-Batch No.940\n",
      "Loss = 3.7471415996551514\n",
      "Training-Batch No.950\n",
      "Loss = 3.0842156410217285\n",
      "Training-Batch No.960\n",
      "Loss = 3.821805715560913\n",
      "Training-Batch No.970\n",
      "Loss = 4.017773628234863\n",
      "Training-Batch No.980\n",
      "Loss = 3.205887794494629\n",
      "Training-Batch No.990\n",
      "Loss = 4.144363880157471\n",
      "Training-Batch No.1000\n",
      "Loss = 4.066403865814209\n",
      "Training-Batch No.1010\n",
      "Loss = 3.8750641345977783\n",
      "Training-Batch No.1020\n",
      "Loss = 4.034514904022217\n",
      "Training-Batch No.1030\n",
      "Loss = 3.6532816886901855\n",
      "Training-Batch No.1040\n",
      "Loss = 3.5571513175964355\n",
      "Training-Batch No.1050\n",
      "Loss = 3.7165327072143555\n",
      "Training-Batch No.1060\n",
      "Loss = 3.7882747650146484\n",
      "Training-Batch No.1070\n",
      "Loss = 3.710904359817505\n",
      "Training-Batch No.1080\n",
      "Loss = 4.199720859527588\n",
      "Training-Batch No.1090\n",
      "Loss = 3.4592459201812744\n",
      "Training-Batch No.1100\n",
      "Loss = 3.394873857498169\n",
      "Training-Batch No.1110\n",
      "Loss = 4.175660133361816\n",
      "Training-Batch No.1120\n",
      "Loss = 3.8204171657562256\n",
      "Training-Batch No.1130\n",
      "Loss = 3.601064443588257\n",
      "Training-Batch No.1140\n",
      "Loss = 3.9405298233032227\n",
      "Training-Batch No.1150\n",
      "Loss = 3.490065574645996\n",
      "Training-Batch No.1160\n",
      "Loss = 3.895193338394165\n",
      "Training-Batch No.1170\n",
      "Loss = 4.237302303314209\n",
      "Training-Batch No.1180\n",
      "Loss = 3.545715570449829\n",
      "Training-Batch No.1190\n",
      "Loss = 3.1592044830322266\n",
      "Training-Batch No.1200\n",
      "Loss = 3.799534797668457\n",
      "Training-Batch No.1210\n",
      "Loss = 3.6689698696136475\n",
      "Training-Batch No.1220\n",
      "Loss = 3.438687801361084\n",
      "Training-Batch No.1230\n",
      "Loss = 3.206577777862549\n",
      "Training-Batch No.1240\n",
      "Loss = 3.830087423324585\n",
      "Training-Batch No.1250\n",
      "Loss = 3.8014137744903564\n",
      "Training-Batch No.1260\n",
      "Loss = 3.937044620513916\n",
      "Training-Batch No.1270\n",
      "Loss = 3.2340824604034424\n",
      "Training-Batch No.1280\n",
      "Loss = 3.509948968887329\n",
      "Training-Batch No.1290\n",
      "Loss = 3.9411275386810303\n",
      "Training-Batch No.1300\n",
      "Loss = 3.9236769676208496\n",
      "Training-Batch No.1310\n",
      "Loss = 3.7946836948394775\n",
      "Training-Batch No.1320\n",
      "Loss = 3.6465582847595215\n",
      "Training-Batch No.1330\n",
      "Loss = 3.8610880374908447\n",
      "Training-Batch No.1340\n",
      "Loss = 3.403299331665039\n",
      "Training-Batch No.1350\n",
      "Loss = 3.0636191368103027\n",
      "Training-Batch No.1360\n",
      "Loss = 3.7108359336853027\n",
      "Training-Batch No.1370\n",
      "Loss = 3.94116473197937\n",
      "Training-Batch No.1380\n",
      "Loss = 3.7546355724334717\n",
      "Training-Batch No.1390\n",
      "Loss = 2.7757675647735596\n",
      "Training-Batch No.1400\n",
      "Loss = 3.9389586448669434\n",
      "Training-Batch No.1410\n",
      "Loss = 3.2984366416931152\n",
      "Training-Batch No.1420\n",
      "Loss = 3.146381378173828\n",
      "Training-Batch No.1430\n",
      "Loss = 3.0507874488830566\n",
      "Training-Batch No.1440\n",
      "Loss = 3.798693895339966\n",
      "Training-Batch No.1450\n",
      "Loss = 3.6320226192474365\n",
      "Training-Batch No.1460\n",
      "Loss = 3.759965419769287\n",
      "Training-Batch No.1470\n",
      "Loss = 3.3582956790924072\n",
      "Training-Batch No.1480\n",
      "Loss = 3.4027113914489746\n",
      "Training-Batch No.1490\n",
      "Loss = 3.8306944370269775\n",
      "Training-Batch No.1500\n",
      "Loss = 3.1331992149353027\n",
      "Training-Batch No.1510\n",
      "Loss = 3.940091848373413\n",
      "Training-Batch No.1520\n",
      "Loss = 3.6934971809387207\n",
      "Training-Batch No.1530\n",
      "Loss = 3.7687151432037354\n",
      "Training-Batch No.1540\n",
      "Loss = 3.8745641708374023\n",
      "Training-Batch No.1550\n",
      "Loss = 3.349902629852295\n",
      "Training-Batch No.1560\n",
      "Loss = 3.743978977203369\n",
      "Training-Batch No.1570\n",
      "Loss = 3.6846542358398438\n",
      "Training-Batch No.1580\n",
      "Loss = 3.63430118560791\n",
      "Training-Batch No.1590\n",
      "Loss = 3.6062774658203125\n",
      "Training-Batch No.1600\n",
      "Loss = 3.532367467880249\n",
      "Training-Batch No.1610\n",
      "Loss = 3.7145137786865234\n",
      "Training-Batch No.1620\n",
      "Loss = 3.5110414028167725\n",
      "Training-Batch No.1630\n",
      "Loss = 3.215165138244629\n",
      "Training-Batch No.1640\n",
      "Loss = 3.6915671825408936\n",
      "Training-Batch No.1650\n",
      "Loss = 3.2553930282592773\n",
      "Training-Batch No.1660\n",
      "Loss = 3.7410101890563965\n",
      "Training-Batch No.1670\n",
      "Loss = 4.110901832580566\n",
      "Training-Batch No.1680\n",
      "Loss = 3.5356225967407227\n",
      "Training-Batch No.1690\n",
      "Loss = 3.62838077545166\n",
      "Training-Batch No.1700\n",
      "Loss = 4.34902286529541\n",
      "Training-Batch No.1710\n",
      "Loss = 3.2478411197662354\n",
      "Training-Batch No.1720\n",
      "Loss = 3.1791014671325684\n",
      "Training-Batch No.1730\n",
      "Loss = 3.23384952545166\n",
      "Training-Batch No.1740\n",
      "Loss = 3.341543197631836\n",
      "Training-Batch No.1750\n",
      "Loss = 3.8183488845825195\n",
      "Training-Batch No.1760\n",
      "Loss = 3.728987216949463\n",
      "Training-Batch No.1770\n",
      "Loss = 3.7967019081115723\n",
      "Training-Batch No.1780\n",
      "Loss = 3.5297646522521973\n",
      "Training-Batch No.1790\n",
      "Loss = 3.5560848712921143\n",
      "Training-Batch No.1800\n",
      "Loss = 3.225215196609497\n",
      "Training-Batch No.1810\n",
      "Loss = 3.0060479640960693\n",
      "Training-Batch No.1820\n",
      "Loss = 3.417996406555176\n",
      "Training-Batch No.1830\n",
      "Loss = 3.581044912338257\n",
      "Training-Batch No.1840\n",
      "Loss = 3.5035157203674316\n",
      "Training-Batch No.1850\n",
      "Loss = 3.540395736694336\n",
      "Training-Batch No.1860\n",
      "Loss = 3.750849485397339\n",
      "Epoch 2 Training Loss: 3.6539\n",
      "Start Validating\n",
      "Epoch 2 Validation Loss: 3.6486\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.35591397849462364\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.46102150537634407\n",
      "Average Top-11 accuracy 0.5056451612903226\n",
      "Average Top-13 accuracy 0.5466397849462366\n",
      "Average Top-15 accuracy 0.5806451612903226\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 3\n",
      "Training-Batch No.1870\n",
      "Loss = 3.765552520751953\n",
      "Training-Batch No.1880\n",
      "Loss = 3.103874683380127\n",
      "Training-Batch No.1890\n",
      "Loss = 3.808131217956543\n",
      "Training-Batch No.1900\n",
      "Loss = 4.023010730743408\n",
      "Training-Batch No.1910\n",
      "Loss = 3.26310658454895\n",
      "Training-Batch No.1920\n",
      "Loss = 4.192639350891113\n",
      "Training-Batch No.1930\n",
      "Loss = 4.123337745666504\n",
      "Training-Batch No.1940\n",
      "Loss = 3.869112730026245\n",
      "Training-Batch No.1950\n",
      "Loss = 4.048976898193359\n",
      "Training-Batch No.1960\n",
      "Loss = 3.6503543853759766\n",
      "Training-Batch No.1970\n",
      "Loss = 3.5657644271850586\n",
      "Training-Batch No.1980\n",
      "Loss = 3.688725233078003\n",
      "Training-Batch No.1990\n",
      "Loss = 3.792574644088745\n",
      "Training-Batch No.2000\n",
      "Loss = 3.7040772438049316\n",
      "Training-Batch No.2010\n",
      "Loss = 4.182419776916504\n",
      "Training-Batch No.2020\n",
      "Loss = 3.4730892181396484\n",
      "Training-Batch No.2030\n",
      "Loss = 3.363067388534546\n",
      "Training-Batch No.2040\n",
      "Loss = 4.185361385345459\n",
      "Training-Batch No.2050\n",
      "Loss = 3.822718381881714\n",
      "Training-Batch No.2060\n",
      "Loss = 3.5910027027130127\n",
      "Training-Batch No.2070\n",
      "Loss = 3.93123459815979\n",
      "Training-Batch No.2080\n",
      "Loss = 3.4810361862182617\n",
      "Training-Batch No.2090\n",
      "Loss = 3.9427151679992676\n",
      "Training-Batch No.2100\n",
      "Loss = 4.2305216789245605\n",
      "Training-Batch No.2110\n",
      "Loss = 3.548097848892212\n",
      "Training-Batch No.2120\n",
      "Loss = 3.1603238582611084\n",
      "Training-Batch No.2130\n",
      "Loss = 3.7854435443878174\n",
      "Training-Batch No.2140\n",
      "Loss = 3.659226894378662\n",
      "Training-Batch No.2150\n",
      "Loss = 3.430001735687256\n",
      "Training-Batch No.2160\n",
      "Loss = 3.2250430583953857\n",
      "Training-Batch No.2170\n",
      "Loss = 3.825014114379883\n",
      "Training-Batch No.2180\n",
      "Loss = 3.8122284412384033\n",
      "Training-Batch No.2190\n",
      "Loss = 3.9431278705596924\n",
      "Training-Batch No.2200\n",
      "Loss = 3.222120523452759\n",
      "Training-Batch No.2210\n",
      "Loss = 3.5016117095947266\n",
      "Training-Batch No.2220\n",
      "Loss = 3.9423675537109375\n",
      "Training-Batch No.2230\n",
      "Loss = 4.107289791107178\n",
      "Training-Batch No.2240\n",
      "Loss = 3.7914328575134277\n",
      "Training-Batch No.2250\n",
      "Loss = 3.643275260925293\n",
      "Training-Batch No.2260\n",
      "Loss = 3.8610339164733887\n",
      "Training-Batch No.2270\n",
      "Loss = 3.401271343231201\n",
      "Training-Batch No.2280\n",
      "Loss = 3.0633456707000732\n",
      "Training-Batch No.2290\n",
      "Loss = 3.706174850463867\n",
      "Training-Batch No.2300\n",
      "Loss = 3.911099910736084\n",
      "Training-Batch No.2310\n",
      "Loss = 3.7573227882385254\n",
      "Training-Batch No.2320\n",
      "Loss = 2.696629762649536\n",
      "Training-Batch No.2330\n",
      "Loss = 3.9202733039855957\n",
      "Training-Batch No.2340\n",
      "Loss = 3.303643226623535\n",
      "Training-Batch No.2350\n",
      "Loss = 3.1495144367218018\n",
      "Training-Batch No.2360\n",
      "Loss = 3.0506792068481445\n",
      "Training-Batch No.2370\n",
      "Loss = 3.788358688354492\n",
      "Training-Batch No.2380\n",
      "Loss = 3.6232786178588867\n",
      "Training-Batch No.2390\n",
      "Loss = 3.758780002593994\n",
      "Training-Batch No.2400\n",
      "Loss = 3.3432857990264893\n",
      "Training-Batch No.2410\n",
      "Loss = 3.4016151428222656\n",
      "Training-Batch No.2420\n",
      "Loss = 3.8323111534118652\n",
      "Training-Batch No.2430\n",
      "Loss = 3.1347901821136475\n",
      "Training-Batch No.2440\n",
      "Loss = 3.9358160495758057\n",
      "Training-Batch No.2450\n",
      "Loss = 3.6888773441314697\n",
      "Training-Batch No.2460\n",
      "Loss = 3.7664847373962402\n",
      "Training-Batch No.2470\n",
      "Loss = 3.873201608657837\n",
      "Training-Batch No.2480\n",
      "Loss = 3.3590176105499268\n",
      "Training-Batch No.2490\n",
      "Loss = 3.735690116882324\n",
      "Training-Batch No.2500\n",
      "Loss = 3.6915605068206787\n",
      "Training-Batch No.2510\n",
      "Loss = 3.6299469470977783\n",
      "Training-Batch No.2520\n",
      "Loss = 3.597346305847168\n",
      "Training-Batch No.2530\n",
      "Loss = 3.5291571617126465\n",
      "Training-Batch No.2540\n",
      "Loss = 3.7095205783843994\n",
      "Training-Batch No.2550\n",
      "Loss = 3.5140247344970703\n",
      "Training-Batch No.2560\n",
      "Loss = 3.217334032058716\n",
      "Training-Batch No.2570\n",
      "Loss = 3.68412184715271\n",
      "Training-Batch No.2580\n",
      "Loss = 3.2465524673461914\n",
      "Training-Batch No.2590\n",
      "Loss = 3.7407028675079346\n",
      "Training-Batch No.2600\n",
      "Loss = 4.109537601470947\n",
      "Training-Batch No.2610\n",
      "Loss = 3.536642074584961\n",
      "Training-Batch No.2620\n",
      "Loss = 3.622960090637207\n",
      "Training-Batch No.2630\n",
      "Loss = 4.346175193786621\n",
      "Training-Batch No.2640\n",
      "Loss = 3.247490406036377\n",
      "Training-Batch No.2650\n",
      "Loss = 3.1783411502838135\n",
      "Training-Batch No.2660\n",
      "Loss = 3.2400107383728027\n",
      "Training-Batch No.2670\n",
      "Loss = 3.3508193492889404\n",
      "Training-Batch No.2680\n",
      "Loss = 3.815808057785034\n",
      "Training-Batch No.2690\n",
      "Loss = 3.7369399070739746\n",
      "Training-Batch No.2700\n",
      "Loss = 3.798532724380493\n",
      "Training-Batch No.2710\n",
      "Loss = 3.5287795066833496\n",
      "Training-Batch No.2720\n",
      "Loss = 3.5576066970825195\n",
      "Training-Batch No.2730\n",
      "Loss = 3.236368179321289\n",
      "Training-Batch No.2740\n",
      "Loss = 3.0123162269592285\n",
      "Training-Batch No.2750\n",
      "Loss = 3.4148128032684326\n",
      "Training-Batch No.2760\n",
      "Loss = 3.577383041381836\n",
      "Training-Batch No.2770\n",
      "Loss = 3.5041024684906006\n",
      "Training-Batch No.2780\n",
      "Loss = 3.540454149246216\n",
      "Training-Batch No.2790\n",
      "Loss = 3.749560594558716\n",
      "Epoch 3 Training Loss: 3.6527\n",
      "Start Validating\n",
      "Epoch 3 Validation Loss: 3.6468\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.35591397849462364\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.46102150537634407\n",
      "Average Top-11 accuracy 0.5056451612903226\n",
      "Average Top-13 accuracy 0.5466397849462366\n",
      "Average Top-15 accuracy 0.5806451612903226\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 4\n",
      "Training-Batch No.2800\n",
      "Loss = 3.7533926963806152\n",
      "Training-Batch No.2810\n",
      "Loss = 3.0948848724365234\n",
      "Training-Batch No.2820\n",
      "Loss = 3.805492639541626\n",
      "Training-Batch No.2830\n",
      "Loss = 4.029270648956299\n",
      "Training-Batch No.2840\n",
      "Loss = 3.2616777420043945\n",
      "Training-Batch No.2850\n",
      "Loss = 4.184948921203613\n",
      "Training-Batch No.2860\n",
      "Loss = 4.075360298156738\n",
      "Training-Batch No.2870\n",
      "Loss = 3.869393825531006\n",
      "Training-Batch No.2880\n",
      "Loss = 4.045522212982178\n",
      "Training-Batch No.2890\n",
      "Loss = 3.6460843086242676\n",
      "Training-Batch No.2900\n",
      "Loss = 3.5636653900146484\n",
      "Training-Batch No.2910\n",
      "Loss = 3.687958240509033\n",
      "Training-Batch No.2920\n",
      "Loss = 3.7962276935577393\n",
      "Training-Batch No.2930\n",
      "Loss = 3.7055182456970215\n",
      "Training-Batch No.2940\n",
      "Loss = 4.180456638336182\n",
      "Training-Batch No.2950\n",
      "Loss = 3.4736931324005127\n",
      "Training-Batch No.2960\n",
      "Loss = 3.3610525131225586\n",
      "Training-Batch No.2970\n",
      "Loss = 4.198084354400635\n",
      "Training-Batch No.2980\n",
      "Loss = 3.823185443878174\n",
      "Training-Batch No.2990\n",
      "Loss = 3.5945944786071777\n",
      "Training-Batch No.3000\n",
      "Loss = 3.9425737857818604\n",
      "Training-Batch No.3010\n",
      "Loss = 3.4762609004974365\n",
      "Training-Batch No.3020\n",
      "Loss = 3.896993398666382\n",
      "Training-Batch No.3030\n",
      "Loss = 4.235626697540283\n",
      "Training-Batch No.3040\n",
      "Loss = 3.5629265308380127\n",
      "Training-Batch No.3050\n",
      "Loss = 3.1647660732269287\n",
      "Training-Batch No.3060\n",
      "Loss = 3.7547261714935303\n",
      "Training-Batch No.3070\n",
      "Loss = 3.6420557498931885\n",
      "Training-Batch No.3080\n",
      "Loss = 3.4178719520568848\n",
      "Training-Batch No.3090\n",
      "Loss = 3.224512815475464\n",
      "Training-Batch No.3100\n",
      "Loss = 3.816755533218384\n",
      "Training-Batch No.3110\n",
      "Loss = 3.806561231613159\n",
      "Training-Batch No.3120\n",
      "Loss = 3.9487500190734863\n",
      "Training-Batch No.3130\n",
      "Loss = 3.2180209159851074\n",
      "Training-Batch No.3140\n",
      "Loss = 3.4990756511688232\n",
      "Training-Batch No.3150\n",
      "Loss = 3.950340986251831\n",
      "Training-Batch No.3160\n",
      "Loss = 3.9299168586730957\n",
      "Training-Batch No.3170\n",
      "Loss = 3.7891881465911865\n",
      "Training-Batch No.3180\n",
      "Loss = 3.640497922897339\n",
      "Training-Batch No.3190\n",
      "Loss = 3.8634989261627197\n",
      "Training-Batch No.3200\n",
      "Loss = 3.3912930488586426\n",
      "Training-Batch No.3210\n",
      "Loss = 3.050083875656128\n",
      "Training-Batch No.3220\n",
      "Loss = 3.7089154720306396\n",
      "Training-Batch No.3230\n",
      "Loss = 3.897993326187134\n",
      "Training-Batch No.3240\n",
      "Loss = 3.715102195739746\n",
      "Training-Batch No.3250\n",
      "Loss = 2.7852845191955566\n",
      "Training-Batch No.3260\n",
      "Loss = 3.9078683853149414\n",
      "Training-Batch No.3270\n",
      "Loss = 3.301483392715454\n",
      "Training-Batch No.3280\n",
      "Loss = 3.141758441925049\n",
      "Training-Batch No.3290\n",
      "Loss = 3.0403130054473877\n",
      "Training-Batch No.3300\n",
      "Loss = 3.7839512825012207\n",
      "Training-Batch No.3310\n",
      "Loss = 3.6276798248291016\n",
      "Training-Batch No.3320\n",
      "Loss = 3.7593719959259033\n",
      "Training-Batch No.3330\n",
      "Loss = 3.330465078353882\n",
      "Training-Batch No.3340\n",
      "Loss = 3.3994059562683105\n",
      "Training-Batch No.3350\n",
      "Loss = 3.8363301753997803\n",
      "Training-Batch No.3360\n",
      "Loss = 3.1337218284606934\n",
      "Training-Batch No.3370\n",
      "Loss = 3.929570436477661\n",
      "Training-Batch No.3380\n",
      "Loss = 3.6849606037139893\n",
      "Training-Batch No.3390\n",
      "Loss = 3.771023750305176\n",
      "Training-Batch No.3400\n",
      "Loss = 3.874152183532715\n",
      "Training-Batch No.3410\n",
      "Loss = 3.3553576469421387\n",
      "Training-Batch No.3420\n",
      "Loss = 3.729030132293701\n",
      "Training-Batch No.3430\n",
      "Loss = 3.687559127807617\n",
      "Training-Batch No.3440\n",
      "Loss = 3.6218175888061523\n",
      "Training-Batch No.3450\n",
      "Loss = 3.5925705432891846\n",
      "Training-Batch No.3460\n",
      "Loss = 3.5299248695373535\n",
      "Training-Batch No.3470\n",
      "Loss = 3.704129457473755\n",
      "Training-Batch No.3480\n",
      "Loss = 3.51735782623291\n",
      "Training-Batch No.3490\n",
      "Loss = 3.221219301223755\n",
      "Training-Batch No.3500\n",
      "Loss = 3.6786980628967285\n",
      "Training-Batch No.3510\n",
      "Loss = 3.217036247253418\n",
      "Training-Batch No.3520\n",
      "Loss = 3.733156681060791\n",
      "Training-Batch No.3530\n",
      "Loss = 4.082854747772217\n",
      "Training-Batch No.3540\n",
      "Loss = 3.54179310798645\n",
      "Training-Batch No.3550\n",
      "Loss = 3.620922565460205\n",
      "Training-Batch No.3560\n",
      "Loss = 4.3391547203063965\n",
      "Training-Batch No.3570\n",
      "Loss = 3.247248888015747\n",
      "Training-Batch No.3580\n",
      "Loss = 3.1739611625671387\n",
      "Training-Batch No.3590\n",
      "Loss = 3.2304282188415527\n",
      "Training-Batch No.3600\n",
      "Loss = 3.3490359783172607\n",
      "Training-Batch No.3610\n",
      "Loss = 3.8091931343078613\n",
      "Training-Batch No.3620\n",
      "Loss = 3.741875648498535\n",
      "Training-Batch No.3630\n",
      "Loss = 3.797158718109131\n",
      "Training-Batch No.3640\n",
      "Loss = 3.5202107429504395\n",
      "Training-Batch No.3650\n",
      "Loss = 3.557161569595337\n",
      "Training-Batch No.3660\n",
      "Loss = 3.236555337905884\n",
      "Training-Batch No.3670\n",
      "Loss = 3.01841402053833\n",
      "Training-Batch No.3680\n",
      "Loss = 3.414854049682617\n",
      "Training-Batch No.3690\n",
      "Loss = 3.5752127170562744\n",
      "Training-Batch No.3700\n",
      "Loss = 3.506978750228882\n",
      "Training-Batch No.3710\n",
      "Loss = 3.5401058197021484\n",
      "Training-Batch No.3720\n",
      "Loss = 3.7484140396118164\n",
      "Epoch 4 Training Loss: 3.6503\n",
      "Start Validating\n",
      "Epoch 4 Validation Loss: 3.6453\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.35591397849462364\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.46841397849462363\n",
      "Average Top-11 accuracy 0.5056451612903226\n",
      "Average Top-13 accuracy 0.5466397849462366\n",
      "Average Top-15 accuracy 0.5806451612903226\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 5\n",
      "Training-Batch No.3730\n",
      "Loss = 3.755450963973999\n",
      "Training-Batch No.3740\n",
      "Loss = 3.110560655593872\n",
      "Training-Batch No.3750\n",
      "Loss = 3.7659342288970947\n",
      "Training-Batch No.3760\n",
      "Loss = 4.047750473022461\n",
      "Training-Batch No.3770\n",
      "Loss = 3.2828586101531982\n",
      "Training-Batch No.3780\n",
      "Loss = 4.218811511993408\n",
      "Training-Batch No.3790\n",
      "Loss = 4.000582218170166\n",
      "Training-Batch No.3800\n",
      "Loss = 3.7975125312805176\n",
      "Training-Batch No.3810\n",
      "Loss = 4.081376075744629\n",
      "Training-Batch No.3820\n",
      "Loss = 3.567216634750366\n",
      "Training-Batch No.3830\n",
      "Loss = 3.6344127655029297\n",
      "Training-Batch No.3840\n",
      "Loss = 3.636507749557495\n",
      "Training-Batch No.3850\n",
      "Loss = 3.864856481552124\n",
      "Training-Batch No.3860\n",
      "Loss = 3.735628604888916\n",
      "Training-Batch No.3870\n",
      "Loss = 4.194727420806885\n",
      "Training-Batch No.3880\n",
      "Loss = 3.5312001705169678\n",
      "Training-Batch No.3890\n",
      "Loss = 3.2850613594055176\n",
      "Training-Batch No.3900\n",
      "Loss = 4.110565185546875\n",
      "Training-Batch No.3910\n",
      "Loss = 3.8579373359680176\n",
      "Training-Batch No.3920\n",
      "Loss = 3.6305108070373535\n",
      "Training-Batch No.3930\n",
      "Loss = 3.9417741298675537\n",
      "Training-Batch No.3940\n",
      "Loss = 3.3963780403137207\n",
      "Training-Batch No.3950\n",
      "Loss = 3.9861843585968018\n",
      "Training-Batch No.3960\n",
      "Loss = 4.195584297180176\n",
      "Training-Batch No.3970\n",
      "Loss = 3.573024272918701\n",
      "Training-Batch No.3980\n",
      "Loss = 3.187070369720459\n",
      "Training-Batch No.3990\n",
      "Loss = 3.8008532524108887\n",
      "Training-Batch No.4000\n",
      "Loss = 3.5963001251220703\n",
      "Training-Batch No.4010\n",
      "Loss = 3.3262712955474854\n",
      "Training-Batch No.4020\n",
      "Loss = 3.217280626296997\n",
      "Training-Batch No.4030\n",
      "Loss = 3.8134219646453857\n",
      "Training-Batch No.4040\n",
      "Loss = 3.798962354660034\n",
      "Training-Batch No.4050\n",
      "Loss = 3.916534900665283\n",
      "Training-Batch No.4060\n",
      "Loss = 3.204942464828491\n",
      "Training-Batch No.4070\n",
      "Loss = 3.5027620792388916\n",
      "Training-Batch No.4080\n",
      "Loss = 3.936328411102295\n",
      "Training-Batch No.4090\n",
      "Loss = 3.9265732765197754\n",
      "Training-Batch No.4100\n",
      "Loss = 3.7532098293304443\n",
      "Training-Batch No.4110\n",
      "Loss = 3.6371779441833496\n",
      "Training-Batch No.4120\n",
      "Loss = 3.8806939125061035\n",
      "Training-Batch No.4130\n",
      "Loss = 3.3284993171691895\n",
      "Training-Batch No.4140\n",
      "Loss = 2.9577646255493164\n",
      "Training-Batch No.4150\n",
      "Loss = 3.6877942085266113\n",
      "Training-Batch No.4160\n",
      "Loss = 3.8298797607421875\n",
      "Training-Batch No.4170\n",
      "Loss = 3.706169843673706\n",
      "Training-Batch No.4180\n",
      "Loss = 2.7866640090942383\n",
      "Training-Batch No.4190\n",
      "Loss = 3.858234405517578\n",
      "Training-Batch No.4200\n",
      "Loss = 3.3240387439727783\n",
      "Training-Batch No.4210\n",
      "Loss = 3.137824773788452\n",
      "Training-Batch No.4220\n",
      "Loss = 3.019834280014038\n",
      "Training-Batch No.4230\n",
      "Loss = 3.792905330657959\n",
      "Training-Batch No.4240\n",
      "Loss = 3.626129627227783\n",
      "Training-Batch No.4250\n",
      "Loss = 3.666733741760254\n",
      "Training-Batch No.4260\n",
      "Loss = 3.2500338554382324\n",
      "Training-Batch No.4270\n",
      "Loss = 3.41174578666687\n",
      "Training-Batch No.4280\n",
      "Loss = 3.90549635887146\n",
      "Training-Batch No.4290\n",
      "Loss = 3.1008141040802\n",
      "Training-Batch No.4300\n",
      "Loss = 3.8972933292388916\n",
      "Training-Batch No.4310\n",
      "Loss = 3.6085634231567383\n",
      "Training-Batch No.4320\n",
      "Loss = 3.7508630752563477\n",
      "Training-Batch No.4330\n",
      "Loss = 3.8871939182281494\n",
      "Training-Batch No.4340\n",
      "Loss = 3.320796251296997\n",
      "Training-Batch No.4350\n",
      "Loss = 3.699061393737793\n",
      "Training-Batch No.4360\n",
      "Loss = 3.712942361831665\n",
      "Training-Batch No.4370\n",
      "Loss = 3.623296022415161\n",
      "Training-Batch No.4380\n",
      "Loss = 3.602782964706421\n",
      "Training-Batch No.4390\n",
      "Loss = 3.5231151580810547\n",
      "Training-Batch No.4400\n",
      "Loss = 3.676788806915283\n",
      "Training-Batch No.4410\n",
      "Loss = 3.4909441471099854\n",
      "Training-Batch No.4420\n",
      "Loss = 3.2222723960876465\n",
      "Training-Batch No.4430\n",
      "Loss = 3.601586103439331\n",
      "Training-Batch No.4440\n",
      "Loss = 3.2023165225982666\n",
      "Training-Batch No.4450\n",
      "Loss = 3.6774814128875732\n",
      "Training-Batch No.4460\n",
      "Loss = 4.064263343811035\n",
      "Training-Batch No.4470\n",
      "Loss = 3.5723822116851807\n",
      "Training-Batch No.4480\n",
      "Loss = 3.582920551300049\n",
      "Training-Batch No.4490\n",
      "Loss = 4.421334743499756\n",
      "Training-Batch No.4500\n",
      "Loss = 3.2601168155670166\n",
      "Training-Batch No.4510\n",
      "Loss = 3.0933194160461426\n",
      "Training-Batch No.4520\n",
      "Loss = 3.1623449325561523\n",
      "Training-Batch No.4530\n",
      "Loss = 3.3116462230682373\n",
      "Training-Batch No.4540\n",
      "Loss = 3.7846860885620117\n",
      "Training-Batch No.4550\n",
      "Loss = 3.7354564666748047\n",
      "Training-Batch No.4560\n",
      "Loss = 3.7847001552581787\n",
      "Training-Batch No.4570\n",
      "Loss = 3.4669861793518066\n",
      "Training-Batch No.4580\n",
      "Loss = 3.547236680984497\n",
      "Training-Batch No.4590\n",
      "Loss = 3.211275339126587\n",
      "Training-Batch No.4600\n",
      "Loss = 2.9959988594055176\n",
      "Training-Batch No.4610\n",
      "Loss = 3.4039909839630127\n",
      "Training-Batch No.4620\n",
      "Loss = 3.5547335147857666\n",
      "Training-Batch No.4630\n",
      "Loss = 3.4911365509033203\n",
      "Training-Batch No.4640\n",
      "Loss = 3.5502374172210693\n",
      "Training-Batch No.4650\n",
      "Loss = 3.739661455154419\n",
      "Epoch 5 Training Loss: 3.6414\n",
      "Start Validating\n",
      "Epoch 5 Validation Loss: 3.6313\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.36478494623655916\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.4635752688172043\n",
      "Average Top-11 accuracy 0.5068548387096774\n",
      "Average Top-13 accuracy 0.5439516129032258\n",
      "Average Top-15 accuracy 0.5865591397849462\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 6\n",
      "Training-Batch No.4660\n",
      "Loss = 3.778627872467041\n",
      "Training-Batch No.4670\n",
      "Loss = 3.058812379837036\n",
      "Training-Batch No.4680\n",
      "Loss = 3.778888702392578\n",
      "Training-Batch No.4690\n",
      "Loss = 4.0909104347229\n",
      "Training-Batch No.4700\n",
      "Loss = 3.249216079711914\n",
      "Training-Batch No.4710\n",
      "Loss = 4.186720848083496\n",
      "Training-Batch No.4720\n",
      "Loss = 4.015179634094238\n",
      "Training-Batch No.4730\n",
      "Loss = 3.8530611991882324\n",
      "Training-Batch No.4740\n",
      "Loss = 4.047567844390869\n",
      "Training-Batch No.4750\n",
      "Loss = 3.5967555046081543\n",
      "Training-Batch No.4760\n",
      "Loss = 3.5850725173950195\n",
      "Training-Batch No.4770\n",
      "Loss = 3.6340858936309814\n",
      "Training-Batch No.4780\n",
      "Loss = 3.837989091873169\n",
      "Training-Batch No.4790\n",
      "Loss = 3.7455151081085205\n",
      "Training-Batch No.4800\n",
      "Loss = 4.174882411956787\n",
      "Training-Batch No.4810\n",
      "Loss = 3.5138611793518066\n",
      "Training-Batch No.4820\n",
      "Loss = 3.283787965774536\n",
      "Training-Batch No.4830\n",
      "Loss = 4.12858247756958\n",
      "Training-Batch No.4840\n",
      "Loss = 3.8433566093444824\n",
      "Training-Batch No.4850\n",
      "Loss = 3.601386070251465\n",
      "Training-Batch No.4860\n",
      "Loss = 3.933161497116089\n",
      "Training-Batch No.4870\n",
      "Loss = 3.4053657054901123\n",
      "Training-Batch No.4880\n",
      "Loss = 3.941848039627075\n",
      "Training-Batch No.4890\n",
      "Loss = 4.213645935058594\n",
      "Training-Batch No.4900\n",
      "Loss = 3.5710508823394775\n",
      "Training-Batch No.4910\n",
      "Loss = 3.167940855026245\n",
      "Training-Batch No.4920\n",
      "Loss = 3.770510196685791\n",
      "Training-Batch No.4930\n",
      "Loss = 3.6094865798950195\n",
      "Training-Batch No.4940\n",
      "Loss = 3.3527886867523193\n",
      "Training-Batch No.4950\n",
      "Loss = 3.211254835128784\n",
      "Training-Batch No.4960\n",
      "Loss = 3.816199541091919\n",
      "Training-Batch No.4970\n",
      "Loss = 3.7850520610809326\n",
      "Training-Batch No.4980\n",
      "Loss = 3.929079294204712\n",
      "Training-Batch No.4990\n",
      "Loss = 3.1832475662231445\n",
      "Training-Batch No.5000\n",
      "Loss = 3.483414649963379\n",
      "Training-Batch No.5010\n",
      "Loss = 3.9304187297821045\n",
      "Training-Batch No.5020\n",
      "Loss = 3.9357750415802\n",
      "Training-Batch No.5030\n",
      "Loss = 3.764908790588379\n",
      "Training-Batch No.5040\n",
      "Loss = 3.63809871673584\n",
      "Training-Batch No.5050\n",
      "Loss = 3.881016969680786\n",
      "Training-Batch No.5060\n",
      "Loss = 3.3141872882843018\n",
      "Training-Batch No.5070\n",
      "Loss = 2.9486167430877686\n",
      "Training-Batch No.5080\n",
      "Loss = 3.6978204250335693\n",
      "Training-Batch No.5090\n",
      "Loss = 3.8495230674743652\n",
      "Training-Batch No.5100\n",
      "Loss = 3.716437339782715\n",
      "Training-Batch No.5110\n",
      "Loss = 2.768644332885742\n",
      "Training-Batch No.5120\n",
      "Loss = 3.860574245452881\n",
      "Training-Batch No.5130\n",
      "Loss = 3.318274974822998\n",
      "Training-Batch No.5140\n",
      "Loss = 3.136915683746338\n",
      "Training-Batch No.5150\n",
      "Loss = 3.0195605754852295\n",
      "Training-Batch No.5160\n",
      "Loss = 3.782020330429077\n",
      "Training-Batch No.5170\n",
      "Loss = 3.618644952774048\n",
      "Training-Batch No.5180\n",
      "Loss = 3.684420585632324\n",
      "Training-Batch No.5190\n",
      "Loss = 3.2550930976867676\n",
      "Training-Batch No.5200\n",
      "Loss = 3.406907081604004\n",
      "Training-Batch No.5210\n",
      "Loss = 3.9006829261779785\n",
      "Training-Batch No.5220\n",
      "Loss = 3.1014792919158936\n",
      "Training-Batch No.5230\n",
      "Loss = 3.896536111831665\n",
      "Training-Batch No.5240\n",
      "Loss = 3.622476816177368\n",
      "Training-Batch No.5250\n",
      "Loss = 3.7647154331207275\n",
      "Training-Batch No.5260\n",
      "Loss = 3.8825125694274902\n",
      "Training-Batch No.5270\n",
      "Loss = 3.3212645053863525\n",
      "Training-Batch No.5280\n",
      "Loss = 3.7060141563415527\n",
      "Training-Batch No.5290\n",
      "Loss = 3.7058050632476807\n",
      "Training-Batch No.5300\n",
      "Loss = 3.623642683029175\n",
      "Training-Batch No.5310\n",
      "Loss = 3.5790188312530518\n",
      "Training-Batch No.5320\n",
      "Loss = 3.5093278884887695\n",
      "Training-Batch No.5330\n",
      "Loss = 3.6791296005249023\n",
      "Training-Batch No.5340\n",
      "Loss = 3.4932496547698975\n",
      "Training-Batch No.5350\n",
      "Loss = 3.2294487953186035\n",
      "Training-Batch No.5360\n",
      "Loss = 3.605760097503662\n",
      "Training-Batch No.5370\n",
      "Loss = 3.2010700702667236\n",
      "Training-Batch No.5380\n",
      "Loss = 3.6823325157165527\n",
      "Training-Batch No.5390\n",
      "Loss = 4.0753278732299805\n",
      "Training-Batch No.5400\n",
      "Loss = 3.5672526359558105\n",
      "Training-Batch No.5410\n",
      "Loss = 3.582366943359375\n",
      "Training-Batch No.5420\n",
      "Loss = 4.408942222595215\n",
      "Training-Batch No.5430\n",
      "Loss = 3.2575058937072754\n",
      "Training-Batch No.5440\n",
      "Loss = 3.101759910583496\n",
      "Training-Batch No.5450\n",
      "Loss = 3.1634624004364014\n",
      "Training-Batch No.5460\n",
      "Loss = 3.3223624229431152\n",
      "Training-Batch No.5470\n",
      "Loss = 3.7826831340789795\n",
      "Training-Batch No.5480\n",
      "Loss = 3.7433688640594482\n",
      "Training-Batch No.5490\n",
      "Loss = 3.7942004203796387\n",
      "Training-Batch No.5500\n",
      "Loss = 3.472285747528076\n",
      "Training-Batch No.5510\n",
      "Loss = 3.554985284805298\n",
      "Training-Batch No.5520\n",
      "Loss = 3.2077465057373047\n",
      "Training-Batch No.5530\n",
      "Loss = 2.9987895488739014\n",
      "Training-Batch No.5540\n",
      "Loss = 3.4051401615142822\n",
      "Training-Batch No.5550\n",
      "Loss = 3.5549190044403076\n",
      "Training-Batch No.5560\n",
      "Loss = 3.50447940826416\n",
      "Training-Batch No.5570\n",
      "Loss = 3.555307626724243\n",
      "Training-Batch No.5580\n",
      "Loss = 3.7352395057678223\n",
      "Epoch 6 Training Loss: 3.6377\n",
      "Start Validating\n",
      "Epoch 6 Validation Loss: 3.6306\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.36478494623655916\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.4622311827956989\n",
      "Average Top-11 accuracy 0.5068548387096774\n",
      "Average Top-13 accuracy 0.5439516129032258\n",
      "Average Top-15 accuracy 0.5865591397849462\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 7\n",
      "Training-Batch No.5590\n",
      "Loss = 3.7721288204193115\n",
      "Training-Batch No.5600\n",
      "Loss = 3.0589349269866943\n",
      "Training-Batch No.5610\n",
      "Loss = 3.7805471420288086\n",
      "Training-Batch No.5620\n",
      "Loss = 4.09318733215332\n",
      "Training-Batch No.5630\n",
      "Loss = 3.2501683235168457\n",
      "Training-Batch No.5640\n",
      "Loss = 4.181548595428467\n",
      "Training-Batch No.5650\n",
      "Loss = 4.021445274353027\n",
      "Training-Batch No.5660\n",
      "Loss = 3.867203950881958\n",
      "Training-Batch No.5670\n",
      "Loss = 4.04217004776001\n",
      "Training-Batch No.5680\n",
      "Loss = 3.594512939453125\n",
      "Training-Batch No.5690\n",
      "Loss = 3.5757734775543213\n",
      "Training-Batch No.5700\n",
      "Loss = 3.6362180709838867\n",
      "Training-Batch No.5710\n",
      "Loss = 3.8335793018341064\n",
      "Training-Batch No.5720\n",
      "Loss = 3.7498440742492676\n",
      "Training-Batch No.5730\n",
      "Loss = 4.1669464111328125\n",
      "Training-Batch No.5740\n",
      "Loss = 3.512227773666382\n",
      "Training-Batch No.5750\n",
      "Loss = 3.2844667434692383\n",
      "Training-Batch No.5760\n",
      "Loss = 4.134123802185059\n",
      "Training-Batch No.5770\n",
      "Loss = 3.8395321369171143\n",
      "Training-Batch No.5780\n",
      "Loss = 3.5949854850769043\n",
      "Training-Batch No.5790\n",
      "Loss = 3.929387331008911\n",
      "Training-Batch No.5800\n",
      "Loss = 3.4080164432525635\n",
      "Training-Batch No.5810\n",
      "Loss = 3.933367967605591\n",
      "Training-Batch No.5820\n",
      "Loss = 4.217532634735107\n",
      "Training-Batch No.5830\n",
      "Loss = 3.5661911964416504\n",
      "Training-Batch No.5840\n",
      "Loss = 3.166161060333252\n",
      "Training-Batch No.5850\n",
      "Loss = 3.764726400375366\n",
      "Training-Batch No.5860\n",
      "Loss = 3.615185499191284\n",
      "Training-Batch No.5870\n",
      "Loss = 3.3605830669403076\n",
      "Training-Batch No.5880\n",
      "Loss = 3.213571548461914\n",
      "Training-Batch No.5890\n",
      "Loss = 3.8173985481262207\n",
      "Training-Batch No.5900\n",
      "Loss = 3.7825746536254883\n",
      "Training-Batch No.5910\n",
      "Loss = 3.9308128356933594\n",
      "Training-Batch No.5920\n",
      "Loss = 3.181917905807495\n",
      "Training-Batch No.5930\n",
      "Loss = 3.479749917984009\n",
      "Training-Batch No.5940\n",
      "Loss = 3.9320476055145264\n",
      "Training-Batch No.5950\n",
      "Loss = 3.93721079826355\n",
      "Training-Batch No.5960\n",
      "Loss = 3.7670912742614746\n",
      "Training-Batch No.5970\n",
      "Loss = 3.639747142791748\n",
      "Training-Batch No.5980\n",
      "Loss = 3.8799257278442383\n",
      "Training-Batch No.5990\n",
      "Loss = 3.3120951652526855\n",
      "Training-Batch No.6000\n",
      "Loss = 2.94913911819458\n",
      "Training-Batch No.6010\n",
      "Loss = 3.7005302906036377\n",
      "Training-Batch No.6020\n",
      "Loss = 3.8545749187469482\n",
      "Training-Batch No.6030\n",
      "Loss = 3.7222557067871094\n",
      "Training-Batch No.6040\n",
      "Loss = 2.767904758453369\n",
      "Training-Batch No.6050\n",
      "Loss = 3.862175703048706\n",
      "Training-Batch No.6060\n",
      "Loss = 3.3198137283325195\n",
      "Training-Batch No.6070\n",
      "Loss = 3.1380512714385986\n",
      "Training-Batch No.6080\n",
      "Loss = 3.020164728164673\n",
      "Training-Batch No.6090\n",
      "Loss = 3.777942419052124\n",
      "Training-Batch No.6100\n",
      "Loss = 3.615947961807251\n",
      "Training-Batch No.6110\n",
      "Loss = 3.6900594234466553\n",
      "Training-Batch No.6120\n",
      "Loss = 3.257061004638672\n",
      "Training-Batch No.6130\n",
      "Loss = 3.404430389404297\n",
      "Training-Batch No.6140\n",
      "Loss = 3.898876190185547\n",
      "Training-Batch No.6150\n",
      "Loss = 3.1033194065093994\n",
      "Training-Batch No.6160\n",
      "Loss = 3.894946336746216\n",
      "Training-Batch No.6170\n",
      "Loss = 3.626615285873413\n",
      "Training-Batch No.6180\n",
      "Loss = 3.7704362869262695\n",
      "Training-Batch No.6190\n",
      "Loss = 3.8812901973724365\n",
      "Training-Batch No.6200\n",
      "Loss = 3.321110725402832\n",
      "Training-Batch No.6210\n",
      "Loss = 3.7078099250793457\n",
      "Training-Batch No.6220\n",
      "Loss = 3.7036404609680176\n",
      "Training-Batch No.6230\n",
      "Loss = 3.622434377670288\n",
      "Training-Batch No.6240\n",
      "Loss = 3.5738131999969482\n",
      "Training-Batch No.6250\n",
      "Loss = 3.506610155105591\n",
      "Training-Batch No.6260\n",
      "Loss = 3.679640293121338\n",
      "Training-Batch No.6270\n",
      "Loss = 3.494382858276367\n",
      "Training-Batch No.6280\n",
      "Loss = 3.232651710510254\n",
      "Training-Batch No.6290\n",
      "Loss = 3.606731414794922\n",
      "Training-Batch No.6300\n",
      "Loss = 3.2007079124450684\n",
      "Training-Batch No.6310\n",
      "Loss = 3.683259963989258\n",
      "Training-Batch No.6320\n",
      "Loss = 4.0777177810668945\n",
      "Training-Batch No.6330\n",
      "Loss = 3.5658435821533203\n",
      "Training-Batch No.6340\n",
      "Loss = 3.583207368850708\n",
      "Training-Batch No.6350\n",
      "Loss = 4.403877258300781\n",
      "Training-Batch No.6360\n",
      "Loss = 3.257828712463379\n",
      "Training-Batch No.6370\n",
      "Loss = 3.103708267211914\n",
      "Training-Batch No.6380\n",
      "Loss = 3.1635119915008545\n",
      "Training-Batch No.6390\n",
      "Loss = 3.3256757259368896\n",
      "Training-Batch No.6400\n",
      "Loss = 3.7818093299865723\n",
      "Training-Batch No.6410\n",
      "Loss = 3.7441318035125732\n",
      "Training-Batch No.6420\n",
      "Loss = 3.7956693172454834\n",
      "Training-Batch No.6430\n",
      "Loss = 3.4737589359283447\n",
      "Training-Batch No.6440\n",
      "Loss = 3.5584821701049805\n",
      "Training-Batch No.6450\n",
      "Loss = 3.20774507522583\n",
      "Training-Batch No.6460\n",
      "Loss = 3.000898599624634\n",
      "Training-Batch No.6470\n",
      "Loss = 3.4053616523742676\n",
      "Training-Batch No.6480\n",
      "Loss = 3.5543160438537598\n",
      "Training-Batch No.6490\n",
      "Loss = 3.5079550743103027\n",
      "Training-Batch No.6500\n",
      "Loss = 3.555424928665161\n",
      "Training-Batch No.6510\n",
      "Loss = 3.7343125343322754\n",
      "Epoch 7 Training Loss: 3.6375\n",
      "Start Validating\n",
      "Epoch 7 Validation Loss: 3.6304\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.36478494623655916\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.4622311827956989\n",
      "Average Top-11 accuracy 0.5068548387096774\n",
      "Average Top-13 accuracy 0.5439516129032258\n",
      "Average Top-15 accuracy 0.5865591397849462\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 8\n",
      "Training-Batch No.6520\n",
      "Loss = 3.7686197757720947\n",
      "Training-Batch No.6530\n",
      "Loss = 3.0599420070648193\n",
      "Training-Batch No.6540\n",
      "Loss = 3.7805020809173584\n",
      "Training-Batch No.6550\n",
      "Loss = 4.091902256011963\n",
      "Training-Batch No.6560\n",
      "Loss = 3.2513198852539062\n",
      "Training-Batch No.6570\n",
      "Loss = 4.180593013763428\n",
      "Training-Batch No.6580\n",
      "Loss = 4.023665428161621\n",
      "Training-Batch No.6590\n",
      "Loss = 3.8712263107299805\n",
      "Training-Batch No.6600\n",
      "Loss = 4.041176795959473\n",
      "Training-Batch No.6610\n",
      "Loss = 3.5926578044891357\n",
      "Training-Batch No.6620\n",
      "Loss = 3.5733542442321777\n",
      "Training-Batch No.6630\n",
      "Loss = 3.6378352642059326\n",
      "Training-Batch No.6640\n",
      "Loss = 3.832456588745117\n",
      "Training-Batch No.6650\n",
      "Loss = 3.7507102489471436\n",
      "Training-Batch No.6660\n",
      "Loss = 4.164766788482666\n",
      "Training-Batch No.6670\n",
      "Loss = 3.511758804321289\n",
      "Training-Batch No.6680\n",
      "Loss = 3.284703493118286\n",
      "Training-Batch No.6690\n",
      "Loss = 4.136188507080078\n",
      "Training-Batch No.6700\n",
      "Loss = 3.838259220123291\n",
      "Training-Batch No.6710\n",
      "Loss = 3.593113660812378\n",
      "Training-Batch No.6720\n",
      "Loss = 3.9287004470825195\n",
      "Training-Batch No.6730\n",
      "Loss = 3.408202648162842\n",
      "Training-Batch No.6740\n",
      "Loss = 3.931169271469116\n",
      "Training-Batch No.6750\n",
      "Loss = 4.219539165496826\n",
      "Training-Batch No.6760\n",
      "Loss = 3.564162492752075\n",
      "Training-Batch No.6770\n",
      "Loss = 3.1656179428100586\n",
      "Training-Batch No.6780\n",
      "Loss = 3.762948513031006\n",
      "Training-Batch No.6790\n",
      "Loss = 3.6166234016418457\n",
      "Training-Batch No.6800\n",
      "Loss = 3.3627474308013916\n",
      "Training-Batch No.6810\n",
      "Loss = 3.213967800140381\n",
      "Training-Batch No.6820\n",
      "Loss = 3.818424701690674\n",
      "Training-Batch No.6830\n",
      "Loss = 3.7823798656463623\n",
      "Training-Batch No.6840\n",
      "Loss = 3.932055711746216\n",
      "Training-Batch No.6850\n",
      "Loss = 3.1810359954833984\n",
      "Training-Batch No.6860\n",
      "Loss = 3.478548526763916\n",
      "Training-Batch No.6870\n",
      "Loss = 3.934732437133789\n",
      "Training-Batch No.6880\n",
      "Loss = 3.938345193862915\n",
      "Training-Batch No.6890\n",
      "Loss = 3.767443895339966\n",
      "Training-Batch No.6900\n",
      "Loss = 3.6399428844451904\n",
      "Training-Batch No.6910\n",
      "Loss = 3.8787782192230225\n",
      "Training-Batch No.6920\n",
      "Loss = 3.3119044303894043\n",
      "Training-Batch No.6930\n",
      "Loss = 2.9505958557128906\n",
      "Training-Batch No.6940\n",
      "Loss = 3.701035976409912\n",
      "Training-Batch No.6950\n",
      "Loss = 3.8568673133850098\n",
      "Training-Batch No.6960\n",
      "Loss = 3.7249181270599365\n",
      "Training-Batch No.6970\n",
      "Loss = 2.7682857513427734\n",
      "Training-Batch No.6980\n",
      "Loss = 3.862529754638672\n",
      "Training-Batch No.6990\n",
      "Loss = 3.3204922676086426\n",
      "Training-Batch No.7000\n",
      "Loss = 3.1386473178863525\n",
      "Training-Batch No.7010\n",
      "Loss = 3.0202784538269043\n",
      "Training-Batch No.7020\n",
      "Loss = 3.7765090465545654\n",
      "Training-Batch No.7030\n",
      "Loss = 3.614776134490967\n",
      "Training-Batch No.7040\n",
      "Loss = 3.6924192905426025\n",
      "Training-Batch No.7050\n",
      "Loss = 3.2574217319488525\n",
      "Training-Batch No.7060\n",
      "Loss = 3.403593063354492\n",
      "Training-Batch No.7070\n",
      "Loss = 3.8986663818359375\n",
      "Training-Batch No.7080\n",
      "Loss = 3.1032540798187256\n",
      "Training-Batch No.7090\n",
      "Loss = 3.894442081451416\n",
      "Training-Batch No.7100\n",
      "Loss = 3.6277077198028564\n",
      "Training-Batch No.7110\n",
      "Loss = 3.7729663848876953\n",
      "Training-Batch No.7120\n",
      "Loss = 3.880612373352051\n",
      "Training-Batch No.7130\n",
      "Loss = 3.320746898651123\n",
      "Training-Batch No.7140\n",
      "Loss = 3.708606719970703\n",
      "Training-Batch No.7150\n",
      "Loss = 3.7023813724517822\n",
      "Training-Batch No.7160\n",
      "Loss = 3.6216366291046143\n",
      "Training-Batch No.7170\n",
      "Loss = 3.572495937347412\n",
      "Training-Batch No.7180\n",
      "Loss = 3.5059032440185547\n",
      "Training-Batch No.7190\n",
      "Loss = 3.6799421310424805\n",
      "Training-Batch No.7200\n",
      "Loss = 3.4946203231811523\n",
      "Training-Batch No.7210\n",
      "Loss = 3.2329821586608887\n",
      "Training-Batch No.7220\n",
      "Loss = 3.606768846511841\n",
      "Training-Batch No.7230\n",
      "Loss = 3.2015929222106934\n",
      "Training-Batch No.7240\n",
      "Loss = 3.683608055114746\n",
      "Training-Batch No.7250\n",
      "Loss = 4.077719211578369\n",
      "Training-Batch No.7260\n",
      "Loss = 3.5654590129852295\n",
      "Training-Batch No.7270\n",
      "Loss = 3.5833969116210938\n",
      "Training-Batch No.7280\n",
      "Loss = 4.400804042816162\n",
      "Training-Batch No.7290\n",
      "Loss = 3.2587900161743164\n",
      "Training-Batch No.7300\n",
      "Loss = 3.105498790740967\n",
      "Training-Batch No.7310\n",
      "Loss = 3.164576768875122\n",
      "Training-Batch No.7320\n",
      "Loss = 3.3273041248321533\n",
      "Training-Batch No.7330\n",
      "Loss = 3.781008243560791\n",
      "Training-Batch No.7340\n",
      "Loss = 3.7438511848449707\n",
      "Training-Batch No.7350\n",
      "Loss = 3.795377254486084\n",
      "Training-Batch No.7360\n",
      "Loss = 3.4739348888397217\n",
      "Training-Batch No.7370\n",
      "Loss = 3.560256004333496\n",
      "Training-Batch No.7380\n",
      "Loss = 3.2076873779296875\n",
      "Training-Batch No.7390\n",
      "Loss = 3.0025007724761963\n",
      "Training-Batch No.7400\n",
      "Loss = 3.4057741165161133\n",
      "Training-Batch No.7410\n",
      "Loss = 3.5542125701904297\n",
      "Training-Batch No.7420\n",
      "Loss = 3.509321451187134\n",
      "Training-Batch No.7430\n",
      "Loss = 3.5558290481567383\n",
      "Training-Batch No.7440\n",
      "Loss = 3.7343156337738037\n",
      "Epoch 8 Training Loss: 3.6374\n",
      "Start Validating\n",
      "Epoch 8 Validation Loss: 3.6303\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.36478494623655916\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.4622311827956989\n",
      "Average Top-11 accuracy 0.5068548387096774\n",
      "Average Top-13 accuracy 0.5439516129032258\n",
      "Average Top-15 accuracy 0.5865591397849462\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 9\n",
      "Training-Batch No.7450\n",
      "Loss = 3.7686781883239746\n",
      "Training-Batch No.7460\n",
      "Loss = 3.0617685317993164\n",
      "Training-Batch No.7470\n",
      "Loss = 3.775766611099243\n",
      "Training-Batch No.7480\n",
      "Loss = 4.097356796264648\n",
      "Training-Batch No.7490\n",
      "Loss = 3.254255533218384\n",
      "Training-Batch No.7500\n",
      "Loss = 4.182523727416992\n",
      "Training-Batch No.7510\n",
      "Loss = 4.015685558319092\n",
      "Training-Batch No.7520\n",
      "Loss = 3.864283323287964\n",
      "Training-Batch No.7530\n",
      "Loss = 4.040259838104248\n",
      "Training-Batch No.7540\n",
      "Loss = 3.5824384689331055\n",
      "Training-Batch No.7550\n",
      "Loss = 3.5755701065063477\n",
      "Training-Batch No.7560\n",
      "Loss = 3.638458728790283\n",
      "Training-Batch No.7570\n",
      "Loss = 3.8429136276245117\n",
      "Training-Batch No.7580\n",
      "Loss = 3.761176109313965\n",
      "Training-Batch No.7590\n",
      "Loss = 4.152955532073975\n",
      "Training-Batch No.7600\n",
      "Loss = 3.52359676361084\n",
      "Training-Batch No.7610\n",
      "Loss = 3.2737114429473877\n",
      "Training-Batch No.7620\n",
      "Loss = 4.121726989746094\n",
      "Training-Batch No.7630\n",
      "Loss = 3.840096950531006\n",
      "Training-Batch No.7640\n",
      "Loss = 3.604151964187622\n",
      "Training-Batch No.7650\n",
      "Loss = 3.917390823364258\n",
      "Training-Batch No.7660\n",
      "Loss = 3.402521848678589\n",
      "Training-Batch No.7670\n",
      "Loss = 3.9421393871307373\n",
      "Training-Batch No.7680\n",
      "Loss = 4.205804824829102\n",
      "Training-Batch No.7690\n",
      "Loss = 3.5677404403686523\n",
      "Training-Batch No.7700\n",
      "Loss = 3.180824041366577\n",
      "Training-Batch No.7710\n",
      "Loss = 3.7697646617889404\n",
      "Training-Batch No.7720\n",
      "Loss = 3.6036736965179443\n",
      "Training-Batch No.7730\n",
      "Loss = 3.3630001544952393\n",
      "Training-Batch No.7740\n",
      "Loss = 3.223217010498047\n",
      "Training-Batch No.7750\n",
      "Loss = 3.8186545372009277\n",
      "Training-Batch No.7760\n",
      "Loss = 3.7851991653442383\n",
      "Training-Batch No.7770\n",
      "Loss = 3.9116432666778564\n",
      "Training-Batch No.7780\n",
      "Loss = 3.1956470012664795\n",
      "Training-Batch No.7790\n",
      "Loss = 3.477592945098877\n",
      "Training-Batch No.7800\n",
      "Loss = 3.9098455905914307\n",
      "Training-Batch No.7810\n",
      "Loss = 3.916578769683838\n",
      "Training-Batch No.7820\n",
      "Loss = 3.7589166164398193\n",
      "Training-Batch No.7830\n",
      "Loss = 3.634572982788086\n",
      "Training-Batch No.7840\n",
      "Loss = 3.875872850418091\n",
      "Training-Batch No.7850\n",
      "Loss = 3.317466974258423\n",
      "Training-Batch No.7860\n",
      "Loss = 2.951502799987793\n",
      "Training-Batch No.7870\n",
      "Loss = 3.6890549659729004\n",
      "Training-Batch No.7880\n",
      "Loss = 3.8449530601501465\n",
      "Training-Batch No.7890\n",
      "Loss = 3.710641622543335\n",
      "Training-Batch No.7900\n",
      "Loss = 2.7921671867370605\n",
      "Training-Batch No.7910\n",
      "Loss = 3.853386878967285\n",
      "Training-Batch No.7920\n",
      "Loss = 3.3358843326568604\n",
      "Training-Batch No.7930\n",
      "Loss = 3.1524498462677\n",
      "Training-Batch No.7940\n",
      "Loss = 3.034233331680298\n",
      "Training-Batch No.7950\n",
      "Loss = 3.777381658554077\n",
      "Training-Batch No.7960\n",
      "Loss = 3.610548973083496\n",
      "Training-Batch No.7970\n",
      "Loss = 3.666241407394409\n",
      "Training-Batch No.7980\n",
      "Loss = 3.263054370880127\n",
      "Training-Batch No.7990\n",
      "Loss = 3.401447057723999\n",
      "Training-Batch No.8000\n",
      "Loss = 3.9065423011779785\n",
      "Training-Batch No.8010\n",
      "Loss = 3.1310665607452393\n",
      "Training-Batch No.8020\n",
      "Loss = 3.8845160007476807\n",
      "Training-Batch No.8030\n",
      "Loss = 3.6172542572021484\n",
      "Training-Batch No.8040\n",
      "Loss = 3.7677009105682373\n",
      "Training-Batch No.8050\n",
      "Loss = 3.8857953548431396\n",
      "Training-Batch No.8060\n",
      "Loss = 3.3258421421051025\n",
      "Training-Batch No.8070\n",
      "Loss = 3.702267646789551\n",
      "Training-Batch No.8080\n",
      "Loss = 3.707932710647583\n",
      "Training-Batch No.8090\n",
      "Loss = 3.625164031982422\n",
      "Training-Batch No.8100\n",
      "Loss = 3.584103584289551\n",
      "Training-Batch No.8110\n",
      "Loss = 3.5177841186523438\n",
      "Training-Batch No.8120\n",
      "Loss = 3.6721553802490234\n",
      "Training-Batch No.8130\n",
      "Loss = 3.4942591190338135\n",
      "Training-Batch No.8140\n",
      "Loss = 3.2388622760772705\n",
      "Training-Batch No.8150\n",
      "Loss = 3.602234125137329\n",
      "Training-Batch No.8160\n",
      "Loss = 3.2025930881500244\n",
      "Training-Batch No.8170\n",
      "Loss = 3.664872169494629\n",
      "Training-Batch No.8180\n",
      "Loss = 4.0610246658325195\n",
      "Training-Batch No.8190\n",
      "Loss = 3.5738914012908936\n",
      "Training-Batch No.8200\n",
      "Loss = 3.58565354347229\n",
      "Training-Batch No.8210\n",
      "Loss = 4.410439491271973\n",
      "Training-Batch No.8220\n",
      "Loss = 3.264200448989868\n",
      "Training-Batch No.8230\n",
      "Loss = 3.0874698162078857\n",
      "Training-Batch No.8240\n",
      "Loss = 3.1487228870391846\n",
      "Training-Batch No.8250\n",
      "Loss = 3.30924916267395\n",
      "Training-Batch No.8260\n",
      "Loss = 3.7809908390045166\n",
      "Training-Batch No.8270\n",
      "Loss = 3.72450852394104\n",
      "Training-Batch No.8280\n",
      "Loss = 3.782365083694458\n",
      "Training-Batch No.8290\n",
      "Loss = 3.470396041870117\n",
      "Training-Batch No.8300\n",
      "Loss = 3.5523924827575684\n",
      "Training-Batch No.8310\n",
      "Loss = 3.1984243392944336\n",
      "Training-Batch No.8320\n",
      "Loss = 2.9905970096588135\n",
      "Training-Batch No.8330\n",
      "Loss = 3.3970346450805664\n",
      "Training-Batch No.8340\n",
      "Loss = 3.5463953018188477\n",
      "Training-Batch No.8350\n",
      "Loss = 3.5007119178771973\n",
      "Training-Batch No.8360\n",
      "Loss = 3.551583766937256\n",
      "Training-Batch No.8370\n",
      "Loss = 3.734846591949463\n",
      "Epoch 9 Training Loss: 3.6355\n",
      "Start Validating\n",
      "Epoch 9 Validation Loss: 3.6298\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.36478494623655916\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.4622311827956989\n",
      "Average Top-11 accuracy 0.5068548387096774\n",
      "Average Top-13 accuracy 0.5439516129032258\n",
      "Average Top-15 accuracy 0.5865591397849462\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n",
      "Epoch No. 10\n",
      "Training-Batch No.8380\n",
      "Loss = 3.7728068828582764\n",
      "Training-Batch No.8390\n",
      "Loss = 3.0518076419830322\n",
      "Training-Batch No.8400\n",
      "Loss = 3.7788405418395996\n",
      "Training-Batch No.8410\n",
      "Loss = 4.104429244995117\n",
      "Training-Batch No.8420\n",
      "Loss = 3.247432231903076\n",
      "Training-Batch No.8430\n",
      "Loss = 4.186681747436523\n",
      "Training-Batch No.8440\n",
      "Loss = 4.019311904907227\n",
      "Training-Batch No.8450\n",
      "Loss = 3.8690500259399414\n",
      "Training-Batch No.8460\n",
      "Loss = 4.042785167694092\n",
      "Training-Batch No.8470\n",
      "Loss = 3.584703207015991\n",
      "Training-Batch No.8480\n",
      "Loss = 3.572587251663208\n",
      "Training-Batch No.8490\n",
      "Loss = 3.638195276260376\n",
      "Training-Batch No.8500\n",
      "Loss = 3.842862367630005\n",
      "Training-Batch No.8510\n",
      "Loss = 3.7609870433807373\n",
      "Training-Batch No.8520\n",
      "Loss = 4.159419536590576\n",
      "Training-Batch No.8530\n",
      "Loss = 3.520721197128296\n",
      "Training-Batch No.8540\n",
      "Loss = 3.2693753242492676\n",
      "Training-Batch No.8550\n",
      "Loss = 4.127245903015137\n",
      "Training-Batch No.8560\n",
      "Loss = 3.8405601978302\n",
      "Training-Batch No.8570\n",
      "Loss = 3.6011736392974854\n",
      "Training-Batch No.8580\n",
      "Loss = 3.920362949371338\n",
      "Training-Batch No.8590\n",
      "Loss = 3.39980149269104\n",
      "Training-Batch No.8600\n",
      "Loss = 3.9420576095581055\n",
      "Training-Batch No.8610\n",
      "Loss = 4.213236331939697\n",
      "Training-Batch No.8620\n",
      "Loss = 3.5680437088012695\n",
      "Training-Batch No.8630\n",
      "Loss = 3.174574136734009\n",
      "Training-Batch No.8640\n",
      "Loss = 3.7675576210021973\n",
      "Training-Batch No.8650\n",
      "Loss = 3.603581190109253\n",
      "Training-Batch No.8660\n",
      "Loss = 3.360703945159912\n",
      "Training-Batch No.8670\n",
      "Loss = 3.2167108058929443\n",
      "Training-Batch No.8680\n",
      "Loss = 3.8202178478240967\n",
      "Training-Batch No.8690\n",
      "Loss = 3.784747838973999\n",
      "Training-Batch No.8700\n",
      "Loss = 3.9155044555664062\n",
      "Training-Batch No.8710\n",
      "Loss = 3.187700033187866\n",
      "Training-Batch No.8720\n",
      "Loss = 3.4752657413482666\n",
      "Training-Batch No.8730\n",
      "Loss = 3.9121878147125244\n",
      "Training-Batch No.8740\n",
      "Loss = 3.922168493270874\n",
      "Training-Batch No.8750\n",
      "Loss = 3.7602927684783936\n",
      "Training-Batch No.8760\n",
      "Loss = 3.635117530822754\n",
      "Training-Batch No.8770\n",
      "Loss = 3.8783419132232666\n",
      "Training-Batch No.8780\n",
      "Loss = 3.312547206878662\n",
      "Training-Batch No.8790\n",
      "Loss = 2.9424023628234863\n",
      "Training-Batch No.8800\n",
      "Loss = 3.6908750534057617\n",
      "Training-Batch No.8810\n",
      "Loss = 3.8491594791412354\n",
      "Training-Batch No.8820\n",
      "Loss = 3.7118756771087646\n",
      "Training-Batch No.8830\n",
      "Loss = 2.7793827056884766\n",
      "Training-Batch No.8840\n",
      "Loss = 3.854788064956665\n",
      "Training-Batch No.8850\n",
      "Loss = 3.3300046920776367\n",
      "Training-Batch No.8860\n",
      "Loss = 3.1457319259643555\n",
      "Training-Batch No.8870\n",
      "Loss = 3.026644229888916\n",
      "Training-Batch No.8880\n",
      "Loss = 3.7788572311401367\n",
      "Training-Batch No.8890\n",
      "Loss = 3.6101436614990234\n",
      "Training-Batch No.8900\n",
      "Loss = 3.6691243648529053\n",
      "Training-Batch No.8910\n",
      "Loss = 3.2583348751068115\n",
      "Training-Batch No.8920\n",
      "Loss = 3.399674654006958\n",
      "Training-Batch No.8930\n",
      "Loss = 3.9091413021087646\n",
      "Training-Batch No.8940\n",
      "Loss = 3.1227259635925293\n",
      "Training-Batch No.8950\n",
      "Loss = 3.8875577449798584\n",
      "Training-Batch No.8960\n",
      "Loss = 3.6171939373016357\n",
      "Training-Batch No.8970\n",
      "Loss = 3.7688190937042236\n",
      "Training-Batch No.8980\n",
      "Loss = 3.8872435092926025\n",
      "Training-Batch No.8990\n",
      "Loss = 3.3224005699157715\n",
      "Training-Batch No.9000\n",
      "Loss = 3.7037456035614014\n",
      "Training-Batch No.9010\n",
      "Loss = 3.707475423812866\n",
      "Training-Batch No.9020\n",
      "Loss = 3.6249032020568848\n",
      "Training-Batch No.9030\n",
      "Loss = 3.581732749938965\n",
      "Training-Batch No.9040\n",
      "Loss = 3.5138657093048096\n",
      "Training-Batch No.9050\n",
      "Loss = 3.6737022399902344\n",
      "Training-Batch No.9060\n",
      "Loss = 3.492455005645752\n",
      "Training-Batch No.9070\n",
      "Loss = 3.2339954376220703\n",
      "Training-Batch No.9080\n",
      "Loss = 3.6020467281341553\n",
      "Training-Batch No.9090\n",
      "Loss = 3.197934150695801\n",
      "Training-Batch No.9100\n",
      "Loss = 3.665567636489868\n",
      "Training-Batch No.9110\n",
      "Loss = 4.066298961639404\n",
      "Training-Batch No.9120\n",
      "Loss = 3.5725226402282715\n",
      "Training-Batch No.9130\n",
      "Loss = 3.5839266777038574\n",
      "Training-Batch No.9140\n",
      "Loss = 4.416646957397461\n",
      "Training-Batch No.9150\n",
      "Loss = 3.260066270828247\n",
      "Training-Batch No.9160\n",
      "Loss = 3.0837507247924805\n",
      "Training-Batch No.9170\n",
      "Loss = 3.1448614597320557\n",
      "Training-Batch No.9180\n",
      "Loss = 3.3072597980499268\n",
      "Training-Batch No.9190\n",
      "Loss = 3.782952308654785\n",
      "Training-Batch No.9200\n",
      "Loss = 3.7280688285827637\n",
      "Training-Batch No.9210\n",
      "Loss = 3.7849671840667725\n",
      "Training-Batch No.9220\n",
      "Loss = 3.4685962200164795\n",
      "Training-Batch No.9230\n",
      "Loss = 3.5524117946624756\n",
      "Training-Batch No.9240\n",
      "Loss = 3.1947145462036133\n",
      "Training-Batch No.9250\n",
      "Loss = 2.98466157913208\n",
      "Training-Batch No.9260\n",
      "Loss = 3.395639419555664\n",
      "Training-Batch No.9270\n",
      "Loss = 3.5462770462036133\n",
      "Training-Batch No.9280\n",
      "Loss = 3.499401092529297\n",
      "Training-Batch No.9290\n",
      "Loss = 3.55145525932312\n",
      "Training-Batch No.9300\n",
      "Loss = 3.7355573177337646\n",
      "Epoch 10 Training Loss: 3.6352\n",
      "Start Validating\n",
      "Epoch 10 Validation Loss: 3.6296\n",
      "total training examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.36478494623655916\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.4622311827956989\n",
      "Average Top-11 accuracy 0.5068548387096774\n",
      "Average Top-13 accuracy 0.5478494623655914\n",
      "Average Top-15 accuracy 0.5865591397849462\n",
      "current acc 0.18508064516129033\n",
      "best acc 0.18508064516129033\n",
      "updated best acc 0.18508064516129033\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "# Training and Validation!\n",
    "\n",
    "with cuda.device(0):\n",
    "\n",
    "    acc_loss = 0\n",
    "    itr = []\n",
    "    for idx, n in enumerate(train_size):\n",
    "        print('```````````````````````````````````````````````````````')\n",
    "        print('Training size is {}'.format(n))\n",
    "\n",
    "        net = PointTransformerV3(num_classes)\n",
    "        net = net.cuda()\n",
    "\n",
    "        #  Optimization parameters:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        opt = optimizer.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "        LR_sch = optimizer.lr_scheduler.MultiStepLR(opt, [4,8,12], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "        count = 0\n",
    "        running_loss = []\n",
    "        running_top1_acc = []\n",
    "        running_top3_acc = []\n",
    "        running_top5_acc = []\n",
    "        running_top7_acc = []\n",
    "        running_top9_acc = []\n",
    "        running_top11_acc = []\n",
    "        running_top13_acc = []\n",
    "        running_top15_acc = []\n",
    "\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch No. ' + str(epoch + 1))\n",
    "            skipped_batches = 0\n",
    "            epoch_train_loss = 0  # To track the training loss for the epoch # Added\n",
    "            for tr_count, (img, label) in enumerate(train_loader):\n",
    "                net.train()\n",
    "                x = img.cuda()\n",
    "                opt.zero_grad()\n",
    "                label = label.cuda()\n",
    "                # print(x.shape)\n",
    "                # x = torch.transpose(x, 1, 2)\n",
    "                out = net.forward(x)\n",
    "                L = criterion(out, label)\n",
    "                L.backward()\n",
    "                opt.step()\n",
    "                batch_loss = L.item()\n",
    "                acc_loss += batch_loss\n",
    "                epoch_train_loss += batch_loss  # Accumulate batch loss for the epoch # Added\n",
    "                count += 1\n",
    "                if np.mod(count, 10) == 0:\n",
    "                    print('Training-Batch No.' + str(count))\n",
    "                    running_loss.append(batch_loss)\n",
    "                    itr.append(count)\n",
    "                    print('Loss = ' + str(running_loss[-1]))\n",
    "\n",
    "            epoch_train_loss /= len(train_loader)  # Calculate average training loss for the epoch # Added\n",
    "            print(f'Epoch {epoch + 1} Training Loss: {epoch_train_loss:.4f}') # Added\n",
    "            \n",
    "            print('Start Training and Validation')\n",
    "            ave_top1_acc = 0\n",
    "            ave_top3_acc = 0\n",
    "            ave_top5_acc = 0\n",
    "            ave_top7_acc = 0\n",
    "            ave_top9_acc = 0\n",
    "            ave_top11_acc = 0\n",
    "            ave_top13_acc = 0\n",
    "            ave_top15_acc = 0\n",
    "            val_loss = 0  # To track the validation loss # Added\n",
    "            ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "            top1_pred_out = []\n",
    "            top3_pred_out = []\n",
    "            top5_pred_out = []\n",
    "            top7_pred_out = []\n",
    "            top9_pred_out = []\n",
    "            top11_pred_out = []\n",
    "            top13_pred_out = []\n",
    "            top15_pred_out = []\n",
    "            gt_beam = []\n",
    "            total_count = 0\n",
    "\n",
    "            for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "                net.eval()\n",
    "                x = imgs.cuda()\n",
    "                opt.zero_grad()\n",
    "                labels = labels.cuda()\n",
    "                total_count += labels.size(0)\n",
    "                out = net.forward(x)\n",
    "                _, top_1_pred = t.max(out, dim=1)\n",
    "                \n",
    "                val_batch_loss = criterion(out, labels).item()  # Calculate validation loss for the batch # Added\n",
    "                val_loss += val_batch_loss  # Accumulate batch loss for the epoch # Added\n",
    "\n",
    "                gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "\n",
    "                top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "                sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "                top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "                top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "                top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "                top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "                top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "                top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "                top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "                top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "                tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "                tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "                tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "                tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "                tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "                tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "                tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "                batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "                batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "                batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "                batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "                batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "                batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "                batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "                batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "                ave_top1_acc += batch_top1_acc.item()\n",
    "                ave_top3_acc += batch_top3_acc.item()\n",
    "                ave_top5_acc += batch_top5_acc.item()\n",
    "                ave_top7_acc += batch_top7_acc.item()\n",
    "                ave_top9_acc += batch_top9_acc.item()\n",
    "                ave_top11_acc += batch_top11_acc.item()\n",
    "                ave_top13_acc += batch_top13_acc.item()\n",
    "                ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "            \n",
    "            val_loss /= len(val_loader)  # Calculate average validation loss for the epoch # Added\n",
    "            print(f'Epoch {epoch + 1} Validation Loss: {val_loss:.4f}') # Added\n",
    "            \n",
    "            print(\"total training examples are\", total_count)\n",
    "            running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "            running_top3_acc.append(ave_top3_acc / total_count)\n",
    "            running_top5_acc.append(ave_top5_acc / total_count)\n",
    "            running_top7_acc.append(ave_top7_acc / total_count)\n",
    "            running_top9_acc.append(ave_top9_acc / total_count)\n",
    "            running_top11_acc.append(ave_top11_acc / total_count)\n",
    "            running_top13_acc.append(ave_top13_acc / total_count)\n",
    "            running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "            print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "            print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "            print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "            print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "            print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "            print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "            print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "            print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "            print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "\n",
    "            cur_accuracy  = running_top1_acc[-1]\n",
    "\n",
    "            print(\"current acc\", cur_accuracy)\n",
    "            print(\"best acc\", best_accuracy)\n",
    "            if cur_accuracy > best_accuracy:\n",
    "                print(\"Saving the best model\")\n",
    "                net_name = checkpoint_directory  + '//' +  'lidar_model_64_beam'\n",
    "                t.save(net.state_dict(), net_name)\n",
    "                best_accuracy =  cur_accuracy\n",
    "            print(\"updated best acc\", best_accuracy)\n",
    "\n",
    "\n",
    "            print(\"Saving the predicted value in a csv file\")\n",
    "            file_to_save = f'{save_directory}//topk_pred_beam_val_after_{epoch+1}th_epoch.csv'\n",
    "            indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "            df1 = pd.DataFrame()\n",
    "            df1['index'] = indx\n",
    "            df1['link_status'] = gt_beam\n",
    "            df1['top1_pred'] = top1_pred_out\n",
    "            df1['top3_pred'] = top3_pred_out\n",
    "            df1['top5_pred'] = top5_pred_out\n",
    "            df1['top7_pred'] = top7_pred_out\n",
    "            df1['top9_pred'] = top9_pred_out\n",
    "            df1['top11_pred'] = top11_pred_out\n",
    "            df1['top13_pred'] = top13_pred_out\n",
    "            df1['top15_pred'] = top15_pred_out\n",
    "            df1.to_csv(file_to_save, index=False)\n",
    "\n",
    "            LR_sch.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aea51a6-0b85-49c3-b367-c0c8bd88de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n",
      "total examples are 7440\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18508064516129033\n",
      "Average Top-3 accuracy 0.29274193548387095\n",
      "Average Top-5 accuracy 0.36478494623655916\n",
      "Average Top-7 accuracy 0.41935483870967744\n",
      "Average Top-9 accuracy 0.4622311827956989\n",
      "Average Top-11 accuracy 0.5068548387096774\n",
      "Average Top-13 accuracy 0.5478494623655914\n",
      "Average Top-15 accuracy 0.5865591397849462\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "print('Start Validation')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "gt_beam = []\n",
    "total_count = 0\n",
    "            \n",
    "for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "    net.eval()\n",
    "    x = imgs.cuda()\n",
    "    opt.zero_grad()\n",
    "    labels = labels.cuda()\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "    \n",
    "print(\"total examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval.csv'\n",
    "\n",
    "indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = indx\n",
    "df2['link_status'] = gt_beam  # Add the link_status column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69a4d0-b733-4c4b-a379-7408a76836e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f3e1fdc-c04e-4502-bbd0-310c80c148a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "test_dir = './scenario36_64_lidar_beam_test.csv'\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(test_dir)\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "link_status_data = test_data['original_unit1_pwr3_best-beam'].tolist()\n",
    "org = test_data['original_index'].tolist()\n",
    "pwr_60ghz = test_data['original_unit1_pwr3'].tolist()\n",
    "\n",
    "# Load the model checkpoint and prepare for evaluation\n",
    "checkpoint_path = f'{checkpoint_directory}/lidar_model_64_beam'\n",
    "net.load_state_dict(t.load(checkpoint_path))\n",
    "net.eval()\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb2a5cde-b01c-465a-b536-465c35c79ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(LidarDataFeed(test_dir),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa199481-f9f5-4561-a24a-b9d44ff0a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n",
      "total test examples are 2480\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.18911290322580646\n",
      "Average Top-3 accuracy 0.29758064516129035\n",
      "Average Top-5 accuracy 0.3588709677419355\n",
      "Average Top-7 accuracy 0.42016129032258065\n",
      "Average Top-9 accuracy 0.4600806451612903\n",
      "Average Top-11 accuracy 0.5016129032258064\n",
      "Average Top-13 accuracy 0.5423387096774194\n",
      "Average Top-15 accuracy 0.5810483870967742\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "print('Start Testing')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "gt_beam = []\n",
    "total_count = 0\n",
    "\n",
    "for val_count, (imgs, labels) in enumerate(test_loader):\n",
    "    net.eval()\n",
    "    x = imgs.cuda()\n",
    "    opt.zero_grad()\n",
    "    labels = labels.cuda()\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total test examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval_Test.csv'\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "# link_status_data = test_data['original_unit1_pwr3_best-beam'].tolist()\n",
    "# org = test_data['original_index'].tolist()\n",
    "# pwr_60ghz = test_data['original_unit1_pwr3'].tolist()\n",
    "\n",
    "indx = test_data.index + 1\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = org\n",
    "df2['link_status'] = link_status_data  # Add the link_status column\n",
    "df2['original_unit1_pwr3'] = pwr_60ghz # Add the original_unit1_pwr3_60ghz column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
