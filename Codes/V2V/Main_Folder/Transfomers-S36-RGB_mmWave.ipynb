{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3b126-3016-4010-9c5f-7576d18c2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSense Scenario 36: 64 Beams: Transfomers - RGB - MaxViT!\n",
    "\n",
    "# Average Top-1 accuracy 0.267741935483871\n",
    "# Average Top-3 accuracy 0.4258064516129032\n",
    "# Average Top-5 accuracy 0.47943548387096774\n",
    "# Average Top-7 accuracy 0.5338709677419354\n",
    "# Average Top-9 accuracy 0.5689516129032258\n",
    "# Average Top-11 accuracy 0.6125\n",
    "# Average Top-13 accuracy 0.6415322580645161\n",
    "# Average Top-15 accuracy 0.6754032258064516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115acb19-d2b4-48b5-94e4-d10eeed1017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d929190-d438-44d6-8b20-56b5bbb740bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "import torch\n",
    "import torch as t\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from torchsummary import summary\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transf\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455f2f9f-0fc1-44c1-ac55-007991776857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07-26-2024\n",
      "01_43\n",
      "C:\\Users\\Baqer\\Desktop\\V2X_CNN_All\\Scenario36_64-Beams\\Main_Folder//saved_folder//07-26-2024_01_43\n"
     ]
    }
   ],
   "source": [
    "# Save directory\n",
    "# year month day\n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds\n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)\n",
    "\n",
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime\n",
    "print(pwd)\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)\n",
    "\n",
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "    isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8184e634-0d79-4c0e-ab04-02d143f05d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Feeding: Create data sample list\n",
    "def create_samples(root, shuffle=False, nat_sort=False):\n",
    "    f = pd.read_csv(root)\n",
    "    data_samples = []\n",
    "    pred_val = []\n",
    "    for idx, row in f.iterrows():\n",
    "        img_paths = row.values[1:3]\n",
    "        data_samples.append(img_paths)\n",
    "    return data_samples\n",
    "\n",
    "class DataFeed(Dataset):\n",
    "    '''\n",
    "    A class retrieving a tuple of (image,label). It can handle the case\n",
    "    of empty classes (empty folders).\n",
    "    '''\n",
    "    def __init__(self,root_dir, nat_sort = False, transform=None, init_shuflle = True):\n",
    "        self.root = root_dir\n",
    "        self.samples = create_samples(self.root,shuffle=init_shuflle,nat_sort=nat_sort)\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.samples )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        img = io.imread(sample[0])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = sample[1]\n",
    "        return (img,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75aacbed-781f-4a96-b54d-6c55f5d7e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(root, shuffle=False, nat_sort=False):\n",
    "    f = pd.read_csv(root)\n",
    "    data_samples = []\n",
    "    pred_val = []\n",
    "    for idx, row in f.iterrows():\n",
    "        img_paths = row.values[1:3]\n",
    "        data_samples.append(img_paths)\n",
    "    return data_samples\n",
    "\n",
    "class DataFeed(Dataset):\n",
    "    def __init__(self, root_dir, nat_sort=False, transform=None, init_shuflle=True):\n",
    "        self.root = root_dir\n",
    "        self.samples = create_samples(self.root, shuffle=init_shuflle, nat_sort=nat_sort)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        img = io.imread(sample[0])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = sample[1]\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498924ed-5c12-47b0-bb8c-7c66ea2fa85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Testing, and Validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d70d23a-766f-4cd3-8f93-bea02ab2b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #8\n",
    "val_batch_size = 1\n",
    "lr = 1e-3\n",
    "decay = 1e-4\n",
    "num_epochs = 20 # After 16 epoch, the accuracy remains same!\n",
    "train_size = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629410df-f445-48a6-8782-45ef912c8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8ce4c-f3b9-4171-9888-b5ddfad46f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1448485e-33bf-48f7-b758-98d2e6c9bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'scenario36_64_img_beam_train.csv'\n",
    "val_dir = 'scenario36_64_img_beam_val.csv'\n",
    "\n",
    "train_loader = DataLoader(DataFeed(train_dir, transform=proc_pipe),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)\n",
    "val_loader = DataLoader(DataFeed(val_dir, transform=proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35acc26-7d6c-4033-8f82-c5660fc1f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 95% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% | 95% |\n"
     ]
    }
   ],
   "source": [
    "# !pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35acbb9-669c-4732-83d7-e501c13b0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf9c67c-51fb-48ab-8d0b-78e8fb76b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1c8a97-af58-465e-83f9-b30456b7576b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c989e835-e084-4b4b-a6e6-9a6cfa8dd0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2af4f1-4a1b-4cca-977f-5ef8aab6c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```````````````````````````````````````````````````````\n",
      "Training size is 1\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Stem: 1-1                              [-1, 64, 112, 112]        --\n",
      "|    └─Conv2dSame: 2-1                   [-1, 64, 112, 112]        1,792\n",
      "|    └─BatchNormAct2d: 2-2               [-1, 64, 112, 112]        --\n",
      "|    |    └─Identity: 3-1                [-1, 64, 112, 112]        --\n",
      "|    |    └─GELUTanh: 3-2                [-1, 64, 112, 112]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 64, 112, 112]        36,928\n",
      "├─Sequential: 1-2                        [-1, 512, 7, 7]           --\n",
      "|    └─MaxxVitStage: 2-4                 [-1, 64, 56, 56]          --\n",
      "|    |    └─Sequential: 3-3              [-1, 64, 56, 56]          290,792\n",
      "|    └─MaxxVitStage: 2-5                 [-1, 128, 28, 28]         --\n",
      "|    |    └─Sequential: 3-4              [-1, 128, 28, 28]         1,114,064\n",
      "|    └─MaxxVitStage: 2-6                 [-1, 256, 14, 14]         --\n",
      "|    |    └─Sequential: 3-5              [-1, 256, 14, 14]         11,165,456\n",
      "|    └─MaxxVitStage: 2-7                 [-1, 512, 7, 7]           --\n",
      "|    |    └─Sequential: 3-6              [-1, 512, 7, 7]           17,530,688\n",
      "├─Identity: 1-3                          [-1, 512, 7, 7]           --\n",
      "├─NormMlpClassifierHead: 1-4             [-1, 65]                  --\n",
      "|    └─SelectAdaptivePool2d: 2-8         [-1, 512, 1, 1]           --\n",
      "|    |    └─AdaptiveAvgPool2d: 3-7       [-1, 512, 1, 1]           --\n",
      "|    |    └─Identity: 3-8                [-1, 512, 1, 1]           --\n",
      "|    └─LayerNorm2d: 2-9                  [-1, 512, 1, 1]           1,024\n",
      "|    └─Flatten: 2-10                     [-1, 512]                 --\n",
      "|    └─Sequential: 2-11                  [-1, 512]                 --\n",
      "|    |    └─Linear: 3-9                  [-1, 512]                 262,656\n",
      "|    |    └─Tanh: 3-10                   [-1, 512]                 --\n",
      "|    └─Dropout: 2-12                     [-1, 512]                 --\n",
      "|    └─Linear: 2-13                      [-1, 65]                  33,345\n",
      "==========================================================================================\n",
      "Total params: 30,436,745\n",
      "Trainable params: 30,436,745\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 604.89\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 12.26\n",
      "Params size (MB): 116.11\n",
      "Estimated Total Size (MB): 128.94\n",
      "==========================================================================================\n",
      "Epoch No. 1\n",
      "Training-Batch No.10\n",
      "Loss = 4.104228496551514\n",
      "Training-Batch No.20\n",
      "Loss = 2.8245630264282227\n",
      "Training-Batch No.30\n",
      "Loss = 3.7894811630249023\n",
      "Training-Batch No.40\n",
      "Loss = 3.854398250579834\n",
      "Training-Batch No.50\n",
      "Loss = 2.9939301013946533\n",
      "Training-Batch No.60\n",
      "Loss = 4.262035846710205\n",
      "Training-Batch No.70\n",
      "Loss = 3.949500799179077\n",
      "Training-Batch No.80\n",
      "Loss = 3.4684207439422607\n",
      "Training-Batch No.90\n",
      "Loss = 3.8984487056732178\n",
      "Training-Batch No.100\n",
      "Loss = 3.6934189796447754\n",
      "Training-Batch No.110\n",
      "Loss = 3.2456588745117188\n",
      "Training-Batch No.120\n",
      "Loss = 3.8129870891571045\n",
      "Training-Batch No.130\n",
      "Loss = 3.6393983364105225\n",
      "Training-Batch No.140\n",
      "Loss = 3.2928738594055176\n",
      "Training-Batch No.150\n",
      "Loss = 4.105774879455566\n",
      "Training-Batch No.160\n",
      "Loss = 3.066126823425293\n",
      "Training-Batch No.170\n",
      "Loss = 2.9786012172698975\n",
      "Training-Batch No.180\n",
      "Loss = 4.035238742828369\n",
      "Training-Batch No.190\n",
      "Loss = 3.47878360748291\n",
      "Training-Batch No.200\n",
      "Loss = 3.4657647609710693\n",
      "Training-Batch No.210\n",
      "Loss = 3.8172554969787598\n",
      "Training-Batch No.220\n",
      "Loss = 3.049189567565918\n",
      "Training-Batch No.230\n",
      "Loss = 3.7482872009277344\n",
      "Training-Batch No.240\n",
      "Loss = 4.058207035064697\n",
      "Training-Batch No.250\n",
      "Loss = 3.3387417793273926\n",
      "Training-Batch No.260\n",
      "Loss = 2.5846123695373535\n",
      "Training-Batch No.270\n",
      "Loss = 3.395188331604004\n",
      "Training-Batch No.280\n",
      "Loss = 3.469778299331665\n",
      "Training-Batch No.290\n",
      "Loss = 3.1004955768585205\n",
      "Training-Batch No.300\n",
      "Loss = 2.5840227603912354\n",
      "Training-Batch No.310\n",
      "Loss = 3.4173941612243652\n",
      "Training-Batch No.320\n",
      "Loss = 3.5353636741638184\n",
      "Training-Batch No.330\n",
      "Loss = 4.056812286376953\n",
      "Training-Batch No.340\n",
      "Loss = 2.7402005195617676\n",
      "Training-Batch No.350\n",
      "Loss = 3.3214612007141113\n",
      "Training-Batch No.360\n",
      "Loss = 3.7195305824279785\n",
      "Training-Batch No.370\n",
      "Loss = 3.7542505264282227\n",
      "Training-Batch No.380\n",
      "Loss = 3.4566116333007812\n",
      "Training-Batch No.390\n",
      "Loss = 3.408267021179199\n",
      "Training-Batch No.400\n",
      "Loss = 3.4332942962646484\n",
      "Training-Batch No.410\n",
      "Loss = 2.8344063758850098\n",
      "Training-Batch No.420\n",
      "Loss = 2.977099657058716\n",
      "Training-Batch No.430\n",
      "Loss = 3.472827434539795\n",
      "Training-Batch No.440\n",
      "Loss = 3.5920350551605225\n",
      "Training-Batch No.450\n",
      "Loss = 3.297571897506714\n",
      "Training-Batch No.460\n",
      "Loss = 2.118709087371826\n",
      "Training-Batch No.470\n",
      "Loss = 3.7835655212402344\n",
      "Training-Batch No.480\n",
      "Loss = 2.999126434326172\n",
      "Training-Batch No.490\n",
      "Loss = 2.9598212242126465\n",
      "Training-Batch No.500\n",
      "Loss = 2.526982545852661\n",
      "Training-Batch No.510\n",
      "Loss = 3.530071258544922\n",
      "Training-Batch No.520\n",
      "Loss = 3.19650936126709\n",
      "Training-Batch No.530\n",
      "Loss = 3.3924949169158936\n",
      "Training-Batch No.540\n",
      "Loss = 3.024409532546997\n",
      "Training-Batch No.550\n",
      "Loss = 2.920175552368164\n",
      "Training-Batch No.560\n",
      "Loss = 3.2941627502441406\n",
      "Training-Batch No.570\n",
      "Loss = 2.6737773418426514\n",
      "Training-Batch No.580\n",
      "Loss = 3.847951889038086\n",
      "Training-Batch No.590\n",
      "Loss = 3.3160343170166016\n",
      "Training-Batch No.600\n",
      "Loss = 3.284235954284668\n",
      "Training-Batch No.610\n",
      "Loss = 3.5305168628692627\n",
      "Training-Batch No.620\n",
      "Loss = 3.070512533187866\n",
      "Training-Batch No.630\n",
      "Loss = 3.588175058364868\n",
      "Training-Batch No.640\n",
      "Loss = 3.428277015686035\n",
      "Training-Batch No.650\n",
      "Loss = 3.150704860687256\n",
      "Training-Batch No.660\n",
      "Loss = 3.38972806930542\n",
      "Training-Batch No.670\n",
      "Loss = 3.51609206199646\n",
      "Training-Batch No.680\n",
      "Loss = 3.1698904037475586\n",
      "Training-Batch No.690\n",
      "Loss = 3.0251476764678955\n",
      "Training-Batch No.700\n",
      "Loss = 2.8137638568878174\n",
      "Training-Batch No.710\n",
      "Loss = 3.405109167098999\n",
      "Training-Batch No.720\n",
      "Loss = 2.706291437149048\n",
      "Training-Batch No.730\n",
      "Loss = 3.3234848976135254\n",
      "Training-Batch No.740\n",
      "Loss = 3.916250467300415\n",
      "Training-Batch No.750\n",
      "Loss = 3.220715284347534\n",
      "Training-Batch No.760\n",
      "Loss = 3.134716033935547\n",
      "Training-Batch No.770\n",
      "Loss = 4.056872367858887\n",
      "Training-Batch No.780\n",
      "Loss = 2.7985141277313232\n",
      "Training-Batch No.790\n",
      "Loss = 2.7979190349578857\n",
      "Training-Batch No.800\n",
      "Loss = 2.5584630966186523\n",
      "Training-Batch No.810\n",
      "Loss = 2.8934926986694336\n",
      "Training-Batch No.820\n",
      "Loss = 3.4417572021484375\n",
      "Training-Batch No.830\n",
      "Loss = 3.5006563663482666\n",
      "Training-Batch No.840\n",
      "Loss = 3.279956102371216\n",
      "Training-Batch No.850\n",
      "Loss = 3.1095616817474365\n",
      "Training-Batch No.860\n",
      "Loss = 3.0839526653289795\n",
      "Training-Batch No.870\n",
      "Loss = 2.8858489990234375\n",
      "Training-Batch No.880\n",
      "Loss = 2.4019014835357666\n",
      "Training-Batch No.890\n",
      "Loss = 3.1123709678649902\n",
      "Training-Batch No.900\n",
      "Loss = 3.15664005279541\n",
      "Training-Batch No.910\n",
      "Loss = 3.071336269378662\n",
      "Training-Batch No.920\n",
      "Loss = 3.219386577606201\n",
      "Training-Batch No.930\n",
      "Loss = 3.5692050457000732\n",
      "Epoch 1 Training Loss: 3.3852\n",
      "Start validation\n",
      "Epoch 1 Validation Loss: 3.2523\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.20739247311827957\n",
      "Average Top-3 accuracy 0.34623655913978496\n",
      "Average Top-5 accuracy 0.4068548387096774\n",
      "Average Top-7 accuracy 0.455241935483871\n",
      "Average Top-9 accuracy 0.49905913978494626\n",
      "Average Top-11 accuracy 0.5387096774193548\n",
      "Average Top-13 accuracy 0.5719086021505376\n",
      "Average Top-15 accuracy 0.6116935483870968\n",
      "current acc 0.20739247311827957\n",
      "best acc 0\n",
      "Saving the best model\n",
      "updated best accuracy 0.20739247311827957\n",
      "Epoch No. 2\n",
      "Training-Batch No.940\n",
      "Loss = 3.4385221004486084\n",
      "Training-Batch No.950\n",
      "Loss = 2.3146116733551025\n",
      "Training-Batch No.960\n",
      "Loss = 3.402791976928711\n",
      "Training-Batch No.970\n",
      "Loss = 3.68939471244812\n",
      "Training-Batch No.980\n",
      "Loss = 2.8569653034210205\n",
      "Training-Batch No.990\n",
      "Loss = 3.8104746341705322\n",
      "Training-Batch No.1000\n",
      "Loss = 3.797816514968872\n",
      "Training-Batch No.1010\n",
      "Loss = 3.3410696983337402\n",
      "Training-Batch No.1020\n",
      "Loss = 4.121992111206055\n",
      "Training-Batch No.1030\n",
      "Loss = 3.448242664337158\n",
      "Training-Batch No.1040\n",
      "Loss = 3.0268852710723877\n",
      "Training-Batch No.1050\n",
      "Loss = 3.3914947509765625\n",
      "Training-Batch No.1060\n",
      "Loss = 3.3639614582061768\n",
      "Training-Batch No.1070\n",
      "Loss = 3.29819393157959\n",
      "Training-Batch No.1080\n",
      "Loss = 4.154541492462158\n",
      "Training-Batch No.1090\n",
      "Loss = 2.954700469970703\n",
      "Training-Batch No.1100\n",
      "Loss = 2.7479419708251953\n",
      "Training-Batch No.1110\n",
      "Loss = 3.858018159866333\n",
      "Training-Batch No.1120\n",
      "Loss = 3.352562665939331\n",
      "Training-Batch No.1130\n",
      "Loss = 3.4947760105133057\n",
      "Training-Batch No.1140\n",
      "Loss = 3.4128003120422363\n",
      "Training-Batch No.1150\n",
      "Loss = 2.9085288047790527\n",
      "Training-Batch No.1160\n",
      "Loss = 3.546072483062744\n",
      "Training-Batch No.1170\n",
      "Loss = 3.8462302684783936\n",
      "Training-Batch No.1180\n",
      "Loss = 2.9649136066436768\n",
      "Training-Batch No.1190\n",
      "Loss = 2.5278313159942627\n",
      "Training-Batch No.1200\n",
      "Loss = 3.281742572784424\n",
      "Training-Batch No.1210\n",
      "Loss = 3.3294477462768555\n",
      "Training-Batch No.1220\n",
      "Loss = 2.944579601287842\n",
      "Training-Batch No.1230\n",
      "Loss = 2.574397325515747\n",
      "Training-Batch No.1240\n",
      "Loss = 3.3865866661071777\n",
      "Training-Batch No.1250\n",
      "Loss = 3.3814022541046143\n",
      "Training-Batch No.1260\n",
      "Loss = 4.0326738357543945\n",
      "Training-Batch No.1270\n",
      "Loss = 2.605752944946289\n",
      "Training-Batch No.1280\n",
      "Loss = 3.2121992111206055\n",
      "Training-Batch No.1290\n",
      "Loss = 3.540278911590576\n",
      "Training-Batch No.1300\n",
      "Loss = 4.056015491485596\n",
      "Training-Batch No.1310\n",
      "Loss = 3.4624626636505127\n",
      "Training-Batch No.1320\n",
      "Loss = 3.238844871520996\n",
      "Training-Batch No.1330\n",
      "Loss = 3.6837198734283447\n",
      "Training-Batch No.1340\n",
      "Loss = 2.912722587585449\n",
      "Training-Batch No.1350\n",
      "Loss = 2.5933451652526855\n",
      "Training-Batch No.1360\n",
      "Loss = 3.3319196701049805\n",
      "Training-Batch No.1370\n",
      "Loss = 3.8520262241363525\n",
      "Training-Batch No.1380\n",
      "Loss = 3.3543190956115723\n",
      "Training-Batch No.1390\n",
      "Loss = 2.138425350189209\n",
      "Training-Batch No.1400\n",
      "Loss = 3.792686700820923\n",
      "Training-Batch No.1410\n",
      "Loss = 2.8478665351867676\n",
      "Training-Batch No.1420\n",
      "Loss = 2.934964179992676\n",
      "Training-Batch No.1430\n",
      "Loss = 2.473742961883545\n",
      "Training-Batch No.1440\n",
      "Loss = 3.3344149589538574\n",
      "Training-Batch No.1450\n",
      "Loss = 3.141829252243042\n",
      "Training-Batch No.1460\n",
      "Loss = 3.291248083114624\n",
      "Training-Batch No.1470\n",
      "Loss = 3.0154826641082764\n",
      "Training-Batch No.1480\n",
      "Loss = 2.90383243560791\n",
      "Training-Batch No.1490\n",
      "Loss = 3.3176872730255127\n",
      "Training-Batch No.1500\n",
      "Loss = 2.5206005573272705\n",
      "Training-Batch No.1510\n",
      "Loss = 3.5600197315216064\n",
      "Training-Batch No.1520\n",
      "Loss = 3.4216864109039307\n",
      "Training-Batch No.1530\n",
      "Loss = 3.2621657848358154\n",
      "Training-Batch No.1540\n",
      "Loss = 3.505901575088501\n",
      "Training-Batch No.1550\n",
      "Loss = 2.744319438934326\n",
      "Training-Batch No.1560\n",
      "Loss = 3.1584575176239014\n",
      "Training-Batch No.1570\n",
      "Loss = 3.3429017066955566\n",
      "Training-Batch No.1580\n",
      "Loss = 3.08225417137146\n",
      "Training-Batch No.1590\n",
      "Loss = 3.4213101863861084\n",
      "Training-Batch No.1600\n",
      "Loss = 3.2559666633605957\n",
      "Training-Batch No.1610\n",
      "Loss = 3.133932590484619\n",
      "Training-Batch No.1620\n",
      "Loss = 2.854292631149292\n",
      "Training-Batch No.1630\n",
      "Loss = 2.8452229499816895\n",
      "Training-Batch No.1640\n",
      "Loss = 3.355497121810913\n",
      "Training-Batch No.1650\n",
      "Loss = 2.592538356781006\n",
      "Training-Batch No.1660\n",
      "Loss = 3.237245559692383\n",
      "Training-Batch No.1670\n",
      "Loss = 3.8219454288482666\n",
      "Training-Batch No.1680\n",
      "Loss = 3.0349316596984863\n",
      "Training-Batch No.1690\n",
      "Loss = 2.991492748260498\n",
      "Training-Batch No.1700\n",
      "Loss = 4.0334272384643555\n",
      "Training-Batch No.1710\n",
      "Loss = 2.752241373062134\n",
      "Training-Batch No.1720\n",
      "Loss = 2.637345314025879\n",
      "Training-Batch No.1730\n",
      "Loss = 2.488436460494995\n",
      "Training-Batch No.1740\n",
      "Loss = 2.956449508666992\n",
      "Training-Batch No.1750\n",
      "Loss = 3.394737482070923\n",
      "Training-Batch No.1760\n",
      "Loss = 3.3125133514404297\n",
      "Training-Batch No.1770\n",
      "Loss = 3.2618155479431152\n",
      "Training-Batch No.1780\n",
      "Loss = 3.0408248901367188\n",
      "Training-Batch No.1790\n",
      "Loss = 3.1016721725463867\n",
      "Training-Batch No.1800\n",
      "Loss = 2.607398271560669\n",
      "Training-Batch No.1810\n",
      "Loss = 2.4099161624908447\n",
      "Training-Batch No.1820\n",
      "Loss = 2.987173557281494\n",
      "Training-Batch No.1830\n",
      "Loss = 3.1203818321228027\n",
      "Training-Batch No.1840\n",
      "Loss = 3.059131622314453\n",
      "Training-Batch No.1850\n",
      "Loss = 3.1573386192321777\n",
      "Training-Batch No.1860\n",
      "Loss = 3.3901498317718506\n",
      "Epoch 2 Training Loss: 3.2602\n",
      "Start validation\n",
      "Epoch 2 Validation Loss: 3.2140\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.20967741935483872\n",
      "Average Top-3 accuracy 0.348252688172043\n",
      "Average Top-5 accuracy 0.4190860215053763\n",
      "Average Top-7 accuracy 0.4676075268817204\n",
      "Average Top-9 accuracy 0.5067204301075269\n",
      "Average Top-11 accuracy 0.5424731182795699\n",
      "Average Top-13 accuracy 0.5805107526881721\n",
      "Average Top-15 accuracy 0.6118279569892473\n",
      "current acc 0.20967741935483872\n",
      "best acc 0.20739247311827957\n",
      "Saving the best model\n",
      "updated best accuracy 0.20967741935483872\n",
      "Epoch No. 3\n",
      "Training-Batch No.1870\n",
      "Loss = 3.424636125564575\n",
      "Training-Batch No.1880\n",
      "Loss = 2.4040586948394775\n",
      "Training-Batch No.1890\n",
      "Loss = 3.299966335296631\n",
      "Training-Batch No.1900\n",
      "Loss = 3.8622889518737793\n",
      "Training-Batch No.1910\n",
      "Loss = 2.8407201766967773\n",
      "Training-Batch No.1920\n",
      "Loss = 4.0438690185546875\n",
      "Training-Batch No.1930\n",
      "Loss = 3.724811553955078\n",
      "Training-Batch No.1940\n",
      "Loss = 3.21282696723938\n",
      "Training-Batch No.1950\n",
      "Loss = 3.749333381652832\n",
      "Training-Batch No.1960\n",
      "Loss = 3.369633436203003\n",
      "Training-Batch No.1970\n",
      "Loss = 2.971369981765747\n",
      "Training-Batch No.1980\n",
      "Loss = 3.126377582550049\n",
      "Training-Batch No.1990\n",
      "Loss = 3.276109457015991\n",
      "Training-Batch No.2000\n",
      "Loss = 3.1823017597198486\n",
      "Training-Batch No.2010\n",
      "Loss = 3.898909568786621\n",
      "Training-Batch No.2020\n",
      "Loss = 3.0332865715026855\n",
      "Training-Batch No.2030\n",
      "Loss = 2.6790711879730225\n",
      "Training-Batch No.2040\n",
      "Loss = 3.762429714202881\n",
      "Training-Batch No.2050\n",
      "Loss = 3.310840606689453\n",
      "Training-Batch No.2060\n",
      "Loss = 3.375544309616089\n",
      "Training-Batch No.2070\n",
      "Loss = 3.450960159301758\n",
      "Training-Batch No.2080\n",
      "Loss = 2.912933349609375\n",
      "Training-Batch No.2090\n",
      "Loss = 3.478172540664673\n",
      "Training-Batch No.2100\n",
      "Loss = 3.801877021789551\n",
      "Training-Batch No.2110\n",
      "Loss = 2.9838802814483643\n",
      "Training-Batch No.2120\n",
      "Loss = 2.4882731437683105\n",
      "Training-Batch No.2130\n",
      "Loss = 3.294034957885742\n",
      "Training-Batch No.2140\n",
      "Loss = 3.3470349311828613\n",
      "Training-Batch No.2150\n",
      "Loss = 2.7663819789886475\n",
      "Training-Batch No.2160\n",
      "Loss = 2.637906551361084\n",
      "Training-Batch No.2170\n",
      "Loss = 3.4991142749786377\n",
      "Training-Batch No.2180\n",
      "Loss = 3.417996883392334\n",
      "Training-Batch No.2190\n",
      "Loss = 3.6176743507385254\n",
      "Training-Batch No.2200\n",
      "Loss = 2.473155975341797\n",
      "Training-Batch No.2210\n",
      "Loss = 2.995307683944702\n",
      "Training-Batch No.2220\n",
      "Loss = 3.35347843170166\n",
      "Training-Batch No.2230\n",
      "Loss = 3.716085910797119\n",
      "Training-Batch No.2240\n",
      "Loss = 3.456325054168701\n",
      "Training-Batch No.2250\n",
      "Loss = 3.122894763946533\n",
      "Training-Batch No.2260\n",
      "Loss = 3.3607916831970215\n",
      "Training-Batch No.2270\n",
      "Loss = 2.696441650390625\n",
      "Training-Batch No.2280\n",
      "Loss = 2.3670871257781982\n",
      "Training-Batch No.2290\n",
      "Loss = 3.426339626312256\n",
      "Training-Batch No.2300\n",
      "Loss = 3.4955291748046875\n",
      "Training-Batch No.2310\n",
      "Loss = 3.124828338623047\n",
      "Training-Batch No.2320\n",
      "Loss = 2.107933521270752\n",
      "Training-Batch No.2330\n",
      "Loss = 3.946744918823242\n",
      "Training-Batch No.2340\n",
      "Loss = 2.807417869567871\n",
      "Training-Batch No.2350\n",
      "Loss = 2.8911430835723877\n",
      "Training-Batch No.2360\n",
      "Loss = 2.3578503131866455\n",
      "Training-Batch No.2370\n",
      "Loss = 3.2985846996307373\n",
      "Training-Batch No.2380\n",
      "Loss = 3.2495813369750977\n",
      "Training-Batch No.2390\n",
      "Loss = 3.2267558574676514\n",
      "Training-Batch No.2400\n",
      "Loss = 2.739985942840576\n",
      "Training-Batch No.2410\n",
      "Loss = 2.774968147277832\n",
      "Training-Batch No.2420\n",
      "Loss = 3.269010066986084\n",
      "Training-Batch No.2430\n",
      "Loss = 2.383305311203003\n",
      "Training-Batch No.2440\n",
      "Loss = 3.759244203567505\n",
      "Training-Batch No.2450\n",
      "Loss = 3.1259500980377197\n",
      "Training-Batch No.2460\n",
      "Loss = 3.1559715270996094\n",
      "Training-Batch No.2470\n",
      "Loss = 3.5232326984405518\n",
      "Training-Batch No.2480\n",
      "Loss = 2.8244974613189697\n",
      "Training-Batch No.2490\n",
      "Loss = 3.096221923828125\n",
      "Training-Batch No.2500\n",
      "Loss = 3.287132978439331\n",
      "Training-Batch No.2510\n",
      "Loss = 2.938018560409546\n",
      "Training-Batch No.2520\n",
      "Loss = 3.2573938369750977\n",
      "Training-Batch No.2530\n",
      "Loss = 3.4082183837890625\n",
      "Training-Batch No.2540\n",
      "Loss = 3.0879251956939697\n",
      "Training-Batch No.2550\n",
      "Loss = 2.8245749473571777\n",
      "Training-Batch No.2560\n",
      "Loss = 2.8225655555725098\n",
      "Training-Batch No.2570\n",
      "Loss = 3.0008111000061035\n",
      "Training-Batch No.2580\n",
      "Loss = 2.3629000186920166\n",
      "Training-Batch No.2590\n",
      "Loss = 3.182499408721924\n",
      "Training-Batch No.2600\n",
      "Loss = 3.713979482650757\n",
      "Training-Batch No.2610\n",
      "Loss = 3.089080333709717\n",
      "Training-Batch No.2620\n",
      "Loss = 3.0293233394622803\n",
      "Training-Batch No.2630\n",
      "Loss = 4.055976390838623\n",
      "Training-Batch No.2640\n",
      "Loss = 2.771008253097534\n",
      "Training-Batch No.2650\n",
      "Loss = 2.5890440940856934\n",
      "Training-Batch No.2660\n",
      "Loss = 2.516256809234619\n",
      "Training-Batch No.2670\n",
      "Loss = 2.752441644668579\n",
      "Training-Batch No.2680\n",
      "Loss = 3.3648135662078857\n",
      "Training-Batch No.2690\n",
      "Loss = 3.317828893661499\n",
      "Training-Batch No.2700\n",
      "Loss = 3.2671892642974854\n",
      "Training-Batch No.2710\n",
      "Loss = 2.9238533973693848\n",
      "Training-Batch No.2720\n",
      "Loss = 3.155341148376465\n",
      "Training-Batch No.2730\n",
      "Loss = 2.3289425373077393\n",
      "Training-Batch No.2740\n",
      "Loss = 2.308377504348755\n",
      "Training-Batch No.2750\n",
      "Loss = 2.730103015899658\n",
      "Training-Batch No.2760\n",
      "Loss = 3.0786099433898926\n",
      "Training-Batch No.2770\n",
      "Loss = 3.1026079654693604\n",
      "Training-Batch No.2780\n",
      "Loss = 3.050286293029785\n",
      "Training-Batch No.2790\n",
      "Loss = 3.447913885116577\n",
      "Epoch 3 Training Loss: 3.2047\n",
      "Start validation\n",
      "Epoch 3 Validation Loss: 3.1637\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.21841397849462366\n",
      "Average Top-3 accuracy 0.3563172043010753\n",
      "Average Top-5 accuracy 0.42325268817204303\n",
      "Average Top-7 accuracy 0.478494623655914\n",
      "Average Top-9 accuracy 0.5174731182795699\n",
      "Average Top-11 accuracy 0.5530913978494624\n",
      "Average Top-13 accuracy 0.5879032258064516\n",
      "Average Top-15 accuracy 0.6206989247311828\n",
      "current acc 0.21841397849462366\n",
      "best acc 0.20967741935483872\n",
      "Saving the best model\n",
      "updated best accuracy 0.21841397849462366\n",
      "Epoch No. 4\n",
      "Training-Batch No.2800\n",
      "Loss = 3.5592644214630127\n",
      "Training-Batch No.2810\n",
      "Loss = 2.2740375995635986\n",
      "Training-Batch No.2820\n",
      "Loss = 3.2829041481018066\n",
      "Training-Batch No.2830\n",
      "Loss = 3.743607521057129\n",
      "Training-Batch No.2840\n",
      "Loss = 2.8105990886688232\n",
      "Training-Batch No.2850\n",
      "Loss = 3.9142801761627197\n",
      "Training-Batch No.2860\n",
      "Loss = 3.759253740310669\n",
      "Training-Batch No.2870\n",
      "Loss = 3.2703442573547363\n",
      "Training-Batch No.2880\n",
      "Loss = 3.6273932456970215\n",
      "Training-Batch No.2890\n",
      "Loss = 3.415290594100952\n",
      "Training-Batch No.2900\n",
      "Loss = 2.9552273750305176\n",
      "Training-Batch No.2910\n",
      "Loss = 3.119203567504883\n",
      "Training-Batch No.2920\n",
      "Loss = 3.1308107376098633\n",
      "Training-Batch No.2930\n",
      "Loss = 3.1589691638946533\n",
      "Training-Batch No.2940\n",
      "Loss = 3.729593276977539\n",
      "Training-Batch No.2950\n",
      "Loss = 2.923161029815674\n",
      "Training-Batch No.2960\n",
      "Loss = 2.655226469039917\n",
      "Training-Batch No.2970\n",
      "Loss = 3.6420562267303467\n",
      "Training-Batch No.2980\n",
      "Loss = 3.308786630630493\n",
      "Training-Batch No.2990\n",
      "Loss = 3.256253957748413\n",
      "Training-Batch No.3000\n",
      "Loss = 3.437246799468994\n",
      "Training-Batch No.3010\n",
      "Loss = 2.908597469329834\n",
      "Training-Batch No.3020\n",
      "Loss = 3.4805376529693604\n",
      "Training-Batch No.3030\n",
      "Loss = 3.7090060710906982\n",
      "Training-Batch No.3040\n",
      "Loss = 2.928917646408081\n",
      "Training-Batch No.3050\n",
      "Loss = 2.4279072284698486\n",
      "Training-Batch No.3060\n",
      "Loss = 3.3566341400146484\n",
      "Training-Batch No.3070\n",
      "Loss = 3.318657159805298\n",
      "Training-Batch No.3080\n",
      "Loss = 2.7228200435638428\n",
      "Training-Batch No.3090\n",
      "Loss = 2.649284839630127\n",
      "Training-Batch No.3100\n",
      "Loss = 3.4130184650421143\n",
      "Training-Batch No.3110\n",
      "Loss = 3.4734671115875244\n",
      "Training-Batch No.3120\n",
      "Loss = 3.5892739295959473\n",
      "Training-Batch No.3130\n",
      "Loss = 2.464700222015381\n",
      "Training-Batch No.3140\n",
      "Loss = 2.960040330886841\n",
      "Training-Batch No.3150\n",
      "Loss = 3.3179283142089844\n",
      "Training-Batch No.3160\n",
      "Loss = 3.4103283882141113\n",
      "Training-Batch No.3170\n",
      "Loss = 3.598896026611328\n",
      "Training-Batch No.3180\n",
      "Loss = 3.199129819869995\n",
      "Training-Batch No.3190\n",
      "Loss = 3.3476755619049072\n",
      "Training-Batch No.3200\n",
      "Loss = 2.694054365158081\n",
      "Training-Batch No.3210\n",
      "Loss = 1.9825845956802368\n",
      "Training-Batch No.3220\n",
      "Loss = 3.3364100456237793\n",
      "Training-Batch No.3230\n",
      "Loss = 3.4611825942993164\n",
      "Training-Batch No.3240\n",
      "Loss = 3.160707712173462\n",
      "Training-Batch No.3250\n",
      "Loss = 2.1087710857391357\n",
      "Training-Batch No.3260\n",
      "Loss = 3.697671413421631\n",
      "Training-Batch No.3270\n",
      "Loss = 2.500819444656372\n",
      "Training-Batch No.3280\n",
      "Loss = 2.8044958114624023\n",
      "Training-Batch No.3290\n",
      "Loss = 2.2575323581695557\n",
      "Training-Batch No.3300\n",
      "Loss = 3.222255229949951\n",
      "Training-Batch No.3310\n",
      "Loss = 3.2830164432525635\n",
      "Training-Batch No.3320\n",
      "Loss = 3.1329545974731445\n",
      "Training-Batch No.3330\n",
      "Loss = 2.6914174556732178\n",
      "Training-Batch No.3340\n",
      "Loss = 2.85556960105896\n",
      "Training-Batch No.3350\n",
      "Loss = 3.299149513244629\n",
      "Training-Batch No.3360\n",
      "Loss = 2.4418115615844727\n",
      "Training-Batch No.3370\n",
      "Loss = 3.76301908493042\n",
      "Training-Batch No.3380\n",
      "Loss = 3.0944736003875732\n",
      "Training-Batch No.3390\n",
      "Loss = 3.0207653045654297\n",
      "Training-Batch No.3400\n",
      "Loss = 3.4059243202209473\n",
      "Training-Batch No.3410\n",
      "Loss = 2.74967098236084\n",
      "Training-Batch No.3420\n",
      "Loss = 3.0296242237091064\n",
      "Training-Batch No.3430\n",
      "Loss = 3.301593542098999\n",
      "Training-Batch No.3440\n",
      "Loss = 2.829026222229004\n",
      "Training-Batch No.3450\n",
      "Loss = 3.228980541229248\n",
      "Training-Batch No.3460\n",
      "Loss = 3.2222089767456055\n",
      "Training-Batch No.3470\n",
      "Loss = 3.0595455169677734\n",
      "Training-Batch No.3480\n",
      "Loss = 2.77451229095459\n",
      "Training-Batch No.3490\n",
      "Loss = 2.846360206604004\n",
      "Training-Batch No.3500\n",
      "Loss = 3.0014920234680176\n",
      "Training-Batch No.3510\n",
      "Loss = 2.373354911804199\n",
      "Training-Batch No.3520\n",
      "Loss = 3.151827335357666\n",
      "Training-Batch No.3530\n",
      "Loss = 3.8741681575775146\n",
      "Training-Batch No.3540\n",
      "Loss = 3.030839681625366\n",
      "Training-Batch No.3550\n",
      "Loss = 2.8590919971466064\n",
      "Training-Batch No.3560\n",
      "Loss = 4.050710678100586\n",
      "Training-Batch No.3570\n",
      "Loss = 2.8391857147216797\n",
      "Training-Batch No.3580\n",
      "Loss = 2.5311644077301025\n",
      "Training-Batch No.3590\n",
      "Loss = 2.614567756652832\n",
      "Training-Batch No.3600\n",
      "Loss = 2.7179133892059326\n",
      "Training-Batch No.3610\n",
      "Loss = 3.3311281204223633\n",
      "Training-Batch No.3620\n",
      "Loss = 3.3218228816986084\n",
      "Training-Batch No.3630\n",
      "Loss = 3.261096954345703\n",
      "Training-Batch No.3640\n",
      "Loss = 2.965935707092285\n",
      "Training-Batch No.3650\n",
      "Loss = 3.198612928390503\n",
      "Training-Batch No.3660\n",
      "Loss = 2.3146629333496094\n",
      "Training-Batch No.3670\n",
      "Loss = 2.375070810317993\n",
      "Training-Batch No.3680\n",
      "Loss = 2.6630992889404297\n",
      "Training-Batch No.3690\n",
      "Loss = 3.1303999423980713\n",
      "Training-Batch No.3700\n",
      "Loss = 3.161363124847412\n",
      "Training-Batch No.3710\n",
      "Loss = 3.1219089031219482\n",
      "Training-Batch No.3720\n",
      "Loss = 3.3492541313171387\n",
      "Epoch 4 Training Loss: 3.1426\n",
      "Start validation\n",
      "Epoch 4 Validation Loss: 3.1169\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.23091397849462367\n",
      "Average Top-3 accuracy 0.3741935483870968\n",
      "Average Top-5 accuracy 0.44650537634408605\n",
      "Average Top-7 accuracy 0.4946236559139785\n",
      "Average Top-9 accuracy 0.535752688172043\n",
      "Average Top-11 accuracy 0.5724462365591397\n",
      "Average Top-13 accuracy 0.603225806451613\n",
      "Average Top-15 accuracy 0.6341397849462366\n",
      "current acc 0.23091397849462367\n",
      "best acc 0.21841397849462366\n",
      "Saving the best model\n",
      "updated best accuracy 0.23091397849462367\n",
      "Epoch No. 5\n",
      "Training-Batch No.3730\n",
      "Loss = 3.4452617168426514\n",
      "Training-Batch No.3740\n",
      "Loss = 2.022486686706543\n",
      "Training-Batch No.3750\n",
      "Loss = 3.2446813583374023\n",
      "Training-Batch No.3760\n",
      "Loss = 3.6170215606689453\n",
      "Training-Batch No.3770\n",
      "Loss = 3.0174343585968018\n",
      "Training-Batch No.3780\n",
      "Loss = 3.673588275909424\n",
      "Training-Batch No.3790\n",
      "Loss = 3.6818132400512695\n",
      "Training-Batch No.3800\n",
      "Loss = 3.1340672969818115\n",
      "Training-Batch No.3810\n",
      "Loss = 3.722245693206787\n",
      "Training-Batch No.3820\n",
      "Loss = 3.4830408096313477\n",
      "Training-Batch No.3830\n",
      "Loss = 2.6495208740234375\n",
      "Training-Batch No.3840\n",
      "Loss = 3.081947088241577\n",
      "Training-Batch No.3850\n",
      "Loss = 3.0812480449676514\n",
      "Training-Batch No.3860\n",
      "Loss = 3.2154223918914795\n",
      "Training-Batch No.3870\n",
      "Loss = 3.5063953399658203\n",
      "Training-Batch No.3880\n",
      "Loss = 2.9904332160949707\n",
      "Training-Batch No.3890\n",
      "Loss = 2.61039400100708\n",
      "Training-Batch No.3900\n",
      "Loss = 3.5462889671325684\n",
      "Training-Batch No.3910\n",
      "Loss = 3.2829439640045166\n",
      "Training-Batch No.3920\n",
      "Loss = 3.146527051925659\n",
      "Training-Batch No.3930\n",
      "Loss = 3.3941094875335693\n",
      "Training-Batch No.3940\n",
      "Loss = 2.9080755710601807\n",
      "Training-Batch No.3950\n",
      "Loss = 3.0709519386291504\n",
      "Training-Batch No.3960\n",
      "Loss = 3.7330968379974365\n",
      "Training-Batch No.3970\n",
      "Loss = 2.886897563934326\n",
      "Training-Batch No.3980\n",
      "Loss = 2.3925867080688477\n",
      "Training-Batch No.3990\n",
      "Loss = 3.334807872772217\n",
      "Training-Batch No.4000\n",
      "Loss = 3.3473622798919678\n",
      "Training-Batch No.4010\n",
      "Loss = 2.6434319019317627\n",
      "Training-Batch No.4020\n",
      "Loss = 2.6000630855560303\n",
      "Training-Batch No.4030\n",
      "Loss = 3.400007486343384\n",
      "Training-Batch No.4040\n",
      "Loss = 3.487992525100708\n",
      "Training-Batch No.4050\n",
      "Loss = 3.835028886795044\n",
      "Training-Batch No.4060\n",
      "Loss = 2.500967502593994\n",
      "Training-Batch No.4070\n",
      "Loss = 2.8415465354919434\n",
      "Training-Batch No.4080\n",
      "Loss = 3.2418627738952637\n",
      "Training-Batch No.4090\n",
      "Loss = 3.312868118286133\n",
      "Training-Batch No.4100\n",
      "Loss = 3.5551066398620605\n",
      "Training-Batch No.4110\n",
      "Loss = 3.1216039657592773\n",
      "Training-Batch No.4120\n",
      "Loss = 3.284022331237793\n",
      "Training-Batch No.4130\n",
      "Loss = 2.699923038482666\n",
      "Training-Batch No.4140\n",
      "Loss = 2.143697738647461\n",
      "Training-Batch No.4150\n",
      "Loss = 3.1074042320251465\n",
      "Training-Batch No.4160\n",
      "Loss = 3.443704843521118\n",
      "Training-Batch No.4170\n",
      "Loss = 2.960414171218872\n",
      "Training-Batch No.4180\n",
      "Loss = 2.087862730026245\n",
      "Training-Batch No.4190\n",
      "Loss = 3.5586929321289062\n",
      "Training-Batch No.4200\n",
      "Loss = 2.463104724884033\n",
      "Training-Batch No.4210\n",
      "Loss = 3.006913661956787\n",
      "Training-Batch No.4220\n",
      "Loss = 2.018390655517578\n",
      "Training-Batch No.4230\n",
      "Loss = 3.049438953399658\n",
      "Training-Batch No.4240\n",
      "Loss = 3.4549710750579834\n",
      "Training-Batch No.4250\n",
      "Loss = 2.9641740322113037\n",
      "Training-Batch No.4260\n",
      "Loss = 2.6986753940582275\n",
      "Training-Batch No.4270\n",
      "Loss = 2.6295902729034424\n",
      "Training-Batch No.4280\n",
      "Loss = 3.2023391723632812\n",
      "Training-Batch No.4290\n",
      "Loss = 2.30815052986145\n",
      "Training-Batch No.4300\n",
      "Loss = 3.738647222518921\n",
      "Training-Batch No.4310\n",
      "Loss = 3.0075466632843018\n",
      "Training-Batch No.4320\n",
      "Loss = 3.0615298748016357\n",
      "Training-Batch No.4330\n",
      "Loss = 3.6049277782440186\n",
      "Training-Batch No.4340\n",
      "Loss = 2.654679298400879\n",
      "Training-Batch No.4350\n",
      "Loss = 3.0086112022399902\n",
      "Training-Batch No.4360\n",
      "Loss = 3.31432843208313\n",
      "Training-Batch No.4370\n",
      "Loss = 2.8165552616119385\n",
      "Training-Batch No.4380\n",
      "Loss = 3.0003819465637207\n",
      "Training-Batch No.4390\n",
      "Loss = 3.2646901607513428\n",
      "Training-Batch No.4400\n",
      "Loss = 3.096737861633301\n",
      "Training-Batch No.4410\n",
      "Loss = 2.7800889015197754\n",
      "Training-Batch No.4420\n",
      "Loss = 2.7776198387145996\n",
      "Training-Batch No.4430\n",
      "Loss = 3.1340935230255127\n",
      "Training-Batch No.4440\n",
      "Loss = 2.3452069759368896\n",
      "Training-Batch No.4450\n",
      "Loss = 3.094230890274048\n",
      "Training-Batch No.4460\n",
      "Loss = 3.6527628898620605\n",
      "Training-Batch No.4470\n",
      "Loss = 2.993241786956787\n",
      "Training-Batch No.4480\n",
      "Loss = 2.667222499847412\n",
      "Training-Batch No.4490\n",
      "Loss = 3.9682493209838867\n",
      "Training-Batch No.4500\n",
      "Loss = 2.7190654277801514\n",
      "Training-Batch No.4510\n",
      "Loss = 2.6237728595733643\n",
      "Training-Batch No.4520\n",
      "Loss = 2.5854172706604004\n",
      "Training-Batch No.4530\n",
      "Loss = 2.652212619781494\n",
      "Training-Batch No.4540\n",
      "Loss = 3.1879711151123047\n",
      "Training-Batch No.4550\n",
      "Loss = 3.2768781185150146\n",
      "Training-Batch No.4560\n",
      "Loss = 3.188929796218872\n",
      "Training-Batch No.4570\n",
      "Loss = 3.0503010749816895\n",
      "Training-Batch No.4580\n",
      "Loss = 3.107825994491577\n",
      "Training-Batch No.4590\n",
      "Loss = 2.2738635540008545\n",
      "Training-Batch No.4600\n",
      "Loss = 2.3228371143341064\n",
      "Training-Batch No.4610\n",
      "Loss = 2.7372238636016846\n",
      "Training-Batch No.4620\n",
      "Loss = 3.0952603816986084\n",
      "Training-Batch No.4630\n",
      "Loss = 3.0784504413604736\n",
      "Training-Batch No.4640\n",
      "Loss = 3.0367038249969482\n",
      "Training-Batch No.4650\n",
      "Loss = 3.335597038269043\n",
      "Epoch 5 Training Loss: 3.0847\n",
      "Start validation\n",
      "Epoch 5 Validation Loss: 3.0911\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2353494623655914\n",
      "Average Top-3 accuracy 0.3788978494623656\n",
      "Average Top-5 accuracy 0.4466397849462366\n",
      "Average Top-7 accuracy 0.4991935483870968\n",
      "Average Top-9 accuracy 0.5399193548387097\n",
      "Average Top-11 accuracy 0.5791666666666667\n",
      "Average Top-13 accuracy 0.6133064516129032\n",
      "Average Top-15 accuracy 0.6431451612903226\n",
      "current acc 0.2353494623655914\n",
      "best acc 0.23091397849462367\n",
      "Saving the best model\n",
      "updated best accuracy 0.2353494623655914\n",
      "Epoch No. 6\n",
      "Training-Batch No.4660\n",
      "Loss = 3.3618240356445312\n",
      "Training-Batch No.4670\n",
      "Loss = 2.065446615219116\n",
      "Training-Batch No.4680\n",
      "Loss = 3.269538640975952\n",
      "Training-Batch No.4690\n",
      "Loss = 3.5770645141601562\n",
      "Training-Batch No.4700\n",
      "Loss = 2.751864433288574\n",
      "Training-Batch No.4710\n",
      "Loss = 3.6335339546203613\n",
      "Training-Batch No.4720\n",
      "Loss = 3.658642530441284\n",
      "Training-Batch No.4730\n",
      "Loss = 3.0782010555267334\n",
      "Training-Batch No.4740\n",
      "Loss = 3.3991498947143555\n",
      "Training-Batch No.4750\n",
      "Loss = 3.5317423343658447\n",
      "Training-Batch No.4760\n",
      "Loss = 2.677213668823242\n",
      "Training-Batch No.4770\n",
      "Loss = 3.1776809692382812\n",
      "Training-Batch No.4780\n",
      "Loss = 3.058408260345459\n",
      "Training-Batch No.4790\n",
      "Loss = 3.0943055152893066\n",
      "Training-Batch No.4800\n",
      "Loss = 3.384176254272461\n",
      "Training-Batch No.4810\n",
      "Loss = 2.8922340869903564\n",
      "Training-Batch No.4820\n",
      "Loss = 2.5530805587768555\n",
      "Training-Batch No.4830\n",
      "Loss = 3.3839049339294434\n",
      "Training-Batch No.4840\n",
      "Loss = 3.2588870525360107\n",
      "Training-Batch No.4850\n",
      "Loss = 3.260770082473755\n",
      "Training-Batch No.4860\n",
      "Loss = 3.326557159423828\n",
      "Training-Batch No.4870\n",
      "Loss = 2.8879034519195557\n",
      "Training-Batch No.4880\n",
      "Loss = 3.0549428462982178\n",
      "Training-Batch No.4890\n",
      "Loss = 3.6259682178497314\n",
      "Training-Batch No.4900\n",
      "Loss = 2.879883289337158\n",
      "Training-Batch No.4910\n",
      "Loss = 2.244717597961426\n",
      "Training-Batch No.4920\n",
      "Loss = 3.078763484954834\n",
      "Training-Batch No.4930\n",
      "Loss = 3.3707172870635986\n",
      "Training-Batch No.4940\n",
      "Loss = 2.4672791957855225\n",
      "Training-Batch No.4950\n",
      "Loss = 2.5064382553100586\n",
      "Training-Batch No.4960\n",
      "Loss = 3.3851661682128906\n",
      "Training-Batch No.4970\n",
      "Loss = 3.4731557369232178\n",
      "Training-Batch No.4980\n",
      "Loss = 3.7826426029205322\n",
      "Training-Batch No.4990\n",
      "Loss = 2.4177346229553223\n",
      "Training-Batch No.5000\n",
      "Loss = 2.6186416149139404\n",
      "Training-Batch No.5010\n",
      "Loss = 3.2102203369140625\n",
      "Training-Batch No.5020\n",
      "Loss = 3.3542673587799072\n",
      "Training-Batch No.5030\n",
      "Loss = 3.510436534881592\n",
      "Training-Batch No.5040\n",
      "Loss = 3.075157403945923\n",
      "Training-Batch No.5050\n",
      "Loss = 3.1923036575317383\n",
      "Training-Batch No.5060\n",
      "Loss = 2.6966021060943604\n",
      "Training-Batch No.5070\n",
      "Loss = 1.9782519340515137\n",
      "Training-Batch No.5080\n",
      "Loss = 2.951590061187744\n",
      "Training-Batch No.5090\n",
      "Loss = 3.490640163421631\n",
      "Training-Batch No.5100\n",
      "Loss = 2.9540212154388428\n",
      "Training-Batch No.5110\n",
      "Loss = 2.0587408542633057\n",
      "Training-Batch No.5120\n",
      "Loss = 3.513380765914917\n",
      "Training-Batch No.5130\n",
      "Loss = 2.4310426712036133\n",
      "Training-Batch No.5140\n",
      "Loss = 2.850482940673828\n",
      "Training-Batch No.5150\n",
      "Loss = 2.0203640460968018\n",
      "Training-Batch No.5160\n",
      "Loss = 3.1834003925323486\n",
      "Training-Batch No.5170\n",
      "Loss = 3.2760438919067383\n",
      "Training-Batch No.5180\n",
      "Loss = 2.9775149822235107\n",
      "Training-Batch No.5190\n",
      "Loss = 2.6732428073883057\n",
      "Training-Batch No.5200\n",
      "Loss = 2.7207272052764893\n",
      "Training-Batch No.5210\n",
      "Loss = 3.204043388366699\n",
      "Training-Batch No.5220\n",
      "Loss = 2.1785874366760254\n",
      "Training-Batch No.5230\n",
      "Loss = 3.785856246948242\n",
      "Training-Batch No.5240\n",
      "Loss = 2.889800548553467\n",
      "Training-Batch No.5250\n",
      "Loss = 3.1183156967163086\n",
      "Training-Batch No.5260\n",
      "Loss = 3.551011323928833\n",
      "Training-Batch No.5270\n",
      "Loss = 2.689180612564087\n",
      "Training-Batch No.5280\n",
      "Loss = 3.047114372253418\n",
      "Training-Batch No.5290\n",
      "Loss = 3.3091256618499756\n",
      "Training-Batch No.5300\n",
      "Loss = 2.8116064071655273\n",
      "Training-Batch No.5310\n",
      "Loss = 2.9371299743652344\n",
      "Training-Batch No.5320\n",
      "Loss = 3.144063949584961\n",
      "Training-Batch No.5330\n",
      "Loss = 3.1352596282958984\n",
      "Training-Batch No.5340\n",
      "Loss = 2.688863754272461\n",
      "Training-Batch No.5350\n",
      "Loss = 2.833782911300659\n",
      "Training-Batch No.5360\n",
      "Loss = 3.0358712673187256\n",
      "Training-Batch No.5370\n",
      "Loss = 2.262355327606201\n",
      "Training-Batch No.5380\n",
      "Loss = 2.9719486236572266\n",
      "Training-Batch No.5390\n",
      "Loss = 3.756662368774414\n",
      "Training-Batch No.5400\n",
      "Loss = 2.9563732147216797\n",
      "Training-Batch No.5410\n",
      "Loss = 2.767258644104004\n",
      "Training-Batch No.5420\n",
      "Loss = 3.9312267303466797\n",
      "Training-Batch No.5430\n",
      "Loss = 2.647322416305542\n",
      "Training-Batch No.5440\n",
      "Loss = 2.440976619720459\n",
      "Training-Batch No.5450\n",
      "Loss = 2.5355074405670166\n",
      "Training-Batch No.5460\n",
      "Loss = 2.7676846981048584\n",
      "Training-Batch No.5470\n",
      "Loss = 3.0583677291870117\n",
      "Training-Batch No.5480\n",
      "Loss = 3.2088935375213623\n",
      "Training-Batch No.5490\n",
      "Loss = 3.129061222076416\n",
      "Training-Batch No.5500\n",
      "Loss = 2.9622392654418945\n",
      "Training-Batch No.5510\n",
      "Loss = 3.013787269592285\n",
      "Training-Batch No.5520\n",
      "Loss = 2.1707639694213867\n",
      "Training-Batch No.5530\n",
      "Loss = 2.1497292518615723\n",
      "Training-Batch No.5540\n",
      "Loss = 2.9095981121063232\n",
      "Training-Batch No.5550\n",
      "Loss = 3.0257503986358643\n",
      "Training-Batch No.5560\n",
      "Loss = 3.0345537662506104\n",
      "Training-Batch No.5570\n",
      "Loss = 2.947906970977783\n",
      "Training-Batch No.5580\n",
      "Loss = 3.3022243976593018\n",
      "Epoch 6 Training Loss: 3.0363\n",
      "Start validation\n",
      "Epoch 6 Validation Loss: 3.0822\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.23172043010752688\n",
      "Average Top-3 accuracy 0.3793010752688172\n",
      "Average Top-5 accuracy 0.45376344086021503\n",
      "Average Top-7 accuracy 0.5045698924731182\n",
      "Average Top-9 accuracy 0.5459677419354839\n",
      "Average Top-11 accuracy 0.5801075268817204\n",
      "Average Top-13 accuracy 0.6150537634408603\n",
      "Average Top-15 accuracy 0.6459677419354839\n",
      "current acc 0.23172043010752688\n",
      "best acc 0.2353494623655914\n",
      "updated best accuracy 0.2353494623655914\n",
      "Epoch No. 7\n",
      "Training-Batch No.5590\n",
      "Loss = 3.1741578578948975\n",
      "Training-Batch No.5600\n",
      "Loss = 1.9416254758834839\n",
      "Training-Batch No.5610\n",
      "Loss = 3.3526177406311035\n",
      "Training-Batch No.5620\n",
      "Loss = 3.5668282508850098\n",
      "Training-Batch No.5630\n",
      "Loss = 2.765247106552124\n",
      "Training-Batch No.5640\n",
      "Loss = 3.685859203338623\n",
      "Training-Batch No.5650\n",
      "Loss = 3.5878891944885254\n",
      "Training-Batch No.5660\n",
      "Loss = 3.010124921798706\n",
      "Training-Batch No.5670\n",
      "Loss = 3.528858184814453\n",
      "Training-Batch No.5680\n",
      "Loss = 3.3344156742095947\n",
      "Training-Batch No.5690\n",
      "Loss = 2.6056556701660156\n",
      "Training-Batch No.5700\n",
      "Loss = 3.0358407497406006\n",
      "Training-Batch No.5710\n",
      "Loss = 3.0446832180023193\n",
      "Training-Batch No.5720\n",
      "Loss = 3.18534779548645\n",
      "Training-Batch No.5730\n",
      "Loss = 3.417026996612549\n",
      "Training-Batch No.5740\n",
      "Loss = 2.8364198207855225\n",
      "Training-Batch No.5750\n",
      "Loss = 2.4851088523864746\n",
      "Training-Batch No.5760\n",
      "Loss = 3.402132272720337\n",
      "Training-Batch No.5770\n",
      "Loss = 3.1582460403442383\n",
      "Training-Batch No.5780\n",
      "Loss = 3.065128803253174\n",
      "Training-Batch No.5790\n",
      "Loss = 3.2339015007019043\n",
      "Training-Batch No.5800\n",
      "Loss = 2.869901657104492\n",
      "Training-Batch No.5810\n",
      "Loss = 2.998793840408325\n",
      "Training-Batch No.5820\n",
      "Loss = 3.5892112255096436\n",
      "Training-Batch No.5830\n",
      "Loss = 2.75508451461792\n",
      "Training-Batch No.5840\n",
      "Loss = 2.2121028900146484\n",
      "Training-Batch No.5850\n",
      "Loss = 3.1371700763702393\n",
      "Training-Batch No.5860\n",
      "Loss = 3.3673954010009766\n",
      "Training-Batch No.5870\n",
      "Loss = 2.473038911819458\n",
      "Training-Batch No.5880\n",
      "Loss = 2.4367377758026123\n",
      "Training-Batch No.5890\n",
      "Loss = 3.2815663814544678\n",
      "Training-Batch No.5900\n",
      "Loss = 3.3426616191864014\n",
      "Training-Batch No.5910\n",
      "Loss = 3.7690553665161133\n",
      "Training-Batch No.5920\n",
      "Loss = 2.374964952468872\n",
      "Training-Batch No.5930\n",
      "Loss = 2.595522165298462\n",
      "Training-Batch No.5940\n",
      "Loss = 3.1733016967773438\n",
      "Training-Batch No.5950\n",
      "Loss = 3.4477691650390625\n",
      "Training-Batch No.5960\n",
      "Loss = 3.6703832149505615\n",
      "Training-Batch No.5970\n",
      "Loss = 3.033231735229492\n",
      "Training-Batch No.5980\n",
      "Loss = 3.245439052581787\n",
      "Training-Batch No.5990\n",
      "Loss = 2.674464464187622\n",
      "Training-Batch No.6000\n",
      "Loss = 1.9217041730880737\n",
      "Training-Batch No.6010\n",
      "Loss = 2.960536241531372\n",
      "Training-Batch No.6020\n",
      "Loss = 3.553082227706909\n",
      "Training-Batch No.6030\n",
      "Loss = 2.7877540588378906\n",
      "Training-Batch No.6040\n",
      "Loss = 1.7935056686401367\n",
      "Training-Batch No.6050\n",
      "Loss = 3.435297966003418\n",
      "Training-Batch No.6060\n",
      "Loss = 2.3893651962280273\n",
      "Training-Batch No.6070\n",
      "Loss = 2.8292648792266846\n",
      "Training-Batch No.6080\n",
      "Loss = 1.9063687324523926\n",
      "Training-Batch No.6090\n",
      "Loss = 2.8705997467041016\n",
      "Training-Batch No.6100\n",
      "Loss = 3.2162227630615234\n",
      "Training-Batch No.6110\n",
      "Loss = 2.9073734283447266\n",
      "Training-Batch No.6120\n",
      "Loss = 2.461087226867676\n",
      "Training-Batch No.6130\n",
      "Loss = 2.8193392753601074\n",
      "Training-Batch No.6140\n",
      "Loss = 3.092957019805908\n",
      "Training-Batch No.6150\n",
      "Loss = 2.148054361343384\n",
      "Training-Batch No.6160\n",
      "Loss = 3.778749704360962\n",
      "Training-Batch No.6170\n",
      "Loss = 2.8965911865234375\n",
      "Training-Batch No.6180\n",
      "Loss = 3.048982620239258\n",
      "Training-Batch No.6190\n",
      "Loss = 3.6424379348754883\n",
      "Training-Batch No.6200\n",
      "Loss = 2.696564197540283\n",
      "Training-Batch No.6210\n",
      "Loss = 3.026190757751465\n",
      "Training-Batch No.6220\n",
      "Loss = 3.316544771194458\n",
      "Training-Batch No.6230\n",
      "Loss = 2.8027501106262207\n",
      "Training-Batch No.6240\n",
      "Loss = 2.921260118484497\n",
      "Training-Batch No.6250\n",
      "Loss = 3.1544766426086426\n",
      "Training-Batch No.6260\n",
      "Loss = 3.0657076835632324\n",
      "Training-Batch No.6270\n",
      "Loss = 2.5048370361328125\n",
      "Training-Batch No.6280\n",
      "Loss = 2.9439210891723633\n",
      "Training-Batch No.6290\n",
      "Loss = 2.9762418270111084\n",
      "Training-Batch No.6300\n",
      "Loss = 2.209836006164551\n",
      "Training-Batch No.6310\n",
      "Loss = 2.9987592697143555\n",
      "Training-Batch No.6320\n",
      "Loss = 3.493420124053955\n",
      "Training-Batch No.6330\n",
      "Loss = 2.7460179328918457\n",
      "Training-Batch No.6340\n",
      "Loss = 2.724367618560791\n",
      "Training-Batch No.6350\n",
      "Loss = 3.842459201812744\n",
      "Training-Batch No.6360\n",
      "Loss = 2.455949306488037\n",
      "Training-Batch No.6370\n",
      "Loss = 2.412139892578125\n",
      "Training-Batch No.6380\n",
      "Loss = 2.4439175128936768\n",
      "Training-Batch No.6390\n",
      "Loss = 2.7494215965270996\n",
      "Training-Batch No.6400\n",
      "Loss = 3.040454626083374\n",
      "Training-Batch No.6410\n",
      "Loss = 3.166104793548584\n",
      "Training-Batch No.6420\n",
      "Loss = 3.1784865856170654\n",
      "Training-Batch No.6430\n",
      "Loss = 2.8430402278900146\n",
      "Training-Batch No.6440\n",
      "Loss = 3.0077805519104004\n",
      "Training-Batch No.6450\n",
      "Loss = 2.2035813331604004\n",
      "Training-Batch No.6460\n",
      "Loss = 2.15486741065979\n",
      "Training-Batch No.6470\n",
      "Loss = 2.392317533493042\n",
      "Training-Batch No.6480\n",
      "Loss = 2.994062662124634\n",
      "Training-Batch No.6490\n",
      "Loss = 2.9739246368408203\n",
      "Training-Batch No.6500\n",
      "Loss = 2.9230525493621826\n",
      "Training-Batch No.6510\n",
      "Loss = 3.226334810256958\n",
      "Epoch 7 Training Loss: 2.9949\n",
      "Start validation\n",
      "Epoch 7 Validation Loss: 3.0535\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2360215053763441\n",
      "Average Top-3 accuracy 0.3879032258064516\n",
      "Average Top-5 accuracy 0.45873655913978495\n",
      "Average Top-7 accuracy 0.5130376344086022\n",
      "Average Top-9 accuracy 0.5565860215053764\n",
      "Average Top-11 accuracy 0.5881720430107527\n",
      "Average Top-13 accuracy 0.6225806451612903\n",
      "Average Top-15 accuracy 0.6518817204301075\n",
      "current acc 0.2360215053763441\n",
      "best acc 0.2353494623655914\n",
      "Saving the best model\n",
      "updated best accuracy 0.2360215053763441\n",
      "Epoch No. 8\n",
      "Training-Batch No.6520\n",
      "Loss = 3.101576328277588\n",
      "Training-Batch No.6530\n",
      "Loss = 2.00793719291687\n",
      "Training-Batch No.6540\n",
      "Loss = 3.23059344291687\n",
      "Training-Batch No.6550\n",
      "Loss = 3.553802490234375\n",
      "Training-Batch No.6560\n",
      "Loss = 2.643183708190918\n",
      "Training-Batch No.6570\n",
      "Loss = 3.791175365447998\n",
      "Training-Batch No.6580\n",
      "Loss = 3.574939727783203\n",
      "Training-Batch No.6590\n",
      "Loss = 2.928375244140625\n",
      "Training-Batch No.6600\n",
      "Loss = 3.5131664276123047\n",
      "Training-Batch No.6610\n",
      "Loss = 3.5574839115142822\n",
      "Training-Batch No.6620\n",
      "Loss = 2.7309584617614746\n",
      "Training-Batch No.6630\n",
      "Loss = 2.9750893115997314\n",
      "Training-Batch No.6640\n",
      "Loss = 3.0007238388061523\n",
      "Training-Batch No.6650\n",
      "Loss = 2.9570510387420654\n",
      "Training-Batch No.6660\n",
      "Loss = 3.2933363914489746\n",
      "Training-Batch No.6670\n",
      "Loss = 2.868208885192871\n",
      "Training-Batch No.6680\n",
      "Loss = 2.475914716720581\n",
      "Training-Batch No.6690\n",
      "Loss = 3.2374253273010254\n",
      "Training-Batch No.6700\n",
      "Loss = 3.0585737228393555\n",
      "Training-Batch No.6710\n",
      "Loss = 3.230957269668579\n",
      "Training-Batch No.6720\n",
      "Loss = 3.250576972961426\n",
      "Training-Batch No.6730\n",
      "Loss = 2.8031015396118164\n",
      "Training-Batch No.6740\n",
      "Loss = 2.9639978408813477\n",
      "Training-Batch No.6750\n",
      "Loss = 3.356113910675049\n",
      "Training-Batch No.6760\n",
      "Loss = 2.7458486557006836\n",
      "Training-Batch No.6770\n",
      "Loss = 2.1742475032806396\n",
      "Training-Batch No.6780\n",
      "Loss = 3.1994636058807373\n",
      "Training-Batch No.6790\n",
      "Loss = 3.400310516357422\n",
      "Training-Batch No.6800\n",
      "Loss = 2.6044957637786865\n",
      "Training-Batch No.6810\n",
      "Loss = 2.4027209281921387\n",
      "Training-Batch No.6820\n",
      "Loss = 3.290196418762207\n",
      "Training-Batch No.6830\n",
      "Loss = 3.221940040588379\n",
      "Training-Batch No.6840\n",
      "Loss = 3.633064031600952\n",
      "Training-Batch No.6850\n",
      "Loss = 2.3992295265197754\n",
      "Training-Batch No.6860\n",
      "Loss = 2.5698747634887695\n",
      "Training-Batch No.6870\n",
      "Loss = 3.1747281551361084\n",
      "Training-Batch No.6880\n",
      "Loss = 3.3226728439331055\n",
      "Training-Batch No.6890\n",
      "Loss = 3.734532356262207\n",
      "Training-Batch No.6900\n",
      "Loss = 2.872267723083496\n",
      "Training-Batch No.6910\n",
      "Loss = 3.2269222736358643\n",
      "Training-Batch No.6920\n",
      "Loss = 2.6400461196899414\n",
      "Training-Batch No.6930\n",
      "Loss = 1.87126624584198\n",
      "Training-Batch No.6940\n",
      "Loss = 2.891177177429199\n",
      "Training-Batch No.6950\n",
      "Loss = 3.4429993629455566\n",
      "Training-Batch No.6960\n",
      "Loss = 2.7035229206085205\n",
      "Training-Batch No.6970\n",
      "Loss = 1.8112214803695679\n",
      "Training-Batch No.6980\n",
      "Loss = 3.306511878967285\n",
      "Training-Batch No.6990\n",
      "Loss = 2.3461315631866455\n",
      "Training-Batch No.7000\n",
      "Loss = 2.688011407852173\n",
      "Training-Batch No.7010\n",
      "Loss = 1.8635108470916748\n",
      "Training-Batch No.7020\n",
      "Loss = 2.8581321239471436\n",
      "Training-Batch No.7030\n",
      "Loss = 2.99070405960083\n",
      "Training-Batch No.7040\n",
      "Loss = 2.8222596645355225\n",
      "Training-Batch No.7050\n",
      "Loss = 2.716905117034912\n",
      "Training-Batch No.7060\n",
      "Loss = 2.5838680267333984\n",
      "Training-Batch No.7070\n",
      "Loss = 3.121100425720215\n",
      "Training-Batch No.7080\n",
      "Loss = 2.0946407318115234\n",
      "Training-Batch No.7090\n",
      "Loss = 3.736999273300171\n",
      "Training-Batch No.7100\n",
      "Loss = 2.7458198070526123\n",
      "Training-Batch No.7110\n",
      "Loss = 3.0427863597869873\n",
      "Training-Batch No.7120\n",
      "Loss = 3.496110439300537\n",
      "Training-Batch No.7130\n",
      "Loss = 2.6616804599761963\n",
      "Training-Batch No.7140\n",
      "Loss = 3.0076956748962402\n",
      "Training-Batch No.7150\n",
      "Loss = 3.5164904594421387\n",
      "Training-Batch No.7160\n",
      "Loss = 2.7173221111297607\n",
      "Training-Batch No.7170\n",
      "Loss = 2.901507616043091\n",
      "Training-Batch No.7180\n",
      "Loss = 3.1107754707336426\n",
      "Training-Batch No.7190\n",
      "Loss = 2.993417263031006\n",
      "Training-Batch No.7200\n",
      "Loss = 2.4582433700561523\n",
      "Training-Batch No.7210\n",
      "Loss = 2.8254029750823975\n",
      "Training-Batch No.7220\n",
      "Loss = 2.7435786724090576\n",
      "Training-Batch No.7230\n",
      "Loss = 2.159822940826416\n",
      "Training-Batch No.7240\n",
      "Loss = 2.8916068077087402\n",
      "Training-Batch No.7250\n",
      "Loss = 3.5117645263671875\n",
      "Training-Batch No.7260\n",
      "Loss = 2.6881070137023926\n",
      "Training-Batch No.7270\n",
      "Loss = 2.526064395904541\n",
      "Training-Batch No.7280\n",
      "Loss = 3.754305362701416\n",
      "Training-Batch No.7290\n",
      "Loss = 2.409409761428833\n",
      "Training-Batch No.7300\n",
      "Loss = 2.187108278274536\n",
      "Training-Batch No.7310\n",
      "Loss = 2.8024165630340576\n",
      "Training-Batch No.7320\n",
      "Loss = 2.488241672515869\n",
      "Training-Batch No.7330\n",
      "Loss = 3.006974935531616\n",
      "Training-Batch No.7340\n",
      "Loss = 2.931485414505005\n",
      "Training-Batch No.7350\n",
      "Loss = 3.04400372505188\n",
      "Training-Batch No.7360\n",
      "Loss = 2.9048542976379395\n",
      "Training-Batch No.7370\n",
      "Loss = 2.8270957469940186\n",
      "Training-Batch No.7380\n",
      "Loss = 2.228778600692749\n",
      "Training-Batch No.7390\n",
      "Loss = 2.047295331954956\n",
      "Training-Batch No.7400\n",
      "Loss = 2.4399399757385254\n",
      "Training-Batch No.7410\n",
      "Loss = 2.754847526550293\n",
      "Training-Batch No.7420\n",
      "Loss = 2.792341947555542\n",
      "Training-Batch No.7430\n",
      "Loss = 2.8570594787597656\n",
      "Training-Batch No.7440\n",
      "Loss = 3.1437902450561523\n",
      "Epoch 8 Training Loss: 2.9432\n",
      "Start validation\n",
      "Epoch 8 Validation Loss: 2.9972\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2461021505376344\n",
      "Average Top-3 accuracy 0.3926075268817204\n",
      "Average Top-5 accuracy 0.46196236559139786\n",
      "Average Top-7 accuracy 0.5086021505376344\n",
      "Average Top-9 accuracy 0.5549731182795699\n",
      "Average Top-11 accuracy 0.5913978494623656\n",
      "Average Top-13 accuracy 0.6244623655913979\n",
      "Average Top-15 accuracy 0.6551075268817205\n",
      "current acc 0.2461021505376344\n",
      "best acc 0.2360215053763441\n",
      "Saving the best model\n",
      "updated best accuracy 0.2461021505376344\n",
      "Epoch No. 9\n",
      "Training-Batch No.7450\n",
      "Loss = 3.093065023422241\n",
      "Training-Batch No.7460\n",
      "Loss = 1.96310555934906\n",
      "Training-Batch No.7470\n",
      "Loss = 3.271113634109497\n",
      "Training-Batch No.7480\n",
      "Loss = 3.3852391242980957\n",
      "Training-Batch No.7490\n",
      "Loss = 2.624274253845215\n",
      "Training-Batch No.7500\n",
      "Loss = 3.548478126525879\n",
      "Training-Batch No.7510\n",
      "Loss = 3.4065544605255127\n",
      "Training-Batch No.7520\n",
      "Loss = 3.0127813816070557\n",
      "Training-Batch No.7530\n",
      "Loss = 3.5089313983917236\n",
      "Training-Batch No.7540\n",
      "Loss = 3.4765117168426514\n",
      "Training-Batch No.7550\n",
      "Loss = 2.853328227996826\n",
      "Training-Batch No.7560\n",
      "Loss = 2.960070848464966\n",
      "Training-Batch No.7570\n",
      "Loss = 3.0049822330474854\n",
      "Training-Batch No.7580\n",
      "Loss = 2.956037759780884\n",
      "Training-Batch No.7590\n",
      "Loss = 3.3280506134033203\n",
      "Training-Batch No.7600\n",
      "Loss = 2.728410005569458\n",
      "Training-Batch No.7610\n",
      "Loss = 2.4808168411254883\n",
      "Training-Batch No.7620\n",
      "Loss = 3.297003984451294\n",
      "Training-Batch No.7630\n",
      "Loss = 3.0899622440338135\n",
      "Training-Batch No.7640\n",
      "Loss = 3.4368858337402344\n",
      "Training-Batch No.7650\n",
      "Loss = 3.1573731899261475\n",
      "Training-Batch No.7660\n",
      "Loss = 2.7129385471343994\n",
      "Training-Batch No.7670\n",
      "Loss = 3.0022006034851074\n",
      "Training-Batch No.7680\n",
      "Loss = 3.4315669536590576\n",
      "Training-Batch No.7690\n",
      "Loss = 2.7643985748291016\n",
      "Training-Batch No.7700\n",
      "Loss = 2.148251533508301\n",
      "Training-Batch No.7710\n",
      "Loss = 3.215317487716675\n",
      "Training-Batch No.7720\n",
      "Loss = 3.3470239639282227\n",
      "Training-Batch No.7730\n",
      "Loss = 2.416933059692383\n",
      "Training-Batch No.7740\n",
      "Loss = 2.4337210655212402\n",
      "Training-Batch No.7750\n",
      "Loss = 3.1181352138519287\n",
      "Training-Batch No.7760\n",
      "Loss = 3.2058982849121094\n",
      "Training-Batch No.7770\n",
      "Loss = 3.7479493618011475\n",
      "Training-Batch No.7780\n",
      "Loss = 2.3356168270111084\n",
      "Training-Batch No.7790\n",
      "Loss = 2.7128617763519287\n",
      "Training-Batch No.7800\n",
      "Loss = 3.1588704586029053\n",
      "Training-Batch No.7810\n",
      "Loss = 3.314692974090576\n",
      "Training-Batch No.7820\n",
      "Loss = 3.6080079078674316\n",
      "Training-Batch No.7830\n",
      "Loss = 2.8546302318573\n",
      "Training-Batch No.7840\n",
      "Loss = 3.1033644676208496\n",
      "Training-Batch No.7850\n",
      "Loss = 2.5662407875061035\n",
      "Training-Batch No.7860\n",
      "Loss = 1.8305606842041016\n",
      "Training-Batch No.7870\n",
      "Loss = 2.767014980316162\n",
      "Training-Batch No.7880\n",
      "Loss = 3.198805570602417\n",
      "Training-Batch No.7890\n",
      "Loss = 2.538409471511841\n",
      "Training-Batch No.7900\n",
      "Loss = 1.7789745330810547\n",
      "Training-Batch No.7910\n",
      "Loss = 3.411189079284668\n",
      "Training-Batch No.7920\n",
      "Loss = 2.254549741744995\n",
      "Training-Batch No.7930\n",
      "Loss = 2.553910732269287\n",
      "Training-Batch No.7940\n",
      "Loss = 1.7957326173782349\n",
      "Training-Batch No.7950\n",
      "Loss = 2.734346866607666\n",
      "Training-Batch No.7960\n",
      "Loss = 2.928816795349121\n",
      "Training-Batch No.7970\n",
      "Loss = 2.5370163917541504\n",
      "Training-Batch No.7980\n",
      "Loss = 2.4999330043792725\n",
      "Training-Batch No.7990\n",
      "Loss = 2.4760208129882812\n",
      "Training-Batch No.8000\n",
      "Loss = 3.0023691654205322\n",
      "Training-Batch No.8010\n",
      "Loss = 2.1157450675964355\n",
      "Training-Batch No.8020\n",
      "Loss = 3.9099607467651367\n",
      "Training-Batch No.8030\n",
      "Loss = 2.6713314056396484\n",
      "Training-Batch No.8040\n",
      "Loss = 2.8785948753356934\n",
      "Training-Batch No.8050\n",
      "Loss = 3.4586410522460938\n",
      "Training-Batch No.8060\n",
      "Loss = 2.714367389678955\n",
      "Training-Batch No.8070\n",
      "Loss = 2.927076578140259\n",
      "Training-Batch No.8080\n",
      "Loss = 3.7889742851257324\n",
      "Training-Batch No.8090\n",
      "Loss = 2.656858205795288\n",
      "Training-Batch No.8100\n",
      "Loss = 2.840409517288208\n",
      "Training-Batch No.8110\n",
      "Loss = 2.9655466079711914\n",
      "Training-Batch No.8120\n",
      "Loss = 2.987600326538086\n",
      "Training-Batch No.8130\n",
      "Loss = 2.305170774459839\n",
      "Training-Batch No.8140\n",
      "Loss = 2.658604621887207\n",
      "Training-Batch No.8150\n",
      "Loss = 2.6546385288238525\n",
      "Training-Batch No.8160\n",
      "Loss = 1.9914369583129883\n",
      "Training-Batch No.8170\n",
      "Loss = 2.9629712104797363\n",
      "Training-Batch No.8180\n",
      "Loss = 3.4847021102905273\n",
      "Training-Batch No.8190\n",
      "Loss = 2.7698423862457275\n",
      "Training-Batch No.8200\n",
      "Loss = 2.5373942852020264\n",
      "Training-Batch No.8210\n",
      "Loss = 3.678793430328369\n",
      "Training-Batch No.8220\n",
      "Loss = 2.5470948219299316\n",
      "Training-Batch No.8230\n",
      "Loss = 2.0699527263641357\n",
      "Training-Batch No.8240\n",
      "Loss = 2.3908843994140625\n",
      "Training-Batch No.8250\n",
      "Loss = 2.581026315689087\n",
      "Training-Batch No.8260\n",
      "Loss = 2.8172011375427246\n",
      "Training-Batch No.8270\n",
      "Loss = 2.8695437908172607\n",
      "Training-Batch No.8280\n",
      "Loss = 3.027811050415039\n",
      "Training-Batch No.8290\n",
      "Loss = 2.8189332485198975\n",
      "Training-Batch No.8300\n",
      "Loss = 2.931082010269165\n",
      "Training-Batch No.8310\n",
      "Loss = 2.314079761505127\n",
      "Training-Batch No.8320\n",
      "Loss = 2.1385200023651123\n",
      "Training-Batch No.8330\n",
      "Loss = 2.2739593982696533\n",
      "Training-Batch No.8340\n",
      "Loss = 2.777099847793579\n",
      "Training-Batch No.8350\n",
      "Loss = 2.6553354263305664\n",
      "Training-Batch No.8360\n",
      "Loss = 2.751307487487793\n",
      "Training-Batch No.8370\n",
      "Loss = 3.095837116241455\n",
      "Epoch 9 Training Loss: 2.8951\n",
      "Start validation\n",
      "Epoch 9 Validation Loss: 2.9510\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2536290322580645\n",
      "Average Top-3 accuracy 0.4036290322580645\n",
      "Average Top-5 accuracy 0.4705645161290323\n",
      "Average Top-7 accuracy 0.5213709677419355\n",
      "Average Top-9 accuracy 0.5619623655913979\n",
      "Average Top-11 accuracy 0.6014784946236559\n",
      "Average Top-13 accuracy 0.6356182795698925\n",
      "Average Top-15 accuracy 0.6629032258064517\n",
      "current acc 0.2536290322580645\n",
      "best acc 0.2461021505376344\n",
      "Saving the best model\n",
      "updated best accuracy 0.2536290322580645\n",
      "Epoch No. 10\n",
      "Training-Batch No.8380\n",
      "Loss = 2.9919097423553467\n",
      "Training-Batch No.8390\n",
      "Loss = 1.9199156761169434\n",
      "Training-Batch No.8400\n",
      "Loss = 3.1837246417999268\n",
      "Training-Batch No.8410\n",
      "Loss = 3.3607547283172607\n",
      "Training-Batch No.8420\n",
      "Loss = 2.475050926208496\n",
      "Training-Batch No.8430\n",
      "Loss = 3.63739013671875\n",
      "Training-Batch No.8440\n",
      "Loss = 3.4083476066589355\n",
      "Training-Batch No.8450\n",
      "Loss = 2.875969648361206\n",
      "Training-Batch No.8460\n",
      "Loss = 3.376960515975952\n",
      "Training-Batch No.8470\n",
      "Loss = 3.2240138053894043\n",
      "Training-Batch No.8480\n",
      "Loss = 2.4463765621185303\n",
      "Training-Batch No.8490\n",
      "Loss = 2.940556764602661\n",
      "Training-Batch No.8500\n",
      "Loss = 3.0632693767547607\n",
      "Training-Batch No.8510\n",
      "Loss = 2.9836020469665527\n",
      "Training-Batch No.8520\n",
      "Loss = 3.2958474159240723\n",
      "Training-Batch No.8530\n",
      "Loss = 2.7214910984039307\n",
      "Training-Batch No.8540\n",
      "Loss = 2.3317923545837402\n",
      "Training-Batch No.8550\n",
      "Loss = 3.1305830478668213\n",
      "Training-Batch No.8560\n",
      "Loss = 2.983738422393799\n",
      "Training-Batch No.8570\n",
      "Loss = 3.195716142654419\n",
      "Training-Batch No.8580\n",
      "Loss = 3.084287405014038\n",
      "Training-Batch No.8590\n",
      "Loss = 2.730393409729004\n",
      "Training-Batch No.8600\n",
      "Loss = 2.8181142807006836\n",
      "Training-Batch No.8610\n",
      "Loss = 3.2699053287506104\n",
      "Training-Batch No.8620\n",
      "Loss = 2.7207164764404297\n",
      "Training-Batch No.8630\n",
      "Loss = 2.151334524154663\n",
      "Training-Batch No.8640\n",
      "Loss = 3.059211254119873\n",
      "Training-Batch No.8650\n",
      "Loss = 3.2966227531433105\n",
      "Training-Batch No.8660\n",
      "Loss = 2.36542010307312\n",
      "Training-Batch No.8670\n",
      "Loss = 2.292661666870117\n",
      "Training-Batch No.8680\n",
      "Loss = 3.1643548011779785\n",
      "Training-Batch No.8690\n",
      "Loss = 3.1303365230560303\n",
      "Training-Batch No.8700\n",
      "Loss = 3.8967692852020264\n",
      "Training-Batch No.8710\n",
      "Loss = 2.282224416732788\n",
      "Training-Batch No.8720\n",
      "Loss = 2.519167184829712\n",
      "Training-Batch No.8730\n",
      "Loss = 3.0980865955352783\n",
      "Training-Batch No.8740\n",
      "Loss = 3.2137348651885986\n",
      "Training-Batch No.8750\n",
      "Loss = 3.6205930709838867\n",
      "Training-Batch No.8760\n",
      "Loss = 2.796245813369751\n",
      "Training-Batch No.8770\n",
      "Loss = 3.1517484188079834\n",
      "Training-Batch No.8780\n",
      "Loss = 2.4561829566955566\n",
      "Training-Batch No.8790\n",
      "Loss = 1.7992452383041382\n",
      "Training-Batch No.8800\n",
      "Loss = 2.6974077224731445\n",
      "Training-Batch No.8810\n",
      "Loss = 3.1846325397491455\n",
      "Training-Batch No.8820\n",
      "Loss = 2.709578275680542\n",
      "Training-Batch No.8830\n",
      "Loss = 1.6752756834030151\n",
      "Training-Batch No.8840\n",
      "Loss = 3.4219577312469482\n",
      "Training-Batch No.8850\n",
      "Loss = 2.2792203426361084\n",
      "Training-Batch No.8860\n",
      "Loss = 2.6809499263763428\n",
      "Training-Batch No.8870\n",
      "Loss = 1.7647302150726318\n",
      "Training-Batch No.8880\n",
      "Loss = 2.676466464996338\n",
      "Training-Batch No.8890\n",
      "Loss = 3.019253969192505\n",
      "Training-Batch No.8900\n",
      "Loss = 2.4233579635620117\n",
      "Training-Batch No.8910\n",
      "Loss = 2.4803223609924316\n",
      "Training-Batch No.8920\n",
      "Loss = 2.578875780105591\n",
      "Training-Batch No.8930\n",
      "Loss = 2.9091179370880127\n",
      "Training-Batch No.8940\n",
      "Loss = 2.072826623916626\n",
      "Training-Batch No.8950\n",
      "Loss = 3.6769418716430664\n",
      "Training-Batch No.8960\n",
      "Loss = 2.7027456760406494\n",
      "Training-Batch No.8970\n",
      "Loss = 2.737618923187256\n",
      "Training-Batch No.8980\n",
      "Loss = 3.455726385116577\n",
      "Training-Batch No.8990\n",
      "Loss = 2.7585220336914062\n",
      "Training-Batch No.9000\n",
      "Loss = 2.869943141937256\n",
      "Training-Batch No.9010\n",
      "Loss = 3.445728063583374\n",
      "Training-Batch No.9020\n",
      "Loss = 2.712728977203369\n",
      "Training-Batch No.9030\n",
      "Loss = 2.8057594299316406\n",
      "Training-Batch No.9040\n",
      "Loss = 2.9192230701446533\n",
      "Training-Batch No.9050\n",
      "Loss = 3.019273519515991\n",
      "Training-Batch No.9060\n",
      "Loss = 2.2226998805999756\n",
      "Training-Batch No.9070\n",
      "Loss = 2.670778512954712\n",
      "Training-Batch No.9080\n",
      "Loss = 2.576385021209717\n",
      "Training-Batch No.9090\n",
      "Loss = 1.9575128555297852\n",
      "Training-Batch No.9100\n",
      "Loss = 2.953925609588623\n",
      "Training-Batch No.9110\n",
      "Loss = 3.523442029953003\n",
      "Training-Batch No.9120\n",
      "Loss = 2.8939449787139893\n",
      "Training-Batch No.9130\n",
      "Loss = 2.4794719219207764\n",
      "Training-Batch No.9140\n",
      "Loss = 3.6789627075195312\n",
      "Training-Batch No.9150\n",
      "Loss = 2.622006416320801\n",
      "Training-Batch No.9160\n",
      "Loss = 2.105343818664551\n",
      "Training-Batch No.9170\n",
      "Loss = 2.2788710594177246\n",
      "Training-Batch No.9180\n",
      "Loss = 2.413238048553467\n",
      "Training-Batch No.9190\n",
      "Loss = 2.7837271690368652\n",
      "Training-Batch No.9200\n",
      "Loss = 2.7639858722686768\n",
      "Training-Batch No.9210\n",
      "Loss = 2.9778892993927\n",
      "Training-Batch No.9220\n",
      "Loss = 2.799661159515381\n",
      "Training-Batch No.9230\n",
      "Loss = 2.9311037063598633\n",
      "Training-Batch No.9240\n",
      "Loss = 2.1679046154022217\n",
      "Training-Batch No.9250\n",
      "Loss = 2.1176769733428955\n",
      "Training-Batch No.9260\n",
      "Loss = 2.3203227519989014\n",
      "Training-Batch No.9270\n",
      "Loss = 2.6267459392547607\n",
      "Training-Batch No.9280\n",
      "Loss = 2.531674861907959\n",
      "Training-Batch No.9290\n",
      "Loss = 2.7980399131774902\n",
      "Training-Batch No.9300\n",
      "Loss = 2.993522882461548\n",
      "Epoch 10 Training Loss: 2.8537\n",
      "Start validation\n",
      "Epoch 10 Validation Loss: 2.9509\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2525537634408602\n",
      "Average Top-3 accuracy 0.4099462365591398\n",
      "Average Top-5 accuracy 0.47486559139784945\n",
      "Average Top-7 accuracy 0.5266129032258065\n",
      "Average Top-9 accuracy 0.5651881720430108\n",
      "Average Top-11 accuracy 0.6\n",
      "Average Top-13 accuracy 0.6272849462365592\n",
      "Average Top-15 accuracy 0.660752688172043\n",
      "current acc 0.2525537634408602\n",
      "best acc 0.2536290322580645\n",
      "updated best accuracy 0.2536290322580645\n",
      "Epoch No. 11\n",
      "Training-Batch No.9310\n",
      "Loss = 2.9794199466705322\n",
      "Training-Batch No.9320\n",
      "Loss = 1.9132075309753418\n",
      "Training-Batch No.9330\n",
      "Loss = 3.1047887802124023\n",
      "Training-Batch No.9340\n",
      "Loss = 3.508188486099243\n",
      "Training-Batch No.9350\n",
      "Loss = 2.5168004035949707\n",
      "Training-Batch No.9360\n",
      "Loss = 3.511134147644043\n",
      "Training-Batch No.9370\n",
      "Loss = 3.3035707473754883\n",
      "Training-Batch No.9380\n",
      "Loss = 2.8846826553344727\n",
      "Training-Batch No.9390\n",
      "Loss = 3.396430492401123\n",
      "Training-Batch No.9400\n",
      "Loss = 3.031904458999634\n",
      "Training-Batch No.9410\n",
      "Loss = 2.3754689693450928\n",
      "Training-Batch No.9420\n",
      "Loss = 2.8621761798858643\n",
      "Training-Batch No.9430\n",
      "Loss = 2.84684419631958\n",
      "Training-Batch No.9440\n",
      "Loss = 2.964641571044922\n",
      "Training-Batch No.9450\n",
      "Loss = 3.2164459228515625\n",
      "Training-Batch No.9460\n",
      "Loss = 2.740339756011963\n",
      "Training-Batch No.9470\n",
      "Loss = 2.225304365158081\n",
      "Training-Batch No.9480\n",
      "Loss = 3.066735029220581\n",
      "Training-Batch No.9490\n",
      "Loss = 2.9685375690460205\n",
      "Training-Batch No.9500\n",
      "Loss = 2.995398759841919\n",
      "Training-Batch No.9510\n",
      "Loss = 3.0756053924560547\n",
      "Training-Batch No.9520\n",
      "Loss = 2.6526172161102295\n",
      "Training-Batch No.9530\n",
      "Loss = 2.8486990928649902\n",
      "Training-Batch No.9540\n",
      "Loss = 3.268138885498047\n",
      "Training-Batch No.9550\n",
      "Loss = 2.880521774291992\n",
      "Training-Batch No.9560\n",
      "Loss = 2.1363632678985596\n",
      "Training-Batch No.9570\n",
      "Loss = 3.0875208377838135\n",
      "Training-Batch No.9580\n",
      "Loss = 3.3226466178894043\n",
      "Training-Batch No.9590\n",
      "Loss = 2.393617868423462\n",
      "Training-Batch No.9600\n",
      "Loss = 2.2182021141052246\n",
      "Training-Batch No.9610\n",
      "Loss = 3.0212182998657227\n",
      "Training-Batch No.9620\n",
      "Loss = 2.9373350143432617\n",
      "Training-Batch No.9630\n",
      "Loss = 3.9243080615997314\n",
      "Training-Batch No.9640\n",
      "Loss = 2.302664041519165\n",
      "Training-Batch No.9650\n",
      "Loss = 2.4671359062194824\n",
      "Training-Batch No.9660\n",
      "Loss = 3.219364881515503\n",
      "Training-Batch No.9670\n",
      "Loss = 3.175654649734497\n",
      "Training-Batch No.9680\n",
      "Loss = 3.758727788925171\n",
      "Training-Batch No.9690\n",
      "Loss = 2.8325514793395996\n",
      "Training-Batch No.9700\n",
      "Loss = 3.152893543243408\n",
      "Training-Batch No.9710\n",
      "Loss = 2.437073230743408\n",
      "Training-Batch No.9720\n",
      "Loss = 1.8262724876403809\n",
      "Training-Batch No.9730\n",
      "Loss = 2.736924886703491\n",
      "Training-Batch No.9740\n",
      "Loss = 3.0886881351470947\n",
      "Training-Batch No.9750\n",
      "Loss = 2.5647482872009277\n",
      "Training-Batch No.9760\n",
      "Loss = 1.7249984741210938\n",
      "Training-Batch No.9770\n",
      "Loss = 3.2501211166381836\n",
      "Training-Batch No.9780\n",
      "Loss = 2.2576375007629395\n",
      "Training-Batch No.9790\n",
      "Loss = 2.446803092956543\n",
      "Training-Batch No.9800\n",
      "Loss = 1.7441126108169556\n",
      "Training-Batch No.9810\n",
      "Loss = 2.793806552886963\n",
      "Training-Batch No.9820\n",
      "Loss = 2.995469570159912\n",
      "Training-Batch No.9830\n",
      "Loss = 2.741269826889038\n",
      "Training-Batch No.9840\n",
      "Loss = 2.367142915725708\n",
      "Training-Batch No.9850\n",
      "Loss = 2.4830880165100098\n",
      "Training-Batch No.9860\n",
      "Loss = 2.828190565109253\n",
      "Training-Batch No.9870\n",
      "Loss = 1.9110429286956787\n",
      "Training-Batch No.9880\n",
      "Loss = 3.739759922027588\n",
      "Training-Batch No.9890\n",
      "Loss = 2.7313966751098633\n",
      "Training-Batch No.9900\n",
      "Loss = 2.830950975418091\n",
      "Training-Batch No.9910\n",
      "Loss = 3.90347957611084\n",
      "Training-Batch No.9920\n",
      "Loss = 2.6210193634033203\n",
      "Training-Batch No.9930\n",
      "Loss = 2.915337085723877\n",
      "Training-Batch No.9940\n",
      "Loss = 3.6227872371673584\n",
      "Training-Batch No.9950\n",
      "Loss = 2.64056658744812\n",
      "Training-Batch No.9960\n",
      "Loss = 2.8624231815338135\n",
      "Training-Batch No.9970\n",
      "Loss = 2.9404587745666504\n",
      "Training-Batch No.9980\n",
      "Loss = 2.933516502380371\n",
      "Training-Batch No.9990\n",
      "Loss = 2.180962085723877\n",
      "Training-Batch No.10000\n",
      "Loss = 2.5797712802886963\n",
      "Training-Batch No.10010\n",
      "Loss = 2.5727365016937256\n",
      "Training-Batch No.10020\n",
      "Loss = 1.8972340822219849\n",
      "Training-Batch No.10030\n",
      "Loss = 3.1019840240478516\n",
      "Training-Batch No.10040\n",
      "Loss = 3.490540027618408\n",
      "Training-Batch No.10050\n",
      "Loss = 2.938809633255005\n",
      "Training-Batch No.10060\n",
      "Loss = 2.4838404655456543\n",
      "Training-Batch No.10070\n",
      "Loss = 3.7424416542053223\n",
      "Training-Batch No.10080\n",
      "Loss = 2.485987663269043\n",
      "Training-Batch No.10090\n",
      "Loss = 2.0208301544189453\n",
      "Training-Batch No.10100\n",
      "Loss = 2.34940242767334\n",
      "Training-Batch No.10110\n",
      "Loss = 2.3324179649353027\n",
      "Training-Batch No.10120\n",
      "Loss = 2.8541574478149414\n",
      "Training-Batch No.10130\n",
      "Loss = 2.78027081489563\n",
      "Training-Batch No.10140\n",
      "Loss = 2.8590075969696045\n",
      "Training-Batch No.10150\n",
      "Loss = 2.8308937549591064\n",
      "Training-Batch No.10160\n",
      "Loss = 3.0309345722198486\n",
      "Training-Batch No.10170\n",
      "Loss = 2.25410532951355\n",
      "Training-Batch No.10180\n",
      "Loss = 1.9602783918380737\n",
      "Training-Batch No.10190\n",
      "Loss = 2.2336552143096924\n",
      "Training-Batch No.10200\n",
      "Loss = 2.526069402694702\n",
      "Training-Batch No.10210\n",
      "Loss = 2.6187925338745117\n",
      "Training-Batch No.10220\n",
      "Loss = 2.7593212127685547\n",
      "Training-Batch No.10230\n",
      "Loss = 3.0065019130706787\n",
      "Epoch 11 Training Loss: 2.8211\n",
      "Start validation\n",
      "Epoch 11 Validation Loss: 2.9508\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.25416666666666665\n",
      "Average Top-3 accuracy 0.40698924731182795\n",
      "Average Top-5 accuracy 0.4790322580645161\n",
      "Average Top-7 accuracy 0.5219086021505376\n",
      "Average Top-9 accuracy 0.5619623655913979\n",
      "Average Top-11 accuracy 0.5985215053763441\n",
      "Average Top-13 accuracy 0.6306451612903226\n",
      "Average Top-15 accuracy 0.6622311827956989\n",
      "current acc 0.25416666666666665\n",
      "best acc 0.2536290322580645\n",
      "Saving the best model\n",
      "updated best accuracy 0.25416666666666665\n",
      "Epoch No. 12\n",
      "Training-Batch No.10240\n",
      "Loss = 2.946053981781006\n",
      "Training-Batch No.10250\n",
      "Loss = 1.9118261337280273\n",
      "Training-Batch No.10260\n",
      "Loss = 3.115839719772339\n",
      "Training-Batch No.10270\n",
      "Loss = 3.5369420051574707\n",
      "Training-Batch No.10280\n",
      "Loss = 2.6478378772735596\n",
      "Training-Batch No.10290\n",
      "Loss = 3.5005829334259033\n",
      "Training-Batch No.10300\n",
      "Loss = 3.3823912143707275\n",
      "Training-Batch No.10310\n",
      "Loss = 2.7866408824920654\n",
      "Training-Batch No.10320\n",
      "Loss = 3.556950569152832\n",
      "Training-Batch No.10330\n",
      "Loss = 3.0909624099731445\n",
      "Training-Batch No.10340\n",
      "Loss = 2.337888717651367\n",
      "Training-Batch No.10350\n",
      "Loss = 2.9073121547698975\n",
      "Training-Batch No.10360\n",
      "Loss = 2.862964153289795\n",
      "Training-Batch No.10370\n",
      "Loss = 2.942936897277832\n",
      "Training-Batch No.10380\n",
      "Loss = 3.2978169918060303\n",
      "Training-Batch No.10390\n",
      "Loss = 2.6965019702911377\n",
      "Training-Batch No.10400\n",
      "Loss = 2.296604633331299\n",
      "Training-Batch No.10410\n",
      "Loss = 3.198342800140381\n",
      "Training-Batch No.10420\n",
      "Loss = 3.0276896953582764\n",
      "Training-Batch No.10430\n",
      "Loss = 3.100968837738037\n",
      "Training-Batch No.10440\n",
      "Loss = 3.0938720703125\n",
      "Training-Batch No.10450\n",
      "Loss = 2.604766607284546\n",
      "Training-Batch No.10460\n",
      "Loss = 2.8668718338012695\n",
      "Training-Batch No.10470\n",
      "Loss = 3.3252408504486084\n",
      "Training-Batch No.10480\n",
      "Loss = 2.835188150405884\n",
      "Training-Batch No.10490\n",
      "Loss = 2.0640451908111572\n",
      "Training-Batch No.10500\n",
      "Loss = 3.220198392868042\n",
      "Training-Batch No.10510\n",
      "Loss = 3.3634839057922363\n",
      "Training-Batch No.10520\n",
      "Loss = 2.3319032192230225\n",
      "Training-Batch No.10530\n",
      "Loss = 2.329599142074585\n",
      "Training-Batch No.10540\n",
      "Loss = 3.124927282333374\n",
      "Training-Batch No.10550\n",
      "Loss = 2.942216634750366\n",
      "Training-Batch No.10560\n",
      "Loss = 3.7464334964752197\n",
      "Training-Batch No.10570\n",
      "Loss = 2.2453837394714355\n",
      "Training-Batch No.10580\n",
      "Loss = 2.2890093326568604\n",
      "Training-Batch No.10590\n",
      "Loss = 3.084747314453125\n",
      "Training-Batch No.10600\n",
      "Loss = 3.251194477081299\n",
      "Training-Batch No.10610\n",
      "Loss = 3.901970863342285\n",
      "Training-Batch No.10620\n",
      "Loss = 2.6365344524383545\n",
      "Training-Batch No.10630\n",
      "Loss = 3.058030605316162\n",
      "Training-Batch No.10640\n",
      "Loss = 2.456559658050537\n",
      "Training-Batch No.10650\n",
      "Loss = 1.7512495517730713\n",
      "Training-Batch No.10660\n",
      "Loss = 2.768678665161133\n",
      "Training-Batch No.10670\n",
      "Loss = 2.97597336769104\n",
      "Training-Batch No.10680\n",
      "Loss = 2.719866991043091\n",
      "Training-Batch No.10690\n",
      "Loss = 1.7563303709030151\n",
      "Training-Batch No.10700\n",
      "Loss = 3.210859537124634\n",
      "Training-Batch No.10710\n",
      "Loss = 2.179851531982422\n",
      "Training-Batch No.10720\n",
      "Loss = 2.7202811241149902\n",
      "Training-Batch No.10730\n",
      "Loss = 1.6615560054779053\n",
      "Training-Batch No.10740\n",
      "Loss = 2.8518407344818115\n",
      "Training-Batch No.10750\n",
      "Loss = 2.98559308052063\n",
      "Training-Batch No.10760\n",
      "Loss = 2.5206758975982666\n",
      "Training-Batch No.10770\n",
      "Loss = 2.363053798675537\n",
      "Training-Batch No.10780\n",
      "Loss = 2.4860100746154785\n",
      "Training-Batch No.10790\n",
      "Loss = 2.805535078048706\n",
      "Training-Batch No.10800\n",
      "Loss = 1.9712326526641846\n",
      "Training-Batch No.10810\n",
      "Loss = 3.9882359504699707\n",
      "Training-Batch No.10820\n",
      "Loss = 2.569390296936035\n",
      "Training-Batch No.10830\n",
      "Loss = 2.7727015018463135\n",
      "Training-Batch No.10840\n",
      "Loss = 3.4637060165405273\n",
      "Training-Batch No.10850\n",
      "Loss = 2.6120941638946533\n",
      "Training-Batch No.10860\n",
      "Loss = 2.7620551586151123\n",
      "Training-Batch No.10870\n",
      "Loss = 3.4363579750061035\n",
      "Training-Batch No.10880\n",
      "Loss = 2.6458041667938232\n",
      "Training-Batch No.10890\n",
      "Loss = 3.041944980621338\n",
      "Training-Batch No.10900\n",
      "Loss = 2.929137945175171\n",
      "Training-Batch No.10910\n",
      "Loss = 2.8664066791534424\n",
      "Training-Batch No.10920\n",
      "Loss = 2.225139617919922\n",
      "Training-Batch No.10930\n",
      "Loss = 2.796295404434204\n",
      "Training-Batch No.10940\n",
      "Loss = 2.626192092895508\n",
      "Training-Batch No.10950\n",
      "Loss = 1.9398117065429688\n",
      "Training-Batch No.10960\n",
      "Loss = 2.8764278888702393\n",
      "Training-Batch No.10970\n",
      "Loss = 3.3326659202575684\n",
      "Training-Batch No.10980\n",
      "Loss = 2.9357986450195312\n",
      "Training-Batch No.10990\n",
      "Loss = 2.4695217609405518\n",
      "Training-Batch No.11000\n",
      "Loss = 3.670400381088257\n",
      "Training-Batch No.11010\n",
      "Loss = 2.388420581817627\n",
      "Training-Batch No.11020\n",
      "Loss = 1.957126498222351\n",
      "Training-Batch No.11030\n",
      "Loss = 2.3568639755249023\n",
      "Training-Batch No.11040\n",
      "Loss = 2.3582940101623535\n",
      "Training-Batch No.11050\n",
      "Loss = 2.762223958969116\n",
      "Training-Batch No.11060\n",
      "Loss = 2.7476298809051514\n",
      "Training-Batch No.11070\n",
      "Loss = 2.7873270511627197\n",
      "Training-Batch No.11080\n",
      "Loss = 2.820751905441284\n",
      "Training-Batch No.11090\n",
      "Loss = 3.0434327125549316\n",
      "Training-Batch No.11100\n",
      "Loss = 2.1520166397094727\n",
      "Training-Batch No.11110\n",
      "Loss = 1.9990407228469849\n",
      "Training-Batch No.11120\n",
      "Loss = 2.1867740154266357\n",
      "Training-Batch No.11130\n",
      "Loss = 2.543018341064453\n",
      "Training-Batch No.11140\n",
      "Loss = 2.6482486724853516\n",
      "Training-Batch No.11150\n",
      "Loss = 2.7915773391723633\n",
      "Training-Batch No.11160\n",
      "Loss = 2.939431667327881\n",
      "Epoch 12 Training Loss: 2.7922\n",
      "Start validation\n",
      "Epoch 12 Validation Loss: 2.9414\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2645161290322581\n",
      "Average Top-3 accuracy 0.41303763440860214\n",
      "Average Top-5 accuracy 0.4801075268817204\n",
      "Average Top-7 accuracy 0.5286290322580646\n",
      "Average Top-9 accuracy 0.5692204301075269\n",
      "Average Top-11 accuracy 0.605241935483871\n",
      "Average Top-13 accuracy 0.6353494623655914\n",
      "Average Top-15 accuracy 0.6635752688172043\n",
      "current acc 0.2645161290322581\n",
      "best acc 0.25416666666666665\n",
      "Saving the best model\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 13\n",
      "Training-Batch No.11170\n",
      "Loss = 3.008212089538574\n",
      "Training-Batch No.11180\n",
      "Loss = 2.0377848148345947\n",
      "Training-Batch No.11190\n",
      "Loss = 3.225026845932007\n",
      "Training-Batch No.11200\n",
      "Loss = 3.3484930992126465\n",
      "Training-Batch No.11210\n",
      "Loss = 2.6803858280181885\n",
      "Training-Batch No.11220\n",
      "Loss = 3.4679136276245117\n",
      "Training-Batch No.11230\n",
      "Loss = 3.3053348064422607\n",
      "Training-Batch No.11240\n",
      "Loss = 2.8723912239074707\n",
      "Training-Batch No.11250\n",
      "Loss = 3.21097469329834\n",
      "Training-Batch No.11260\n",
      "Loss = 3.0008113384246826\n",
      "Training-Batch No.11270\n",
      "Loss = 2.3262219429016113\n",
      "Training-Batch No.11280\n",
      "Loss = 2.862910032272339\n",
      "Training-Batch No.11290\n",
      "Loss = 2.649043560028076\n",
      "Training-Batch No.11300\n",
      "Loss = 2.925098180770874\n",
      "Training-Batch No.11310\n",
      "Loss = 3.1369967460632324\n",
      "Training-Batch No.11320\n",
      "Loss = 2.6146647930145264\n",
      "Training-Batch No.11330\n",
      "Loss = 2.2648420333862305\n",
      "Training-Batch No.11340\n",
      "Loss = 3.1299564838409424\n",
      "Training-Batch No.11350\n",
      "Loss = 2.910837173461914\n",
      "Training-Batch No.11360\n",
      "Loss = 3.08140230178833\n",
      "Training-Batch No.11370\n",
      "Loss = 2.882354736328125\n",
      "Training-Batch No.11380\n",
      "Loss = 2.5814995765686035\n",
      "Training-Batch No.11390\n",
      "Loss = 2.9425172805786133\n",
      "Training-Batch No.11400\n",
      "Loss = 3.123096227645874\n",
      "Training-Batch No.11410\n",
      "Loss = 2.757413387298584\n",
      "Training-Batch No.11420\n",
      "Loss = 2.205918788909912\n",
      "Training-Batch No.11430\n",
      "Loss = 3.0201051235198975\n",
      "Training-Batch No.11440\n",
      "Loss = 3.470106601715088\n",
      "Training-Batch No.11450\n",
      "Loss = 2.362001657485962\n",
      "Training-Batch No.11460\n",
      "Loss = 2.1627488136291504\n",
      "Training-Batch No.11470\n",
      "Loss = 2.9646553993225098\n",
      "Training-Batch No.11480\n",
      "Loss = 2.936764717102051\n",
      "Training-Batch No.11490\n",
      "Loss = 3.761223554611206\n",
      "Training-Batch No.11500\n",
      "Loss = 2.264467239379883\n",
      "Training-Batch No.11510\n",
      "Loss = 2.274317979812622\n",
      "Training-Batch No.11520\n",
      "Loss = 3.065666437149048\n",
      "Training-Batch No.11530\n",
      "Loss = 3.2569713592529297\n",
      "Training-Batch No.11540\n",
      "Loss = 4.047871112823486\n",
      "Training-Batch No.11550\n",
      "Loss = 2.6491751670837402\n",
      "Training-Batch No.11560\n",
      "Loss = 3.000849485397339\n",
      "Training-Batch No.11570\n",
      "Loss = 2.3447999954223633\n",
      "Training-Batch No.11580\n",
      "Loss = 1.8081127405166626\n",
      "Training-Batch No.11590\n",
      "Loss = 2.7349226474761963\n",
      "Training-Batch No.11600\n",
      "Loss = 2.970151424407959\n",
      "Training-Batch No.11610\n",
      "Loss = 2.5342979431152344\n",
      "Training-Batch No.11620\n",
      "Loss = 1.6744555234909058\n",
      "Training-Batch No.11630\n",
      "Loss = 3.1505398750305176\n",
      "Training-Batch No.11640\n",
      "Loss = 2.2280356884002686\n",
      "Training-Batch No.11650\n",
      "Loss = 2.609248638153076\n",
      "Training-Batch No.11660\n",
      "Loss = 1.8394290208816528\n",
      "Training-Batch No.11670\n",
      "Loss = 2.7138352394104004\n",
      "Training-Batch No.11680\n",
      "Loss = 3.093759059906006\n",
      "Training-Batch No.11690\n",
      "Loss = 2.521775245666504\n",
      "Training-Batch No.11700\n",
      "Loss = 2.2425107955932617\n",
      "Training-Batch No.11710\n",
      "Loss = 2.5511300563812256\n",
      "Training-Batch No.11720\n",
      "Loss = 2.8438069820404053\n",
      "Training-Batch No.11730\n",
      "Loss = 1.9149646759033203\n",
      "Training-Batch No.11740\n",
      "Loss = 3.721485137939453\n",
      "Training-Batch No.11750\n",
      "Loss = 2.5624969005584717\n",
      "Training-Batch No.11760\n",
      "Loss = 2.6749496459960938\n",
      "Training-Batch No.11770\n",
      "Loss = 3.530329942703247\n",
      "Training-Batch No.11780\n",
      "Loss = 2.4162657260894775\n",
      "Training-Batch No.11790\n",
      "Loss = 2.820762872695923\n",
      "Training-Batch No.11800\n",
      "Loss = 3.412532091140747\n",
      "Training-Batch No.11810\n",
      "Loss = 2.623731851577759\n",
      "Training-Batch No.11820\n",
      "Loss = 2.7547409534454346\n",
      "Training-Batch No.11830\n",
      "Loss = 2.9043662548065186\n",
      "Training-Batch No.11840\n",
      "Loss = 2.7613000869750977\n",
      "Training-Batch No.11850\n",
      "Loss = 2.0808870792388916\n",
      "Training-Batch No.11860\n",
      "Loss = 2.4963653087615967\n",
      "Training-Batch No.11870\n",
      "Loss = 2.5003514289855957\n",
      "Training-Batch No.11880\n",
      "Loss = 1.9184212684631348\n",
      "Training-Batch No.11890\n",
      "Loss = 2.895726203918457\n",
      "Training-Batch No.11900\n",
      "Loss = 3.394104480743408\n",
      "Training-Batch No.11910\n",
      "Loss = 2.9481935501098633\n",
      "Training-Batch No.11920\n",
      "Loss = 2.4679744243621826\n",
      "Training-Batch No.11930\n",
      "Loss = 3.703216552734375\n",
      "Training-Batch No.11940\n",
      "Loss = 2.5159599781036377\n",
      "Training-Batch No.11950\n",
      "Loss = 1.919694185256958\n",
      "Training-Batch No.11960\n",
      "Loss = 2.3430163860321045\n",
      "Training-Batch No.11970\n",
      "Loss = 2.25934100151062\n",
      "Training-Batch No.11980\n",
      "Loss = 2.8568034172058105\n",
      "Training-Batch No.11990\n",
      "Loss = 2.797621965408325\n",
      "Training-Batch No.12000\n",
      "Loss = 2.8867084980010986\n",
      "Training-Batch No.12010\n",
      "Loss = 2.7919921875\n",
      "Training-Batch No.12020\n",
      "Loss = 3.190964937210083\n",
      "Training-Batch No.12030\n",
      "Loss = 2.1575098037719727\n",
      "Training-Batch No.12040\n",
      "Loss = 2.0158743858337402\n",
      "Training-Batch No.12050\n",
      "Loss = 2.1151158809661865\n",
      "Training-Batch No.12060\n",
      "Loss = 2.517566680908203\n",
      "Training-Batch No.12070\n",
      "Loss = 2.511073589324951\n",
      "Training-Batch No.12080\n",
      "Loss = 2.6599783897399902\n",
      "Training-Batch No.12090\n",
      "Loss = 2.9495279788970947\n",
      "Epoch 13 Training Loss: 2.7606\n",
      "Start validation\n",
      "Epoch 13 Validation Loss: 2.9283\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2600806451612903\n",
      "Average Top-3 accuracy 0.4245967741935484\n",
      "Average Top-5 accuracy 0.4838709677419355\n",
      "Average Top-7 accuracy 0.5306451612903226\n",
      "Average Top-9 accuracy 0.5694892473118279\n",
      "Average Top-11 accuracy 0.6053763440860215\n",
      "Average Top-13 accuracy 0.6326612903225807\n",
      "Average Top-15 accuracy 0.6631720430107527\n",
      "current acc 0.2600806451612903\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 14\n",
      "Training-Batch No.12100\n",
      "Loss = 2.914769411087036\n",
      "Training-Batch No.12110\n",
      "Loss = 1.9612220525741577\n",
      "Training-Batch No.12120\n",
      "Loss = 3.175163507461548\n",
      "Training-Batch No.12130\n",
      "Loss = 3.364959716796875\n",
      "Training-Batch No.12140\n",
      "Loss = 2.494736671447754\n",
      "Training-Batch No.12150\n",
      "Loss = 3.328508138656616\n",
      "Training-Batch No.12160\n",
      "Loss = 3.4508609771728516\n",
      "Training-Batch No.12170\n",
      "Loss = 2.6546034812927246\n",
      "Training-Batch No.12180\n",
      "Loss = 3.3108291625976562\n",
      "Training-Batch No.12190\n",
      "Loss = 2.9821691513061523\n",
      "Training-Batch No.12200\n",
      "Loss = 2.4566099643707275\n",
      "Training-Batch No.12210\n",
      "Loss = 2.85150146484375\n",
      "Training-Batch No.12220\n",
      "Loss = 2.6013782024383545\n",
      "Training-Batch No.12230\n",
      "Loss = 2.716176986694336\n",
      "Training-Batch No.12240\n",
      "Loss = 3.314377546310425\n",
      "Training-Batch No.12250\n",
      "Loss = 2.713559865951538\n",
      "Training-Batch No.12260\n",
      "Loss = 2.1725170612335205\n",
      "Training-Batch No.12270\n",
      "Loss = 3.1962392330169678\n",
      "Training-Batch No.12280\n",
      "Loss = 2.8020265102386475\n",
      "Training-Batch No.12290\n",
      "Loss = 3.1605236530303955\n",
      "Training-Batch No.12300\n",
      "Loss = 2.843106746673584\n",
      "Training-Batch No.12310\n",
      "Loss = 2.5469038486480713\n",
      "Training-Batch No.12320\n",
      "Loss = 3.023414134979248\n",
      "Training-Batch No.12330\n",
      "Loss = 3.323690891265869\n",
      "Training-Batch No.12340\n",
      "Loss = 2.791388988494873\n",
      "Training-Batch No.12350\n",
      "Loss = 1.9063520431518555\n",
      "Training-Batch No.12360\n",
      "Loss = 3.098231315612793\n",
      "Training-Batch No.12370\n",
      "Loss = 3.408334493637085\n",
      "Training-Batch No.12380\n",
      "Loss = 2.3019752502441406\n",
      "Training-Batch No.12390\n",
      "Loss = 2.0631496906280518\n",
      "Training-Batch No.12400\n",
      "Loss = 2.9232265949249268\n",
      "Training-Batch No.12410\n",
      "Loss = 2.850834608078003\n",
      "Training-Batch No.12420\n",
      "Loss = 3.6168582439422607\n",
      "Training-Batch No.12430\n",
      "Loss = 2.2505152225494385\n",
      "Training-Batch No.12440\n",
      "Loss = 2.1365673542022705\n",
      "Training-Batch No.12450\n",
      "Loss = 3.0787909030914307\n",
      "Training-Batch No.12460\n",
      "Loss = 3.408024787902832\n",
      "Training-Batch No.12470\n",
      "Loss = 3.9264819622039795\n",
      "Training-Batch No.12480\n",
      "Loss = 2.571396589279175\n",
      "Training-Batch No.12490\n",
      "Loss = 2.862100601196289\n",
      "Training-Batch No.12500\n",
      "Loss = 2.3601667881011963\n",
      "Training-Batch No.12510\n",
      "Loss = 1.776970386505127\n",
      "Training-Batch No.12520\n",
      "Loss = 2.6957781314849854\n",
      "Training-Batch No.12530\n",
      "Loss = 2.843571901321411\n",
      "Training-Batch No.12540\n",
      "Loss = 2.624645948410034\n",
      "Training-Batch No.12550\n",
      "Loss = 1.7743849754333496\n",
      "Training-Batch No.12560\n",
      "Loss = 3.141763210296631\n",
      "Training-Batch No.12570\n",
      "Loss = 2.2025527954101562\n",
      "Training-Batch No.12580\n",
      "Loss = 2.1993460655212402\n",
      "Training-Batch No.12590\n",
      "Loss = 1.649404764175415\n",
      "Training-Batch No.12600\n",
      "Loss = 2.7021689414978027\n",
      "Training-Batch No.12610\n",
      "Loss = 2.972663640975952\n",
      "Training-Batch No.12620\n",
      "Loss = 2.4178361892700195\n",
      "Training-Batch No.12630\n",
      "Loss = 2.34122371673584\n",
      "Training-Batch No.12640\n",
      "Loss = 2.432701826095581\n",
      "Training-Batch No.12650\n",
      "Loss = 2.753655433654785\n",
      "Training-Batch No.12660\n",
      "Loss = 1.987552523612976\n",
      "Training-Batch No.12670\n",
      "Loss = 3.869694232940674\n",
      "Training-Batch No.12680\n",
      "Loss = 2.583678960800171\n",
      "Training-Batch No.12690\n",
      "Loss = 2.631497621536255\n",
      "Training-Batch No.12700\n",
      "Loss = 3.289952516555786\n",
      "Training-Batch No.12710\n",
      "Loss = 2.4232730865478516\n",
      "Training-Batch No.12720\n",
      "Loss = 2.8210015296936035\n",
      "Training-Batch No.12730\n",
      "Loss = 3.4682517051696777\n",
      "Training-Batch No.12740\n",
      "Loss = 2.4288899898529053\n",
      "Training-Batch No.12750\n",
      "Loss = 2.6736316680908203\n",
      "Training-Batch No.12760\n",
      "Loss = 2.875131845474243\n",
      "Training-Batch No.12770\n",
      "Loss = 2.7222185134887695\n",
      "Training-Batch No.12780\n",
      "Loss = 2.0956597328186035\n",
      "Training-Batch No.12790\n",
      "Loss = 2.6880810260772705\n",
      "Training-Batch No.12800\n",
      "Loss = 2.258089542388916\n",
      "Training-Batch No.12810\n",
      "Loss = 1.702480435371399\n",
      "Training-Batch No.12820\n",
      "Loss = 2.988790273666382\n",
      "Training-Batch No.12830\n",
      "Loss = 3.419750928878784\n",
      "Training-Batch No.12840\n",
      "Loss = 2.9264469146728516\n",
      "Training-Batch No.12850\n",
      "Loss = 2.4476044178009033\n",
      "Training-Batch No.12860\n",
      "Loss = 3.731642723083496\n",
      "Training-Batch No.12870\n",
      "Loss = 2.4018821716308594\n",
      "Training-Batch No.12880\n",
      "Loss = 1.9071540832519531\n",
      "Training-Batch No.12890\n",
      "Loss = 2.2383158206939697\n",
      "Training-Batch No.12900\n",
      "Loss = 2.239274263381958\n",
      "Training-Batch No.12910\n",
      "Loss = 2.58665132522583\n",
      "Training-Batch No.12920\n",
      "Loss = 2.7738327980041504\n",
      "Training-Batch No.12930\n",
      "Loss = 2.7654223442077637\n",
      "Training-Batch No.12940\n",
      "Loss = 2.7939414978027344\n",
      "Training-Batch No.12950\n",
      "Loss = 3.108614206314087\n",
      "Training-Batch No.12960\n",
      "Loss = 2.0720396041870117\n",
      "Training-Batch No.12970\n",
      "Loss = 2.00187349319458\n",
      "Training-Batch No.12980\n",
      "Loss = 1.948630690574646\n",
      "Training-Batch No.12990\n",
      "Loss = 2.465024471282959\n",
      "Training-Batch No.13000\n",
      "Loss = 2.286111831665039\n",
      "Training-Batch No.13010\n",
      "Loss = 2.4783756732940674\n",
      "Training-Batch No.13020\n",
      "Loss = 2.902026414871216\n",
      "Epoch 14 Training Loss: 2.7189\n",
      "Start validation\n",
      "Epoch 14 Validation Loss: 2.9403\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2600806451612903\n",
      "Average Top-3 accuracy 0.41774193548387095\n",
      "Average Top-5 accuracy 0.4823924731182796\n",
      "Average Top-7 accuracy 0.532258064516129\n",
      "Average Top-9 accuracy 0.5720430107526882\n",
      "Average Top-11 accuracy 0.605510752688172\n",
      "Average Top-13 accuracy 0.6372311827956989\n",
      "Average Top-15 accuracy 0.6684139784946237\n",
      "current acc 0.2600806451612903\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 15\n",
      "Training-Batch No.13030\n",
      "Loss = 2.7525081634521484\n",
      "Training-Batch No.13040\n",
      "Loss = 1.9554463624954224\n",
      "Training-Batch No.13050\n",
      "Loss = 3.2028887271881104\n",
      "Training-Batch No.13060\n",
      "Loss = 3.1368253231048584\n",
      "Training-Batch No.13070\n",
      "Loss = 2.3683884143829346\n",
      "Training-Batch No.13080\n",
      "Loss = 3.322732448577881\n",
      "Training-Batch No.13090\n",
      "Loss = 3.3425967693328857\n",
      "Training-Batch No.13100\n",
      "Loss = 2.6514389514923096\n",
      "Training-Batch No.13110\n",
      "Loss = 3.3169896602630615\n",
      "Training-Batch No.13120\n",
      "Loss = 3.0115137100219727\n",
      "Training-Batch No.13130\n",
      "Loss = 2.2444496154785156\n",
      "Training-Batch No.13140\n",
      "Loss = 2.757108688354492\n",
      "Training-Batch No.13150\n",
      "Loss = 2.4777915477752686\n",
      "Training-Batch No.13160\n",
      "Loss = 2.675689220428467\n",
      "Training-Batch No.13170\n",
      "Loss = 3.2100534439086914\n",
      "Training-Batch No.13180\n",
      "Loss = 2.580432891845703\n",
      "Training-Batch No.13190\n",
      "Loss = 2.1608922481536865\n",
      "Training-Batch No.13200\n",
      "Loss = 3.1022143363952637\n",
      "Training-Batch No.13210\n",
      "Loss = 2.926748752593994\n",
      "Training-Batch No.13220\n",
      "Loss = 2.9176576137542725\n",
      "Training-Batch No.13230\n",
      "Loss = 2.8821144104003906\n",
      "Training-Batch No.13240\n",
      "Loss = 2.5665581226348877\n",
      "Training-Batch No.13250\n",
      "Loss = 2.810563325881958\n",
      "Training-Batch No.13260\n",
      "Loss = 3.3336403369903564\n",
      "Training-Batch No.13270\n",
      "Loss = 2.6800453662872314\n",
      "Training-Batch No.13280\n",
      "Loss = 2.096644401550293\n",
      "Training-Batch No.13290\n",
      "Loss = 2.982515811920166\n",
      "Training-Batch No.13300\n",
      "Loss = 3.509181022644043\n",
      "Training-Batch No.13310\n",
      "Loss = 2.526609420776367\n",
      "Training-Batch No.13320\n",
      "Loss = 2.1174116134643555\n",
      "Training-Batch No.13330\n",
      "Loss = 3.0979557037353516\n",
      "Training-Batch No.13340\n",
      "Loss = 2.9678821563720703\n",
      "Training-Batch No.13350\n",
      "Loss = 3.6188321113586426\n",
      "Training-Batch No.13360\n",
      "Loss = 2.2035446166992188\n",
      "Training-Batch No.13370\n",
      "Loss = 2.0852952003479004\n",
      "Training-Batch No.13380\n",
      "Loss = 3.1335315704345703\n",
      "Training-Batch No.13390\n",
      "Loss = 3.092257261276245\n",
      "Training-Batch No.13400\n",
      "Loss = 3.6896109580993652\n",
      "Training-Batch No.13410\n",
      "Loss = 2.478736400604248\n",
      "Training-Batch No.13420\n",
      "Loss = 3.0070438385009766\n",
      "Training-Batch No.13430\n",
      "Loss = 2.3200252056121826\n",
      "Training-Batch No.13440\n",
      "Loss = 1.960843801498413\n",
      "Training-Batch No.13450\n",
      "Loss = 2.6915767192840576\n",
      "Training-Batch No.13460\n",
      "Loss = 2.8571128845214844\n",
      "Training-Batch No.13470\n",
      "Loss = 2.4599578380584717\n",
      "Training-Batch No.13480\n",
      "Loss = 1.7705157995224\n",
      "Training-Batch No.13490\n",
      "Loss = 3.0371503829956055\n",
      "Training-Batch No.13500\n",
      "Loss = 2.2490017414093018\n",
      "Training-Batch No.13510\n",
      "Loss = 2.3347346782684326\n",
      "Training-Batch No.13520\n",
      "Loss = 1.6742762327194214\n",
      "Training-Batch No.13530\n",
      "Loss = 2.605421781539917\n",
      "Training-Batch No.13540\n",
      "Loss = 3.058195114135742\n",
      "Training-Batch No.13550\n",
      "Loss = 2.476210832595825\n",
      "Training-Batch No.13560\n",
      "Loss = 2.1620171070098877\n",
      "Training-Batch No.13570\n",
      "Loss = 2.2603304386138916\n",
      "Training-Batch No.13580\n",
      "Loss = 2.6631646156311035\n",
      "Training-Batch No.13590\n",
      "Loss = 1.972041130065918\n",
      "Training-Batch No.13600\n",
      "Loss = 3.855424404144287\n",
      "Training-Batch No.13610\n",
      "Loss = 2.664661407470703\n",
      "Training-Batch No.13620\n",
      "Loss = 2.6646385192871094\n",
      "Training-Batch No.13630\n",
      "Loss = 3.339642286300659\n",
      "Training-Batch No.13640\n",
      "Loss = 2.483417272567749\n",
      "Training-Batch No.13650\n",
      "Loss = 2.7078332901000977\n",
      "Training-Batch No.13660\n",
      "Loss = 3.1957521438598633\n",
      "Training-Batch No.13670\n",
      "Loss = 2.3727426528930664\n",
      "Training-Batch No.13680\n",
      "Loss = 2.6473166942596436\n",
      "Training-Batch No.13690\n",
      "Loss = 2.945509433746338\n",
      "Training-Batch No.13700\n",
      "Loss = 2.5687520503997803\n",
      "Training-Batch No.13710\n",
      "Loss = 2.0834174156188965\n",
      "Training-Batch No.13720\n",
      "Loss = 2.682626485824585\n",
      "Training-Batch No.13730\n",
      "Loss = 2.399104356765747\n",
      "Training-Batch No.13740\n",
      "Loss = 1.790528655052185\n",
      "Training-Batch No.13750\n",
      "Loss = 3.1858153343200684\n",
      "Training-Batch No.13760\n",
      "Loss = 3.4746618270874023\n",
      "Training-Batch No.13770\n",
      "Loss = 2.994363307952881\n",
      "Training-Batch No.13780\n",
      "Loss = 2.500415563583374\n",
      "Training-Batch No.13790\n",
      "Loss = 3.5067498683929443\n",
      "Training-Batch No.13800\n",
      "Loss = 2.3790812492370605\n",
      "Training-Batch No.13810\n",
      "Loss = 1.86077880859375\n",
      "Training-Batch No.13820\n",
      "Loss = 2.284191370010376\n",
      "Training-Batch No.13830\n",
      "Loss = 2.16738224029541\n",
      "Training-Batch No.13840\n",
      "Loss = 2.6194260120391846\n",
      "Training-Batch No.13850\n",
      "Loss = 2.6738340854644775\n",
      "Training-Batch No.13860\n",
      "Loss = 2.792569398880005\n",
      "Training-Batch No.13870\n",
      "Loss = 2.853083610534668\n",
      "Training-Batch No.13880\n",
      "Loss = 2.929386615753174\n",
      "Training-Batch No.13890\n",
      "Loss = 2.119976043701172\n",
      "Training-Batch No.13900\n",
      "Loss = 1.9756470918655396\n",
      "Training-Batch No.13910\n",
      "Loss = 1.9194600582122803\n",
      "Training-Batch No.13920\n",
      "Loss = 2.4962117671966553\n",
      "Training-Batch No.13930\n",
      "Loss = 2.3939149379730225\n",
      "Training-Batch No.13940\n",
      "Loss = 2.490952730178833\n",
      "Training-Batch No.13950\n",
      "Loss = 2.8266873359680176\n",
      "Epoch 15 Training Loss: 2.6984\n",
      "Start validation\n",
      "Epoch 15 Validation Loss: 2.9883\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2630376344086022\n",
      "Average Top-3 accuracy 0.4120967741935484\n",
      "Average Top-5 accuracy 0.47661290322580646\n",
      "Average Top-7 accuracy 0.5245967741935483\n",
      "Average Top-9 accuracy 0.562768817204301\n",
      "Average Top-11 accuracy 0.5975806451612903\n",
      "Average Top-13 accuracy 0.6275537634408602\n",
      "Average Top-15 accuracy 0.656989247311828\n",
      "current acc 0.2630376344086022\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 16\n",
      "Training-Batch No.13960\n",
      "Loss = 2.803363800048828\n",
      "Training-Batch No.13970\n",
      "Loss = 1.9033305644989014\n",
      "Training-Batch No.13980\n",
      "Loss = 3.166440486907959\n",
      "Training-Batch No.13990\n",
      "Loss = 3.1561522483825684\n",
      "Training-Batch No.14000\n",
      "Loss = 2.621739149093628\n",
      "Training-Batch No.14010\n",
      "Loss = 3.3636507987976074\n",
      "Training-Batch No.14020\n",
      "Loss = 3.392629384994507\n",
      "Training-Batch No.14030\n",
      "Loss = 2.526458501815796\n",
      "Training-Batch No.14040\n",
      "Loss = 3.3065733909606934\n",
      "Training-Batch No.14050\n",
      "Loss = 2.9400250911712646\n",
      "Training-Batch No.14060\n",
      "Loss = 2.249281644821167\n",
      "Training-Batch No.14070\n",
      "Loss = 2.735692024230957\n",
      "Training-Batch No.14080\n",
      "Loss = 2.4214301109313965\n",
      "Training-Batch No.14090\n",
      "Loss = 2.6763384342193604\n",
      "Training-Batch No.14100\n",
      "Loss = 3.176557779312134\n",
      "Training-Batch No.14110\n",
      "Loss = 2.4030234813690186\n",
      "Training-Batch No.14120\n",
      "Loss = 2.258124589920044\n",
      "Training-Batch No.14130\n",
      "Loss = 3.134856700897217\n",
      "Training-Batch No.14140\n",
      "Loss = 2.817652702331543\n",
      "Training-Batch No.14150\n",
      "Loss = 2.9048514366149902\n",
      "Training-Batch No.14160\n",
      "Loss = 2.98730206489563\n",
      "Training-Batch No.14170\n",
      "Loss = 2.6198694705963135\n",
      "Training-Batch No.14180\n",
      "Loss = 2.786637306213379\n",
      "Training-Batch No.14190\n",
      "Loss = 3.2699966430664062\n",
      "Training-Batch No.14200\n",
      "Loss = 2.704601764678955\n",
      "Training-Batch No.14210\n",
      "Loss = 2.026496171951294\n",
      "Training-Batch No.14220\n",
      "Loss = 2.995126962661743\n",
      "Training-Batch No.14230\n",
      "Loss = 3.4546937942504883\n",
      "Training-Batch No.14240\n",
      "Loss = 2.4566574096679688\n",
      "Training-Batch No.14250\n",
      "Loss = 2.074141502380371\n",
      "Training-Batch No.14260\n",
      "Loss = 2.998809337615967\n",
      "Training-Batch No.14270\n",
      "Loss = 3.009101629257202\n",
      "Training-Batch No.14280\n",
      "Loss = 3.7237226963043213\n",
      "Training-Batch No.14290\n",
      "Loss = 2.2326438426971436\n",
      "Training-Batch No.14300\n",
      "Loss = 2.153590679168701\n",
      "Training-Batch No.14310\n",
      "Loss = 2.854956865310669\n",
      "Training-Batch No.14320\n",
      "Loss = 3.038435697555542\n",
      "Training-Batch No.14330\n",
      "Loss = 3.9117794036865234\n",
      "Training-Batch No.14340\n",
      "Loss = 2.5583655834198\n",
      "Training-Batch No.14350\n",
      "Loss = 2.805793285369873\n",
      "Training-Batch No.14360\n",
      "Loss = 2.470964193344116\n",
      "Training-Batch No.14370\n",
      "Loss = 1.9169654846191406\n",
      "Training-Batch No.14380\n",
      "Loss = 2.929011821746826\n",
      "Training-Batch No.14390\n",
      "Loss = 2.9949700832366943\n",
      "Training-Batch No.14400\n",
      "Loss = 2.3988733291625977\n",
      "Training-Batch No.14410\n",
      "Loss = 1.9059338569641113\n",
      "Training-Batch No.14420\n",
      "Loss = 2.877822160720825\n",
      "Training-Batch No.14430\n",
      "Loss = 2.259375810623169\n",
      "Training-Batch No.14440\n",
      "Loss = 2.4075684547424316\n",
      "Training-Batch No.14450\n",
      "Loss = 1.5739550590515137\n",
      "Training-Batch No.14460\n",
      "Loss = 2.4912641048431396\n",
      "Training-Batch No.14470\n",
      "Loss = 2.8301773071289062\n",
      "Training-Batch No.14480\n",
      "Loss = 2.4517979621887207\n",
      "Training-Batch No.14490\n",
      "Loss = 2.091811180114746\n",
      "Training-Batch No.14500\n",
      "Loss = 2.2939491271972656\n",
      "Training-Batch No.14510\n",
      "Loss = 2.6422696113586426\n",
      "Training-Batch No.14520\n",
      "Loss = 1.8884644508361816\n",
      "Training-Batch No.14530\n",
      "Loss = 3.7326714992523193\n",
      "Training-Batch No.14540\n",
      "Loss = 2.4547810554504395\n",
      "Training-Batch No.14550\n",
      "Loss = 2.6201415061950684\n",
      "Training-Batch No.14560\n",
      "Loss = 3.36423397064209\n",
      "Training-Batch No.14570\n",
      "Loss = 2.1592416763305664\n",
      "Training-Batch No.14580\n",
      "Loss = 2.6989922523498535\n",
      "Training-Batch No.14590\n",
      "Loss = 3.1787993907928467\n",
      "Training-Batch No.14600\n",
      "Loss = 2.4729650020599365\n",
      "Training-Batch No.14610\n",
      "Loss = 2.586887836456299\n",
      "Training-Batch No.14620\n",
      "Loss = 2.83164381980896\n",
      "Training-Batch No.14630\n",
      "Loss = 2.5731801986694336\n",
      "Training-Batch No.14640\n",
      "Loss = 1.9648847579956055\n",
      "Training-Batch No.14650\n",
      "Loss = 2.6411097049713135\n",
      "Training-Batch No.14660\n",
      "Loss = 2.4509177207946777\n",
      "Training-Batch No.14670\n",
      "Loss = 1.6916816234588623\n",
      "Training-Batch No.14680\n",
      "Loss = 2.8652403354644775\n",
      "Training-Batch No.14690\n",
      "Loss = 3.4415555000305176\n",
      "Training-Batch No.14700\n",
      "Loss = 3.1407172679901123\n",
      "Training-Batch No.14710\n",
      "Loss = 2.3439908027648926\n",
      "Training-Batch No.14720\n",
      "Loss = 3.4589710235595703\n",
      "Training-Batch No.14730\n",
      "Loss = 2.3283181190490723\n",
      "Training-Batch No.14740\n",
      "Loss = 1.8261878490447998\n",
      "Training-Batch No.14750\n",
      "Loss = 2.2785592079162598\n",
      "Training-Batch No.14760\n",
      "Loss = 2.1715633869171143\n",
      "Training-Batch No.14770\n",
      "Loss = 2.6078319549560547\n",
      "Training-Batch No.14780\n",
      "Loss = 2.6520986557006836\n",
      "Training-Batch No.14790\n",
      "Loss = 2.6768579483032227\n",
      "Training-Batch No.14800\n",
      "Loss = 2.6817829608917236\n",
      "Training-Batch No.14810\n",
      "Loss = 2.789060115814209\n",
      "Training-Batch No.14820\n",
      "Loss = 2.121616840362549\n",
      "Training-Batch No.14830\n",
      "Loss = 2.0086772441864014\n",
      "Training-Batch No.14840\n",
      "Loss = 1.802296757698059\n",
      "Training-Batch No.14850\n",
      "Loss = 2.162134885787964\n",
      "Training-Batch No.14860\n",
      "Loss = 2.249101161956787\n",
      "Training-Batch No.14870\n",
      "Loss = 2.4102022647857666\n",
      "Training-Batch No.14880\n",
      "Loss = 2.913119077682495\n",
      "Epoch 16 Training Loss: 2.6601\n",
      "Start validation\n",
      "Epoch 16 Validation Loss: 2.9902\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.25564516129032255\n",
      "Average Top-3 accuracy 0.4174731182795699\n",
      "Average Top-5 accuracy 0.48333333333333334\n",
      "Average Top-7 accuracy 0.5337365591397849\n",
      "Average Top-9 accuracy 0.571236559139785\n",
      "Average Top-11 accuracy 0.6049731182795699\n",
      "Average Top-13 accuracy 0.634005376344086\n",
      "Average Top-15 accuracy 0.6598118279569892\n",
      "current acc 0.25564516129032255\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 17\n",
      "Training-Batch No.14890\n",
      "Loss = 2.628046751022339\n",
      "Training-Batch No.14900\n",
      "Loss = 1.9737775325775146\n",
      "Training-Batch No.14910\n",
      "Loss = 3.088200092315674\n",
      "Training-Batch No.14920\n",
      "Loss = 3.3073480129241943\n",
      "Training-Batch No.14930\n",
      "Loss = 2.4820055961608887\n",
      "Training-Batch No.14940\n",
      "Loss = 3.3471341133117676\n",
      "Training-Batch No.14950\n",
      "Loss = 3.2204484939575195\n",
      "Training-Batch No.14960\n",
      "Loss = 2.649191379547119\n",
      "Training-Batch No.14970\n",
      "Loss = 3.3570103645324707\n",
      "Training-Batch No.14980\n",
      "Loss = 2.760725498199463\n",
      "Training-Batch No.14990\n",
      "Loss = 2.1712288856506348\n",
      "Training-Batch No.15000\n",
      "Loss = 2.7186739444732666\n",
      "Training-Batch No.15010\n",
      "Loss = 2.3578639030456543\n",
      "Training-Batch No.15020\n",
      "Loss = 2.5360593795776367\n",
      "Training-Batch No.15030\n",
      "Loss = 3.178365707397461\n",
      "Training-Batch No.15040\n",
      "Loss = 2.609302520751953\n",
      "Training-Batch No.15050\n",
      "Loss = 2.197230339050293\n",
      "Training-Batch No.15060\n",
      "Loss = 3.02803373336792\n",
      "Training-Batch No.15070\n",
      "Loss = 2.8527379035949707\n",
      "Training-Batch No.15080\n",
      "Loss = 3.0379457473754883\n",
      "Training-Batch No.15090\n",
      "Loss = 2.8414101600646973\n",
      "Training-Batch No.15100\n",
      "Loss = 2.4538238048553467\n",
      "Training-Batch No.15110\n",
      "Loss = 2.8062875270843506\n",
      "Training-Batch No.15120\n",
      "Loss = 3.19173264503479\n",
      "Training-Batch No.15130\n",
      "Loss = 2.7712252140045166\n",
      "Training-Batch No.15140\n",
      "Loss = 1.8764125108718872\n",
      "Training-Batch No.15150\n",
      "Loss = 2.7099456787109375\n",
      "Training-Batch No.15160\n",
      "Loss = 3.5191216468811035\n",
      "Training-Batch No.15170\n",
      "Loss = 2.435410737991333\n",
      "Training-Batch No.15180\n",
      "Loss = 2.0572617053985596\n",
      "Training-Batch No.15190\n",
      "Loss = 2.9034180641174316\n",
      "Training-Batch No.15200\n",
      "Loss = 2.8445637226104736\n",
      "Training-Batch No.15210\n",
      "Loss = 3.9797000885009766\n",
      "Training-Batch No.15220\n",
      "Loss = 2.175553321838379\n",
      "Training-Batch No.15230\n",
      "Loss = 2.1919124126434326\n",
      "Training-Batch No.15240\n",
      "Loss = 3.0522189140319824\n",
      "Training-Batch No.15250\n",
      "Loss = 3.204272747039795\n",
      "Training-Batch No.15260\n",
      "Loss = 4.038660049438477\n",
      "Training-Batch No.15270\n",
      "Loss = 2.371250629425049\n",
      "Training-Batch No.15280\n",
      "Loss = 2.900498628616333\n",
      "Training-Batch No.15290\n",
      "Loss = 2.3263323307037354\n",
      "Training-Batch No.15300\n",
      "Loss = 1.8329079151153564\n",
      "Training-Batch No.15310\n",
      "Loss = 2.6183021068573\n",
      "Training-Batch No.15320\n",
      "Loss = 2.896690845489502\n",
      "Training-Batch No.15330\n",
      "Loss = 2.6424105167388916\n",
      "Training-Batch No.15340\n",
      "Loss = 1.7889151573181152\n",
      "Training-Batch No.15350\n",
      "Loss = 2.830275058746338\n",
      "Training-Batch No.15360\n",
      "Loss = 2.03678297996521\n",
      "Training-Batch No.15370\n",
      "Loss = 2.3479576110839844\n",
      "Training-Batch No.15380\n",
      "Loss = 1.6233105659484863\n",
      "Training-Batch No.15390\n",
      "Loss = 2.565702438354492\n",
      "Training-Batch No.15400\n",
      "Loss = 2.85717511177063\n",
      "Training-Batch No.15410\n",
      "Loss = 2.3447561264038086\n",
      "Training-Batch No.15420\n",
      "Loss = 2.005284070968628\n",
      "Training-Batch No.15430\n",
      "Loss = 2.40675950050354\n",
      "Training-Batch No.15440\n",
      "Loss = 2.652510166168213\n",
      "Training-Batch No.15450\n",
      "Loss = 1.832493782043457\n",
      "Training-Batch No.15460\n",
      "Loss = 3.529034376144409\n",
      "Training-Batch No.15470\n",
      "Loss = 2.433802604675293\n",
      "Training-Batch No.15480\n",
      "Loss = 2.5416524410247803\n",
      "Training-Batch No.15490\n",
      "Loss = 3.041353464126587\n",
      "Training-Batch No.15500\n",
      "Loss = 2.194208860397339\n",
      "Training-Batch No.15510\n",
      "Loss = 2.6363115310668945\n",
      "Training-Batch No.15520\n",
      "Loss = 2.948432683944702\n",
      "Training-Batch No.15530\n",
      "Loss = 2.4310810565948486\n",
      "Training-Batch No.15540\n",
      "Loss = 2.4930648803710938\n",
      "Training-Batch No.15550\n",
      "Loss = 2.8738393783569336\n",
      "Training-Batch No.15560\n",
      "Loss = 2.6633713245391846\n",
      "Training-Batch No.15570\n",
      "Loss = 2.0523602962493896\n",
      "Training-Batch No.15580\n",
      "Loss = 2.589191436767578\n",
      "Training-Batch No.15590\n",
      "Loss = 2.3177690505981445\n",
      "Training-Batch No.15600\n",
      "Loss = 1.7612477540969849\n",
      "Training-Batch No.15610\n",
      "Loss = 2.9778828620910645\n",
      "Training-Batch No.15620\n",
      "Loss = 3.1381282806396484\n",
      "Training-Batch No.15630\n",
      "Loss = 2.935123920440674\n",
      "Training-Batch No.15640\n",
      "Loss = 2.2992866039276123\n",
      "Training-Batch No.15650\n",
      "Loss = 3.332014322280884\n",
      "Training-Batch No.15660\n",
      "Loss = 2.281675100326538\n",
      "Training-Batch No.15670\n",
      "Loss = 1.9414920806884766\n",
      "Training-Batch No.15680\n",
      "Loss = 2.017611026763916\n",
      "Training-Batch No.15690\n",
      "Loss = 2.042361259460449\n",
      "Training-Batch No.15700\n",
      "Loss = 2.568079710006714\n",
      "Training-Batch No.15710\n",
      "Loss = 2.4286365509033203\n",
      "Training-Batch No.15720\n",
      "Loss = 2.6894264221191406\n",
      "Training-Batch No.15730\n",
      "Loss = 2.6014885902404785\n",
      "Training-Batch No.15740\n",
      "Loss = 2.472285747528076\n",
      "Training-Batch No.15750\n",
      "Loss = 2.037057876586914\n",
      "Training-Batch No.15760\n",
      "Loss = 2.027909278869629\n",
      "Training-Batch No.15770\n",
      "Loss = 2.124727249145508\n",
      "Training-Batch No.15780\n",
      "Loss = 2.383720874786377\n",
      "Training-Batch No.15790\n",
      "Loss = 2.2131974697113037\n",
      "Training-Batch No.15800\n",
      "Loss = 2.5000059604644775\n",
      "Training-Batch No.15810\n",
      "Loss = 2.814089298248291\n",
      "Epoch 17 Training Loss: 2.6279\n",
      "Start validation\n",
      "Epoch 17 Validation Loss: 2.9794\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.25416666666666665\n",
      "Average Top-3 accuracy 0.421505376344086\n",
      "Average Top-5 accuracy 0.4842741935483871\n",
      "Average Top-7 accuracy 0.5331989247311828\n",
      "Average Top-9 accuracy 0.571505376344086\n",
      "Average Top-11 accuracy 0.6056451612903225\n",
      "Average Top-13 accuracy 0.6362903225806451\n",
      "Average Top-15 accuracy 0.6654569892473118\n",
      "current acc 0.25416666666666665\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 18\n",
      "Training-Batch No.15820\n",
      "Loss = 2.5988848209381104\n",
      "Training-Batch No.15830\n",
      "Loss = 1.995405673980713\n",
      "Training-Batch No.15840\n",
      "Loss = 3.0395851135253906\n",
      "Training-Batch No.15850\n",
      "Loss = 3.120523452758789\n",
      "Training-Batch No.15860\n",
      "Loss = 2.604041337966919\n",
      "Training-Batch No.15870\n",
      "Loss = 3.6253068447113037\n",
      "Training-Batch No.15880\n",
      "Loss = 3.116206169128418\n",
      "Training-Batch No.15890\n",
      "Loss = 2.785977840423584\n",
      "Training-Batch No.15900\n",
      "Loss = 3.267735719680786\n",
      "Training-Batch No.15910\n",
      "Loss = 2.8818540573120117\n",
      "Training-Batch No.15920\n",
      "Loss = 2.2137563228607178\n",
      "Training-Batch No.15930\n",
      "Loss = 2.7385802268981934\n",
      "Training-Batch No.15940\n",
      "Loss = 2.33382248878479\n",
      "Training-Batch No.15950\n",
      "Loss = 2.558238983154297\n",
      "Training-Batch No.15960\n",
      "Loss = 3.0657851696014404\n",
      "Training-Batch No.15970\n",
      "Loss = 2.4884414672851562\n",
      "Training-Batch No.15980\n",
      "Loss = 2.20674467086792\n",
      "Training-Batch No.15990\n",
      "Loss = 3.250382423400879\n",
      "Training-Batch No.16000\n",
      "Loss = 2.8951311111450195\n",
      "Training-Batch No.16010\n",
      "Loss = 2.874541759490967\n",
      "Training-Batch No.16020\n",
      "Loss = 2.6934609413146973\n",
      "Training-Batch No.16030\n",
      "Loss = 2.5430140495300293\n",
      "Training-Batch No.16040\n",
      "Loss = 2.755110502243042\n",
      "Training-Batch No.16050\n",
      "Loss = 3.152606964111328\n",
      "Training-Batch No.16060\n",
      "Loss = 2.6750199794769287\n",
      "Training-Batch No.16070\n",
      "Loss = 1.9719948768615723\n",
      "Training-Batch No.16080\n",
      "Loss = 2.791027545928955\n",
      "Training-Batch No.16090\n",
      "Loss = 3.467780351638794\n",
      "Training-Batch No.16100\n",
      "Loss = 2.299903154373169\n",
      "Training-Batch No.16110\n",
      "Loss = 2.0622920989990234\n",
      "Training-Batch No.16120\n",
      "Loss = 2.8662214279174805\n",
      "Training-Batch No.16130\n",
      "Loss = 2.868813991546631\n",
      "Training-Batch No.16140\n",
      "Loss = 3.636976957321167\n",
      "Training-Batch No.16150\n",
      "Loss = 2.160356283187866\n",
      "Training-Batch No.16160\n",
      "Loss = 1.9871983528137207\n",
      "Training-Batch No.16170\n",
      "Loss = 2.8957064151763916\n",
      "Training-Batch No.16180\n",
      "Loss = 3.1756889820098877\n",
      "Training-Batch No.16190\n",
      "Loss = 3.9134862422943115\n",
      "Training-Batch No.16200\n",
      "Loss = 2.4430792331695557\n",
      "Training-Batch No.16210\n",
      "Loss = 2.8639209270477295\n",
      "Training-Batch No.16220\n",
      "Loss = 2.3565499782562256\n",
      "Training-Batch No.16230\n",
      "Loss = 1.9103144407272339\n",
      "Training-Batch No.16240\n",
      "Loss = 2.349442958831787\n",
      "Training-Batch No.16250\n",
      "Loss = 2.6628007888793945\n",
      "Training-Batch No.16260\n",
      "Loss = 2.4310896396636963\n",
      "Training-Batch No.16270\n",
      "Loss = 1.6639354228973389\n",
      "Training-Batch No.16280\n",
      "Loss = 2.788433313369751\n",
      "Training-Batch No.16290\n",
      "Loss = 2.0529675483703613\n",
      "Training-Batch No.16300\n",
      "Loss = 2.517958164215088\n",
      "Training-Batch No.16310\n",
      "Loss = 1.5554289817810059\n",
      "Training-Batch No.16320\n",
      "Loss = 2.384366989135742\n",
      "Training-Batch No.16330\n",
      "Loss = 2.8439390659332275\n",
      "Training-Batch No.16340\n",
      "Loss = 2.563263177871704\n",
      "Training-Batch No.16350\n",
      "Loss = 1.99262535572052\n",
      "Training-Batch No.16360\n",
      "Loss = 2.2390081882476807\n",
      "Training-Batch No.16370\n",
      "Loss = 2.6121225357055664\n",
      "Training-Batch No.16380\n",
      "Loss = 2.0228562355041504\n",
      "Training-Batch No.16390\n",
      "Loss = 3.3450307846069336\n",
      "Training-Batch No.16400\n",
      "Loss = 2.342456340789795\n",
      "Training-Batch No.16410\n",
      "Loss = 2.4134891033172607\n",
      "Training-Batch No.16420\n",
      "Loss = 3.050736427307129\n",
      "Training-Batch No.16430\n",
      "Loss = 2.2099714279174805\n",
      "Training-Batch No.16440\n",
      "Loss = 2.631819248199463\n",
      "Training-Batch No.16450\n",
      "Loss = 3.0335896015167236\n",
      "Training-Batch No.16460\n",
      "Loss = 2.423649311065674\n",
      "Training-Batch No.16470\n",
      "Loss = 2.53287410736084\n",
      "Training-Batch No.16480\n",
      "Loss = 2.8004345893859863\n",
      "Training-Batch No.16490\n",
      "Loss = 2.7434730529785156\n",
      "Training-Batch No.16500\n",
      "Loss = 1.8718608617782593\n",
      "Training-Batch No.16510\n",
      "Loss = 2.5649092197418213\n",
      "Training-Batch No.16520\n",
      "Loss = 2.437532424926758\n",
      "Training-Batch No.16530\n",
      "Loss = 1.6840779781341553\n",
      "Training-Batch No.16540\n",
      "Loss = 2.7839038372039795\n",
      "Training-Batch No.16550\n",
      "Loss = 3.100905179977417\n",
      "Training-Batch No.16560\n",
      "Loss = 3.1503403186798096\n",
      "Training-Batch No.16570\n",
      "Loss = 2.4327147006988525\n",
      "Training-Batch No.16580\n",
      "Loss = 3.523012161254883\n",
      "Training-Batch No.16590\n",
      "Loss = 2.3095784187316895\n",
      "Training-Batch No.16600\n",
      "Loss = 1.5945968627929688\n",
      "Training-Batch No.16610\n",
      "Loss = 2.4834299087524414\n",
      "Training-Batch No.16620\n",
      "Loss = 2.0098297595977783\n",
      "Training-Batch No.16630\n",
      "Loss = 2.4987428188323975\n",
      "Training-Batch No.16640\n",
      "Loss = 2.4734818935394287\n",
      "Training-Batch No.16650\n",
      "Loss = 2.568845748901367\n",
      "Training-Batch No.16660\n",
      "Loss = 2.4760916233062744\n",
      "Training-Batch No.16670\n",
      "Loss = 2.675386428833008\n",
      "Training-Batch No.16680\n",
      "Loss = 2.0916433334350586\n",
      "Training-Batch No.16690\n",
      "Loss = 1.9466067552566528\n",
      "Training-Batch No.16700\n",
      "Loss = 1.8990769386291504\n",
      "Training-Batch No.16710\n",
      "Loss = 2.139969825744629\n",
      "Training-Batch No.16720\n",
      "Loss = 2.0130138397216797\n",
      "Training-Batch No.16730\n",
      "Loss = 2.1430046558380127\n",
      "Training-Batch No.16740\n",
      "Loss = 2.8905327320098877\n",
      "Epoch 18 Training Loss: 2.5947\n",
      "Start validation\n",
      "Epoch 18 Validation Loss: 3.0414\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.256989247311828\n",
      "Average Top-3 accuracy 0.4220430107526882\n",
      "Average Top-5 accuracy 0.4869623655913978\n",
      "Average Top-7 accuracy 0.5345430107526882\n",
      "Average Top-9 accuracy 0.5706989247311828\n",
      "Average Top-11 accuracy 0.6022849462365591\n",
      "Average Top-13 accuracy 0.6325268817204301\n",
      "Average Top-15 accuracy 0.655510752688172\n",
      "current acc 0.256989247311828\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 19\n",
      "Training-Batch No.16750\n",
      "Loss = 2.557403564453125\n",
      "Training-Batch No.16760\n",
      "Loss = 1.945209264755249\n",
      "Training-Batch No.16770\n",
      "Loss = 2.9828169345855713\n",
      "Training-Batch No.16780\n",
      "Loss = 3.1262106895446777\n",
      "Training-Batch No.16790\n",
      "Loss = 2.3398070335388184\n",
      "Training-Batch No.16800\n",
      "Loss = 3.055154800415039\n",
      "Training-Batch No.16810\n",
      "Loss = 3.191575050354004\n",
      "Training-Batch No.16820\n",
      "Loss = 2.4190244674682617\n",
      "Training-Batch No.16830\n",
      "Loss = 3.485905408859253\n",
      "Training-Batch No.16840\n",
      "Loss = 2.6501882076263428\n",
      "Training-Batch No.16850\n",
      "Loss = 2.225750207901001\n",
      "Training-Batch No.16860\n",
      "Loss = 2.682084798812866\n",
      "Training-Batch No.16870\n",
      "Loss = 2.137688159942627\n",
      "Training-Batch No.16880\n",
      "Loss = 2.4196722507476807\n",
      "Training-Batch No.16890\n",
      "Loss = 3.144808053970337\n",
      "Training-Batch No.16900\n",
      "Loss = 2.4752357006073\n",
      "Training-Batch No.16910\n",
      "Loss = 2.344393730163574\n",
      "Training-Batch No.16920\n",
      "Loss = 3.126896381378174\n",
      "Training-Batch No.16930\n",
      "Loss = 2.779104471206665\n",
      "Training-Batch No.16940\n",
      "Loss = 2.77023983001709\n",
      "Training-Batch No.16950\n",
      "Loss = 2.7600090503692627\n",
      "Training-Batch No.16960\n",
      "Loss = 2.6945948600769043\n",
      "Training-Batch No.16970\n",
      "Loss = 2.984398126602173\n",
      "Training-Batch No.16980\n",
      "Loss = 3.1979923248291016\n",
      "Training-Batch No.16990\n",
      "Loss = 2.7702443599700928\n",
      "Training-Batch No.17000\n",
      "Loss = 1.8506699800491333\n",
      "Training-Batch No.17010\n",
      "Loss = 2.6718060970306396\n",
      "Training-Batch No.17020\n",
      "Loss = 3.3991875648498535\n",
      "Training-Batch No.17030\n",
      "Loss = 2.414931297302246\n",
      "Training-Batch No.17040\n",
      "Loss = 1.963045358657837\n",
      "Training-Batch No.17050\n",
      "Loss = 2.8477706909179688\n",
      "Training-Batch No.17060\n",
      "Loss = 3.1484711170196533\n",
      "Training-Batch No.17070\n",
      "Loss = 3.4933674335479736\n",
      "Training-Batch No.17080\n",
      "Loss = 2.160095691680908\n",
      "Training-Batch No.17090\n",
      "Loss = 2.1640408039093018\n",
      "Training-Batch No.17100\n",
      "Loss = 2.895293712615967\n",
      "Training-Batch No.17110\n",
      "Loss = 3.164717674255371\n",
      "Training-Batch No.17120\n",
      "Loss = 4.00681209564209\n",
      "Training-Batch No.17130\n",
      "Loss = 2.3579349517822266\n",
      "Training-Batch No.17140\n",
      "Loss = 2.830007314682007\n",
      "Training-Batch No.17150\n",
      "Loss = 2.2870821952819824\n",
      "Training-Batch No.17160\n",
      "Loss = 1.748098611831665\n",
      "Training-Batch No.17170\n",
      "Loss = 2.578052520751953\n",
      "Training-Batch No.17180\n",
      "Loss = 2.7036983966827393\n",
      "Training-Batch No.17190\n",
      "Loss = 2.3798794746398926\n",
      "Training-Batch No.17200\n",
      "Loss = 1.798845648765564\n",
      "Training-Batch No.17210\n",
      "Loss = 2.7404744625091553\n",
      "Training-Batch No.17220\n",
      "Loss = 1.9736151695251465\n",
      "Training-Batch No.17230\n",
      "Loss = 2.359555959701538\n",
      "Training-Batch No.17240\n",
      "Loss = 1.5490034818649292\n",
      "Training-Batch No.17250\n",
      "Loss = 2.548564910888672\n",
      "Training-Batch No.17260\n",
      "Loss = 2.7928075790405273\n",
      "Training-Batch No.17270\n",
      "Loss = 2.4916744232177734\n",
      "Training-Batch No.17280\n",
      "Loss = 1.9944937229156494\n",
      "Training-Batch No.17290\n",
      "Loss = 2.1754159927368164\n",
      "Training-Batch No.17300\n",
      "Loss = 2.455301284790039\n",
      "Training-Batch No.17310\n",
      "Loss = 1.9427131414413452\n",
      "Training-Batch No.17320\n",
      "Loss = 3.40970778465271\n",
      "Training-Batch No.17330\n",
      "Loss = 2.4001762866973877\n",
      "Training-Batch No.17340\n",
      "Loss = 2.37721848487854\n",
      "Training-Batch No.17350\n",
      "Loss = 3.025007486343384\n",
      "Training-Batch No.17360\n",
      "Loss = 2.1176700592041016\n",
      "Training-Batch No.17370\n",
      "Loss = 2.5093915462493896\n",
      "Training-Batch No.17380\n",
      "Loss = 3.0144131183624268\n",
      "Training-Batch No.17390\n",
      "Loss = 2.2881104946136475\n",
      "Training-Batch No.17400\n",
      "Loss = 2.7137508392333984\n",
      "Training-Batch No.17410\n",
      "Loss = 2.60483717918396\n",
      "Training-Batch No.17420\n",
      "Loss = 2.5745372772216797\n",
      "Training-Batch No.17430\n",
      "Loss = 1.8732340335845947\n",
      "Training-Batch No.17440\n",
      "Loss = 2.3213393688201904\n",
      "Training-Batch No.17450\n",
      "Loss = 2.6450953483581543\n",
      "Training-Batch No.17460\n",
      "Loss = 1.6319078207015991\n",
      "Training-Batch No.17470\n",
      "Loss = 2.7817935943603516\n",
      "Training-Batch No.17480\n",
      "Loss = 3.1753275394439697\n",
      "Training-Batch No.17490\n",
      "Loss = 2.9454667568206787\n",
      "Training-Batch No.17500\n",
      "Loss = 2.495450258255005\n",
      "Training-Batch No.17510\n",
      "Loss = 3.5183892250061035\n",
      "Training-Batch No.17520\n",
      "Loss = 2.035102128982544\n",
      "Training-Batch No.17530\n",
      "Loss = 1.685233235359192\n",
      "Training-Batch No.17540\n",
      "Loss = 2.2624385356903076\n",
      "Training-Batch No.17550\n",
      "Loss = 2.1709961891174316\n",
      "Training-Batch No.17560\n",
      "Loss = 2.4805755615234375\n",
      "Training-Batch No.17570\n",
      "Loss = 2.6534957885742188\n",
      "Training-Batch No.17580\n",
      "Loss = 2.4389760494232178\n",
      "Training-Batch No.17590\n",
      "Loss = 2.260715961456299\n",
      "Training-Batch No.17600\n",
      "Loss = 2.4978561401367188\n",
      "Training-Batch No.17610\n",
      "Loss = 1.9564372301101685\n",
      "Training-Batch No.17620\n",
      "Loss = 2.0760624408721924\n",
      "Training-Batch No.17630\n",
      "Loss = 1.7500275373458862\n",
      "Training-Batch No.17640\n",
      "Loss = 2.1884379386901855\n",
      "Training-Batch No.17650\n",
      "Loss = 2.050687789916992\n",
      "Training-Batch No.17660\n",
      "Loss = 2.2640204429626465\n",
      "Training-Batch No.17670\n",
      "Loss = 2.605689764022827\n",
      "Epoch 19 Training Loss: 2.5685\n",
      "Start validation\n",
      "Epoch 19 Validation Loss: 3.0963\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.260752688172043\n",
      "Average Top-3 accuracy 0.4204301075268817\n",
      "Average Top-5 accuracy 0.4829301075268817\n",
      "Average Top-7 accuracy 0.5286290322580646\n",
      "Average Top-9 accuracy 0.5635752688172043\n",
      "Average Top-11 accuracy 0.5962365591397849\n",
      "Average Top-13 accuracy 0.6239247311827957\n",
      "Average Top-15 accuracy 0.6506720430107527\n",
      "current acc 0.260752688172043\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Epoch No. 20\n",
      "Training-Batch No.17680\n",
      "Loss = 2.708920955657959\n",
      "Training-Batch No.17690\n",
      "Loss = 1.798516869544983\n",
      "Training-Batch No.17700\n",
      "Loss = 2.9312078952789307\n",
      "Training-Batch No.17710\n",
      "Loss = 3.085869550704956\n",
      "Training-Batch No.17720\n",
      "Loss = 2.428234577178955\n",
      "Training-Batch No.17730\n",
      "Loss = 3.0940451622009277\n",
      "Training-Batch No.17740\n",
      "Loss = 3.2256155014038086\n",
      "Training-Batch No.17750\n",
      "Loss = 2.44940185546875\n",
      "Training-Batch No.17760\n",
      "Loss = 3.1418893337249756\n",
      "Training-Batch No.17770\n",
      "Loss = 2.859192132949829\n",
      "Training-Batch No.17780\n",
      "Loss = 2.310716152191162\n",
      "Training-Batch No.17790\n",
      "Loss = 2.7311768531799316\n",
      "Training-Batch No.17800\n",
      "Loss = 2.3295888900756836\n",
      "Training-Batch No.17810\n",
      "Loss = 2.423964023590088\n",
      "Training-Batch No.17820\n",
      "Loss = 3.02594256401062\n",
      "Training-Batch No.17830\n",
      "Loss = 2.3849756717681885\n",
      "Training-Batch No.17840\n",
      "Loss = 2.4155571460723877\n",
      "Training-Batch No.17850\n",
      "Loss = 3.162315845489502\n",
      "Training-Batch No.17860\n",
      "Loss = 2.576174259185791\n",
      "Training-Batch No.17870\n",
      "Loss = 3.142552614212036\n",
      "Training-Batch No.17880\n",
      "Loss = 2.6692824363708496\n",
      "Training-Batch No.17890\n",
      "Loss = 2.480316400527954\n",
      "Training-Batch No.17900\n",
      "Loss = 2.68218731880188\n",
      "Training-Batch No.17910\n",
      "Loss = 2.878247022628784\n",
      "Training-Batch No.17920\n",
      "Loss = 2.6853690147399902\n",
      "Training-Batch No.17930\n",
      "Loss = 1.7415186166763306\n",
      "Training-Batch No.17940\n",
      "Loss = 2.5964369773864746\n",
      "Training-Batch No.17950\n",
      "Loss = 3.0980398654937744\n",
      "Training-Batch No.17960\n",
      "Loss = 2.3536159992218018\n",
      "Training-Batch No.17970\n",
      "Loss = 1.983885645866394\n",
      "Training-Batch No.17980\n",
      "Loss = 2.9075348377227783\n",
      "Training-Batch No.17990\n",
      "Loss = 2.803196430206299\n",
      "Training-Batch No.18000\n",
      "Loss = 3.786943197250366\n",
      "Training-Batch No.18010\n",
      "Loss = 2.1116180419921875\n",
      "Training-Batch No.18020\n",
      "Loss = 1.9743311405181885\n",
      "Training-Batch No.18030\n",
      "Loss = 2.745267152786255\n",
      "Training-Batch No.18040\n",
      "Loss = 3.091419219970703\n",
      "Training-Batch No.18050\n",
      "Loss = 4.095034122467041\n",
      "Training-Batch No.18060\n",
      "Loss = 2.4153265953063965\n",
      "Training-Batch No.18070\n",
      "Loss = 2.939819574356079\n",
      "Training-Batch No.18080\n",
      "Loss = 2.362597942352295\n",
      "Training-Batch No.18090\n",
      "Loss = 1.7624837160110474\n",
      "Training-Batch No.18100\n",
      "Loss = 2.4390475749969482\n",
      "Training-Batch No.18110\n",
      "Loss = 2.6119186878204346\n",
      "Training-Batch No.18120\n",
      "Loss = 2.5802743434906006\n",
      "Training-Batch No.18130\n",
      "Loss = 1.7920026779174805\n",
      "Training-Batch No.18140\n",
      "Loss = 2.8842763900756836\n",
      "Training-Batch No.18150\n",
      "Loss = 2.0080699920654297\n",
      "Training-Batch No.18160\n",
      "Loss = 2.442589521408081\n",
      "Training-Batch No.18170\n",
      "Loss = 1.7667686939239502\n",
      "Training-Batch No.18180\n",
      "Loss = 2.4972357749938965\n",
      "Training-Batch No.18190\n",
      "Loss = 2.9452686309814453\n",
      "Training-Batch No.18200\n",
      "Loss = 2.310019016265869\n",
      "Training-Batch No.18210\n",
      "Loss = 1.9059028625488281\n",
      "Training-Batch No.18220\n",
      "Loss = 2.2309067249298096\n",
      "Training-Batch No.18230\n",
      "Loss = 2.6187899112701416\n",
      "Training-Batch No.18240\n",
      "Loss = 1.9630786180496216\n",
      "Training-Batch No.18250\n",
      "Loss = 3.662559986114502\n",
      "Training-Batch No.18260\n",
      "Loss = 2.474210739135742\n",
      "Training-Batch No.18270\n",
      "Loss = 2.445040225982666\n",
      "Training-Batch No.18280\n",
      "Loss = 2.9205307960510254\n",
      "Training-Batch No.18290\n",
      "Loss = 2.182950735092163\n",
      "Training-Batch No.18300\n",
      "Loss = 2.336719036102295\n",
      "Training-Batch No.18310\n",
      "Loss = 3.0605061054229736\n",
      "Training-Batch No.18320\n",
      "Loss = 2.2815561294555664\n",
      "Training-Batch No.18330\n",
      "Loss = 2.6174769401550293\n",
      "Training-Batch No.18340\n",
      "Loss = 2.7523980140686035\n",
      "Training-Batch No.18350\n",
      "Loss = 2.702918529510498\n",
      "Training-Batch No.18360\n",
      "Loss = 1.7834275960922241\n",
      "Training-Batch No.18370\n",
      "Loss = 2.5694515705108643\n",
      "Training-Batch No.18380\n",
      "Loss = 2.3352115154266357\n",
      "Training-Batch No.18390\n",
      "Loss = 1.5392718315124512\n",
      "Training-Batch No.18400\n",
      "Loss = 2.9603025913238525\n",
      "Training-Batch No.18410\n",
      "Loss = 3.1245057582855225\n",
      "Training-Batch No.18420\n",
      "Loss = 3.1093907356262207\n",
      "Training-Batch No.18430\n",
      "Loss = 2.2899999618530273\n",
      "Training-Batch No.18440\n",
      "Loss = 3.2432045936584473\n",
      "Training-Batch No.18450\n",
      "Loss = 2.0583205223083496\n",
      "Training-Batch No.18460\n",
      "Loss = 1.6194359064102173\n",
      "Training-Batch No.18470\n",
      "Loss = 2.004700183868408\n",
      "Training-Batch No.18480\n",
      "Loss = 2.221621036529541\n",
      "Training-Batch No.18490\n",
      "Loss = 2.4586234092712402\n",
      "Training-Batch No.18500\n",
      "Loss = 2.3863821029663086\n",
      "Training-Batch No.18510\n",
      "Loss = 2.425464630126953\n",
      "Training-Batch No.18520\n",
      "Loss = 2.1855506896972656\n",
      "Training-Batch No.18530\n",
      "Loss = 2.6047728061676025\n",
      "Training-Batch No.18540\n",
      "Loss = 1.8878804445266724\n",
      "Training-Batch No.18550\n",
      "Loss = 1.9493324756622314\n",
      "Training-Batch No.18560\n",
      "Loss = 1.7085429430007935\n",
      "Training-Batch No.18570\n",
      "Loss = 2.2300806045532227\n",
      "Training-Batch No.18580\n",
      "Loss = 1.9279670715332031\n",
      "Training-Batch No.18590\n",
      "Loss = 1.9327681064605713\n",
      "Training-Batch No.18600\n",
      "Loss = 2.528827667236328\n",
      "Epoch 20 Training Loss: 2.5282\n",
      "Start validation\n",
      "Epoch 20 Validation Loss: 3.1231\n",
      "total test examples are 7440\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2622311827956989\n",
      "Average Top-3 accuracy 0.4252688172043011\n",
      "Average Top-5 accuracy 0.49233870967741933\n",
      "Average Top-7 accuracy 0.5346774193548387\n",
      "Average Top-9 accuracy 0.5688172043010753\n",
      "Average Top-11 accuracy 0.6004032258064517\n",
      "Average Top-13 accuracy 0.6307795698924731\n",
      "Average Top-15 accuracy 0.6534946236559139\n",
      "current acc 0.2622311827956989\n",
      "best acc 0.2645161290322581\n",
      "updated best accuracy 0.2645161290322581\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda as cuda\n",
    "\n",
    "with cuda.device(0):\n",
    "    acc_loss = 0\n",
    "    itr = []\n",
    "    for idx, n in enumerate(train_size):\n",
    "        print('```````````````````````````````````````````````````````')\n",
    "        print('Training size is {}'.format(n))\n",
    "        # Build the network:\n",
    "        # net = resnet50(pretrained=True, progress=True, num_classes=64)\n",
    "\n",
    "        net = create_model('maxvit_tiny_tf_224', pretrained=True, num_classes=65)\n",
    "        net = net.cuda()\n",
    "        summary(net.cuda(), (3, 224, 224))\n",
    "\n",
    "        # Optimization parameters:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        opt = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "        LR_sch = torch.optim.lr_scheduler.MultiStepLR(opt, [4, 8, 12], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "        count = 0\n",
    "        running_loss = []\n",
    "        running_top1_acc = []\n",
    "        running_top3_acc = []\n",
    "        running_top5_acc = []\n",
    "        running_top7_acc = []\n",
    "        running_top9_acc = []\n",
    "        running_top11_acc = []\n",
    "        running_top13_acc = []\n",
    "        running_top15_acc = []\n",
    "\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch No. ' + str(epoch + 1))\n",
    "            skipped_batches = 0\n",
    "            epoch_train_loss = 0  # To track the training loss for the epoch\n",
    "            for tr_count, (img, label) in enumerate(train_loader):\n",
    "                net.train()\n",
    "                x = img.cuda()\n",
    "                opt.zero_grad()\n",
    "                label = label.cuda()\n",
    "                out = net(x)\n",
    "                L = criterion(out, label)\n",
    "                L.backward()\n",
    "                opt.step()\n",
    "                batch_loss = L.item()\n",
    "                acc_loss += batch_loss\n",
    "                epoch_train_loss += batch_loss  # Accumulate batch loss for the epoch\n",
    "                count += 1\n",
    "                if count % 10 == 0:\n",
    "                    print('Training-Batch No.' + str(count))\n",
    "                    running_loss.append(batch_loss)\n",
    "                    itr.append(count)\n",
    "                    print('Loss = ' + str(running_loss[-1]))\n",
    "\n",
    "            epoch_train_loss /= len(train_loader)  # Calculate average training loss for the epoch\n",
    "            print(f'Epoch {epoch + 1} Training Loss: {epoch_train_loss:.4f}')\n",
    "\n",
    "            print('Start Training and Validation')\n",
    "            ave_top1_acc = 0\n",
    "            ave_top3_acc = 0\n",
    "            ave_top5_acc = 0\n",
    "            ave_top7_acc = 0\n",
    "            ave_top9_acc = 0\n",
    "            ave_top11_acc = 0\n",
    "            ave_top13_acc = 0\n",
    "            ave_top15_acc = 0\n",
    "            val_loss = 0  # To track the validation loss\n",
    "            ind_ten = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "            top1_pred_out = []\n",
    "            top3_pred_out = []\n",
    "            top5_pred_out = []\n",
    "            top7_pred_out = []\n",
    "            top9_pred_out = []\n",
    "            top11_pred_out = []\n",
    "            top13_pred_out = []\n",
    "            top15_pred_out = []\n",
    "            gt_beam = []\n",
    "            total_count = 0\n",
    "            for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "                net.eval()\n",
    "                x = imgs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                total_count += labels.size(0)\n",
    "                out = net(x)\n",
    "                _, top_1_pred = torch.max(out, dim=1)\n",
    "\n",
    "                val_batch_loss = criterion(out, labels).item()  # Calculate validation loss for the batch\n",
    "                val_loss += val_batch_loss  # Accumulate batch loss for the epoch\n",
    "\n",
    "                gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "\n",
    "                top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "                sorted_out = torch.argsort(out, dim=1, descending=True)\n",
    "\n",
    "                top_3_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "                top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_5_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "                top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_7_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "                top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_9_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "                top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_11_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "                top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_13_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "                top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                top_15_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "                top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "                reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "                tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "                tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "                tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "                tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "                tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "                tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "                tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "                batch_top1_acc = torch.sum(top_1_pred == labels, dtype=torch.float32)\n",
    "                batch_top3_acc = torch.sum(top_3_pred == tiled_3_labels, dtype=torch.float32)\n",
    "                batch_top5_acc = torch.sum(top_5_pred == tiled_5_labels, dtype=torch.float32)\n",
    "                batch_top7_acc = torch.sum(top_7_pred == tiled_7_labels, dtype=torch.float32)\n",
    "                batch_top9_acc = torch.sum(top_9_pred == tiled_9_labels, dtype=torch.float32)\n",
    "                batch_top11_acc = torch.sum(top_11_pred == tiled_11_labels, dtype=torch.float32)\n",
    "                batch_top13_acc = torch.sum(top_13_pred == tiled_13_labels, dtype=torch.float32)\n",
    "                batch_top15_acc = torch.sum(top_15_pred == tiled_15_labels, dtype=torch.float32)\n",
    "\n",
    "                ave_top1_acc += batch_top1_acc.item()\n",
    "                ave_top3_acc += batch_top3_acc.item()\n",
    "                ave_top5_acc += batch_top5_acc.item()\n",
    "                ave_top7_acc += batch_top7_acc.item()\n",
    "                ave_top9_acc += batch_top9_acc.item()\n",
    "                ave_top11_acc += batch_top11_acc.item()\n",
    "                ave_top13_acc += batch_top13_acc.item()\n",
    "                ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "            val_loss /= len(val_loader)  # Calculate average validation loss for the epoch\n",
    "            print(f'Epoch {epoch + 1} Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "            print(\"total training examples are\", total_count)\n",
    "            running_top1_acc.append(ave_top1_acc / total_count)\n",
    "            running_top3_acc.append(ave_top3_acc / total_count)\n",
    "            running_top5_acc.append(ave_top5_acc / total_count)\n",
    "            running_top7_acc.append(ave_top7_acc / total_count)\n",
    "            running_top9_acc.append(ave_top9_acc / total_count)\n",
    "            running_top11_acc.append(ave_top11_acc / total_count)\n",
    "            running_top13_acc.append(ave_top13_acc / total_count)\n",
    "            running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "            print('Training_size {}--No. of skipped batches {}'.format(n, skipped_batches))\n",
    "            print('Average Top-1 accuracy {}'.format(running_top1_acc[-1]))\n",
    "            print('Average Top-3 accuracy {}'.format(running_top3_acc[-1]))\n",
    "            print('Average Top-5 accuracy {}'.format(running_top5_acc[-1]))\n",
    "            print('Average Top-7 accuracy {}'.format(running_top7_acc[-1]))\n",
    "            print('Average Top-9 accuracy {}'.format(running_top9_acc[-1]))\n",
    "            print('Average Top-11 accuracy {}'.format(running_top11_acc[-1]))\n",
    "            print('Average Top-13 accuracy {}'.format(running_top13_acc[-1]))\n",
    "            print('Average Top-15 accuracy {}'.format(running_top15_acc[-1]))\n",
    "\n",
    "            cur_accuracy = running_top1_acc[-1]\n",
    "\n",
    "            print(\"current acc\", cur_accuracy)\n",
    "            print(\"best acc\", best_accuracy)\n",
    "            if cur_accuracy > best_accuracy:\n",
    "                print(\"Saving the best model\")\n",
    "                net_name = checkpoint_directory + '//' + '/maxvit_64_beam'\n",
    "                torch.save(net.state_dict(), net_name)\n",
    "                best_accuracy = cur_accuracy\n",
    "            print(\"updated best accuracy\", best_accuracy)\n",
    "\n",
    "\n",
    "        print(\"Saving the predicted value in a csv file\")\n",
    "        file_to_save = f'{save_directory}//topk_pred_beam_val_after_{epoch + 1}th_epoch.csv'\n",
    "        indx = np.arange(1, len(top1_pred_out) + 1, 1)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['index'] = indx\n",
    "        df1['link_status'] = gt_beam\n",
    "        df1['top1_pred'] = top1_pred_out\n",
    "        df1['top3_pred'] = top3_pred_out\n",
    "        df1['top5_pred'] = top5_pred_out\n",
    "        df1['top7_pred'] = top7_pred_out\n",
    "        df1['top9_pred'] = top9_pred_out\n",
    "        df1['top11_pred'] = top11_pred_out\n",
    "        df1['top13_pred'] = top13_pred_out\n",
    "        df1['top15_pred'] = top15_pred_out\n",
    "        df1.to_csv(file_to_save, index=False)\n",
    "\n",
    "        LR_sch.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a8ffc96-87b6-4a33-abe1-b1ad27abfc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n",
      "total examples are 7440\n",
      "Average Top-1 accuracy 0.2622311827956989\n",
      "Average Top-3 accuracy 0.4252688172043011\n",
      "Average Top-5 accuracy 0.49233870967741933\n",
      "Average Top-7 accuracy 0.5346774193548387\n",
      "Average Top-9 accuracy 0.5688172043010753\n",
      "Average Top-11 accuracy 0.6004032258064517\n",
      "Average Top-13 accuracy 0.6307795698924731\n",
      "Average Top-15 accuracy 0.6534946236559139\n"
     ]
    }
   ],
   "source": [
    "print('Start Validation')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = torch.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "gt_beam = []\n",
    "total_count = 0\n",
    "\n",
    "for val_count, (imgs, labels) in enumerate(val_loader):\n",
    "    net.eval()\n",
    "    x = imgs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = torch.max(out, dim=1)\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "    sorted_out = torch.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = torch.sum(top_1_pred == labels, dtype=torch.float32)\n",
    "    batch_top3_acc = torch.sum(top_3_pred == tiled_3_labels, dtype=torch.float32)\n",
    "    batch_top5_acc = torch.sum(top_5_pred == tiled_5_labels, dtype=torch.float32)\n",
    "    batch_top7_acc = torch.sum(top_7_pred == tiled_7_labels, dtype=torch.float32)\n",
    "    batch_top9_acc = torch.sum(top_9_pred == tiled_9_labels, dtype=torch.float32)\n",
    "    batch_top11_acc = torch.sum(top_11_pred == tiled_11_labels, dtype=torch.float32)\n",
    "    batch_top13_acc = torch.sum(top_13_pred == tiled_13_labels, dtype=torch.float32)\n",
    "    batch_top15_acc = torch.sum(top_15_pred == tiled_15_labels, dtype=torch.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Average Top-1 accuracy {}'.format(running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format(running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format(running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format(running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format(running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format(running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format(running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format(running_top15_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c21de0f8-7e7c-4fcf-8178-51ae9753bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval.csv'\n",
    "\n",
    "indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = indx\n",
    "df2['link_status'] = gt_beam  # Add the link_status column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23036a43-6283-4e6a-9c32-0ebbbbd85d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd5fe60-ae8f-42c9-babe-25976aa72d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "test_dir = './scenario36_64_img_beam_test.csv'\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(test_dir)\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "link_status_data = test_data['original_unit1_pwr3_best-beam'].tolist()\n",
    "org = test_data['original_index'].tolist()\n",
    "pwr_60ghz = test_data['original_unit1_pwr3'].tolist()\n",
    "\n",
    "# Load the model checkpoint and prepare for evaluation\n",
    "checkpoint_path = f'{checkpoint_directory}/maxvit_64_beam'  # Update the checkpoint path\n",
    "net = create_model('maxvit_tiny_tf_224', pretrained=False, num_classes=65)  # Ensure the model is instantiated correctly\n",
    "net.load_state_dict(torch.load(checkpoint_path))\n",
    "net.eval()\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7c80d90-ff2c-4cf7-8663-5f981b0f52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(DataFeed(test_dir, transform=proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f358d4a-067c-4a3b-b770-ce00a2555db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n",
      "total test examples are 2480\n",
      "Average Top-1 accuracy 0.267741935483871\n",
      "Average Top-3 accuracy 0.4258064516129032\n",
      "Average Top-5 accuracy 0.47943548387096774\n",
      "Average Top-7 accuracy 0.5338709677419354\n",
      "Average Top-9 accuracy 0.5689516129032258\n",
      "Average Top-11 accuracy 0.6125\n",
      "Average Top-13 accuracy 0.6415322580645161\n",
      "Average Top-15 accuracy 0.6754032258064516\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "print('Start Testing')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = torch.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "gt_beam = []\n",
    "total_count = 0\n",
    "\n",
    "for val_count, (imgs, labels) in enumerate(test_loader):\n",
    "    net.eval()\n",
    "    x = imgs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = torch.max(out, dim=1)\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "    sorted_out = torch.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = torch.sum(top_1_pred == labels, dtype=torch.float32)\n",
    "    batch_top3_acc = torch.sum(top_3_pred == tiled_3_labels, dtype=torch.float32)\n",
    "    batch_top5_acc = torch.sum(top_5_pred == tiled_5_labels, dtype=torch.float32)\n",
    "    batch_top7_acc = torch.sum(top_7_pred == tiled_7_labels, dtype=torch.float32)\n",
    "    batch_top9_acc = torch.sum(top_9_pred == tiled_9_labels, dtype=torch.float32)\n",
    "    batch_top11_acc = torch.sum(top_11_pred == tiled_11_labels, dtype=torch.float32)\n",
    "    batch_top13_acc = torch.sum(top_13_pred == tiled_13_labels, dtype=torch.float32)\n",
    "    batch_top15_acc = torch.sum(top_15_pred == tiled_15_labels, dtype=torch.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total test examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Average Top-1 accuracy {}'.format(running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format(running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format(running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format(running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format(running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format(running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format(running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format(running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval_Test.csv'\n",
    "\n",
    "indx = test_data.index + 1\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = org\n",
    "df2['link_status'] = link_status_data\n",
    "df2['original_unit1_pwr3'] = pwr_60ghz\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
