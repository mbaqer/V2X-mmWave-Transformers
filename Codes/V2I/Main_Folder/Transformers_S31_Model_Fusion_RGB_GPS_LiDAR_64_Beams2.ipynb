{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual, LiDAR, and GPS: Cross-Attention and Shared Attention Mode Fusion with RGB and LiDAR Transformers\n",
    "# DeepSense Scenario 31 64 Beams!\n",
    "\n",
    "# Average Top-1 accuracy 0.36894586894586895\n",
    "# Average Top-3 accuracy 0.7350427350427351\n",
    "# Average Top-5 accuracy 0.8603988603988604\n",
    "# Average Top-7 accuracy 0.915954415954416\n",
    "# Average Top-9 accuracy 0.9401709401709402\n",
    "# Average Top-11 accuracy 0.9529914529914529\n",
    "# Average Top-13 accuracy 0.9643874643874644\n",
    "# Average Top-15 accuracy 0.9672364672364673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "filHmVtMIoQ-",
    "outputId": "d59b1f0e-2ae2-4ed8-f32c-d1d71a777e79"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZ8W3-e6IK_k",
    "outputId": "fb8620f1-5dce-4c3a-cecf-4b20196b466c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# Define the zip file path and the extraction folder\n",
    "# zip_path = \"/content/drive/MyDrive/Shared2/scenario36.zip\"\n",
    "# extract_folder = \"\"\n",
    "\n",
    "# if not os.path.exists(\"/content/scenario36\"):\n",
    "  # Open and extract the zip file\n",
    "  # with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "      # zip_ref.extractall(extract_folder)\n",
    "\n",
    "# print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "P7CrazIKBKWX"
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torchvision.transforms as transf\n",
    "from torchsummary import summary\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import torch as t\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from timm import create_model\n",
    "\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1lfBlZ9BOuY",
    "outputId": "29b28047-d38e-4af2-af09-3ee5493caccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-07-2025\n",
      "02_48\n",
      "C:\\Users\\Baqer\\Desktop\\V2X_CNN_All\\Scenario31_64-Beams\\Main_Folder//saved_folder//02-07-2025_02_48\n"
     ]
    }
   ],
   "source": [
    "# Save directory\n",
    "# year month day\n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds\n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)\n",
    "\n",
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime\n",
    "print(pwd)\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)\n",
    "\n",
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "    isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from .ply file\n",
    "def load_ply_file(file_path):\n",
    "    plydata = PlyData.read(file_path)\n",
    "    points = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Feeding: Create data sample list\n",
    "curPath = str(os.getcwd())\n",
    "class DataFeed(Dataset):\n",
    "    '''\n",
    "    A class retrieving a tuple of (image,label). It can handle the case\n",
    "    of empty classes (empty folders).\n",
    "    Args:\n",
    "        path_to_img: path to image csv file data\n",
    "        path_to_lidar: path to LiDAR csv data\n",
    "    '''\n",
    "    def __init__(self, path_to_img,\n",
    "                        path_to_gps,\n",
    "                        path_to_lidar,\n",
    "                        num_points=15000,\n",
    "                        nat_sort = False, transform=None, init_shuflle = True):\n",
    "\n",
    "        self.rgb_samples = self.read_csv(path_to_img)\n",
    "        self.gps_samples = self.read_csv(path_to_gps)\n",
    "        self.lidar_samples = self.read_csv(path_to_lidar)\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.transform = transform\n",
    "\n",
    "    def read_csv(self, path_to_cvs):\n",
    "        return pd.read_csv(path_to_cvs)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.rgb_samples )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_rgb = self.rgb_samples.loc[idx]\n",
    "        idx = sample_rgb['original_index']\n",
    "\n",
    "        img = io.imread(sample_rgb[1])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        gps_idx = self.gps_samples.index[self.gps_samples['original_index'] == idx].item()\n",
    "        sample_gps = self.gps_samples.loc[gps_idx]\n",
    "\n",
    "        pos_val = sample_gps[1]\n",
    "        pos_val = ast.literal_eval(pos_val)\n",
    "    \n",
    "        ######################\n",
    "        \n",
    "        lidar_idx = self.lidar_samples.index[self.lidar_samples['original_index'] == idx].item()\n",
    "        sample_lidar = self.lidar_samples.loc[lidar_idx]\n",
    "        # print(sample_lidar[1])\n",
    "        points = load_ply_file(sample_lidar[1])\n",
    "\n",
    "        if points.shape[0] < self.num_points:\n",
    "            points = np.pad(points, ((0, self.num_points - points.shape[0]), (0, 0)), mode='constant')\n",
    "        elif points.shape[0] > self.num_points:\n",
    "            indices = np.random.choice(points.shape[0], self.num_points, replace=False)\n",
    "            points = points[indices]\n",
    "            \n",
    "        points = torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "        label = sample_rgb[2]\n",
    "        return img, torch.tensor(pos_val), torch.tensor(points).float().T, torch.tensor(label).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-60-en1zCKRe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 12% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 13% |\n"
     ]
    }
   ],
   "source": [
    "# !pip install GP\n",
    "# !pip install numbaUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SY-z1G3COeC",
    "outputId": "e10f75fa-d4f6-47cd-d081-b3726774520f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if device == \"cuda\":\n",
    "  print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "  print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ctM1HVGcEgjM"
   },
   "outputs": [],
   "source": [
    "class InputLayer(nn.Module):\n",
    "    \"\"\"Input Layer to accept input point cloud data.\"\"\"\n",
    "    def __init__(self, k=3):\n",
    "        super(InputLayer, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure the input shape is (batch_size, num_points, channels)\n",
    "        return x.transpose(1, 2)  # Change shape to (batch_size, channels, num_points)\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"Embedding Layer for dimensionality reduction.\"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be in shape (batch_size, channels, num_points)\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, channels, num_points)\n",
    "        return F.relu(self.conv(x))  # Applies a 1x1 convolution\n",
    "\n",
    "\n",
    "class EncoderStage(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.3):\n",
    "        super(EncoderStage, self).__init__()\n",
    "        self.cpe = nn.Conv1d(in_channels, out_channels, kernel_size=1)  # CPE\n",
    "        self.norm = nn.BatchNorm1d(out_channels)  # Normalization\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=out_channels, num_heads=8)  # Attention\n",
    "        self.dropout_att = nn.Dropout(dropout_rate)  # Dropout after attention\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels, out_channels),  # MLP layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout after first MLP layer\n",
    "            nn.Linear(out_channels, out_channels)   # Another MLP layer\n",
    "        )\n",
    "        self.dropout_mlp = nn.Dropout(dropout_rate)  # Dropout after second MLP layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply CPE\n",
    "        identity = x  # Save the input for residual connection\n",
    "        x = F.relu(self.cpe(x))\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Prepare for Attention mechanism\n",
    "        x_att = x.transpose(2, 1)  # Shape change for attention\n",
    "        x_att, _ = self.attention(x_att, x_att, x_att)  # Attention output\n",
    "        x_att = self.dropout_att(x_att)  # Apply dropout\n",
    "\n",
    "        x_att = x_att.transpose(2, 1)  # Transpose back\n",
    "        x = x + x_att  # Skip connection\n",
    "\n",
    "        # MLP\n",
    "        b, c, p = x.size()\n",
    "        x = x.view(b * p, c)  # Flatten for MLP\n",
    "        x = self.mlp(x)  # Apply MLP\n",
    "        x = self.dropout_mlp(x)  # Apply dropout after MLP\n",
    "\n",
    "        # Reshape back to (B, out_channels, num_points)\n",
    "        output_channels = x.size(1)\n",
    "        x = x.view(b, output_channels, p)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SerializedPoolingLayer(nn.Module):\n",
    "    \"\"\"Serialized Pooling Layer for dimensionality reduction.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SerializedPoolingLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool1d(x, kernel_size=2)  # Average pooling\n",
    "\n",
    "\n",
    "class PointTransformerV3(nn.Module):\n",
    "    \"\"\"Main Point Transformer V3 for Classification.\"\"\"\n",
    "    def __init__(self, num_classes= 65, num_points=15000, enc_depths=[64, 128, 256], dropout_rate=0.3):\n",
    "        super(PointTransformerV3, self).__init__()\n",
    "\n",
    "        # Input and embedding\n",
    "        self.input_layer = InputLayer()\n",
    "        self.embedding = EmbeddingLayer(input_channels=3, output_channels=enc_depths[0])  # Initial embedding layer\n",
    "\n",
    "        # Encoder stages\n",
    "        self.encoders = nn.ModuleList()\n",
    "        in_channels = enc_depths[0]\n",
    "        for out_channels in enc_depths:\n",
    "            self.encoders.append(EncoderStage(in_channels, out_channels, dropout_rate=dropout_rate))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Serialized pooling\n",
    "        self.serialized_pooling = SerializedPoolingLayer()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(enc_depths[-1], 128)  # Input size from enc_depths[-1]\n",
    "        self.fc2 = nn.Linear(128, 64)  # Intermediate layer\n",
    "        self.fc3 = nn.Linear(64, num_classes)  # Output layer for 65 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pass through encoder stages\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "\n",
    "        # Serialized pooling\n",
    "        x = self.serialized_pooling(x)\n",
    "\n",
    "        # Global feature extraction\n",
    "        x = torch.max(x, dim=2)[0]  # Global max pooling across points\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 rgb_encoder,\n",
    "                 lidar_encoder,\n",
    "                 num_features,\n",
    "                 num_heads,\n",
    "                 num_classes=65):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.rgb_encoder = rgb_encoder\n",
    "        self.lidar_encoder = lidar_encoder\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_heads = num_heads\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Shared attention mechanism\n",
    "        self.shared_attn = nn.MultiheadAttention(embed_dim=num_features, num_heads=num_heads)\n",
    "\n",
    "        # Shared projection layers for Q, K, V\n",
    "        self.q_proj = nn.Linear(num_features, num_features)\n",
    "        self.k_proj = nn.Linear(num_features, num_features)\n",
    "        self.v_proj = nn.Linear(num_features, num_features)\n",
    "\n",
    "        # Normalization layers\n",
    "        self.bn_fusion = nn.BatchNorm1d(512)\n",
    "\n",
    "        # Final projection layers\n",
    "        self.fc1 = nn.Linear(520, 3 * num_features)\n",
    "        self.fc2 = nn.Linear(3 * num_features, num_features)\n",
    "        self.fc3 = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Using a simple MLP for the beam prediction\n",
    "        self.positional_fc = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def modal_transformer(self, z):\n",
    "        \"\"\"Shared projections for all modalities\"\"\"\n",
    "        q = self.q_proj(z)\n",
    "        k = self.k_proj(z)\n",
    "        v = self.v_proj(z)\n",
    "        return q, k, v\n",
    "\n",
    "    def cross_modal_attention(self, Qa, Kb, Vb):\n",
    "        \"\"\"Shared attention mechanism for cross-modal interaction\"\"\"\n",
    "        Qa = Qa.unsqueeze(0)  # Add sequence dimension\n",
    "        Kb = Kb.unsqueeze(0)\n",
    "        Vb = Vb.unsqueeze(0)\n",
    "\n",
    "        attn_output, _ = self.shared_attn(Qa, Kb, Vb)\n",
    "        return attn_output.squeeze(0)  # Remove sequence dimension\n",
    "\n",
    "    def forward(self, inp_rgb, inp_gps, inp_lidar):\n",
    "        # Encode modalities\n",
    "        encoded_rgb = self.rgb_encoder(inp_rgb)\n",
    "        encoded_lidar = self.lidar_encoder(inp_lidar)\n",
    "\n",
    "        # Generate shared projections\n",
    "        Q_rgb, K_rgb, V_rgb = self.modal_transformer(encoded_rgb)\n",
    "        Q_lidar, K_lidar, V_lidar = self.modal_transformer(encoded_lidar)\n",
    "\n",
    "        # Cross-modal attention with shared mechanism\n",
    "        C_lidar_rgb = self.cross_modal_attention(Q_rgb, K_lidar, V_lidar) + encoded_rgb\n",
    "        C_rgb_lidar = self.cross_modal_attention(Q_lidar, K_rgb, V_rgb) + encoded_lidar\n",
    "\n",
    "        # Fusion and classification\n",
    "        F_cross = torch.cat((C_lidar_rgb, C_rgb_lidar), dim=-1)\n",
    "        F_cross = self.bn_fusion(F_cross)\n",
    "\n",
    "        # Process positional data through the simple MLP\n",
    "        y = self.positional_fc(inp_gps)\n",
    "        \n",
    "        x = torch.cat([F_cross, y], dim=1)\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YT-8r2NjB5Qr"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # 8\n",
    "val_batch_size = 1\n",
    "lr = 0.001 # 1e-3\n",
    "decay = 1e-4\n",
    "num_epochs = 40 # After ** epoch, the accuracy remains same!\n",
    "train_size = [1]\n",
    "\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "rgb_proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "\n",
    "rgb_train_dir = 'scenario31_64_img_beam_train.csv'\n",
    "rgb_val_dir = 'scenario31_64_img_beam_val.csv'\n",
    "\n",
    "gps_train_dir = 'scenario31_64_pos_beam_train.csv'\n",
    "gps_val_dir = 'scenario31_64_pos_beam_val.csv'\n",
    "\n",
    "lidar_train_dir = 'scenario31_64_lidar_beam_train.csv'\n",
    "lidar_val_dir = 'scenario31_64_lidar_beam_val.csv'\n",
    "\n",
    "\n",
    "ds_train = DataFeed(path_to_img=rgb_train_dir,\n",
    "                        path_to_gps=gps_train_dir,\n",
    "                        path_to_lidar=lidar_train_dir,\n",
    "                        transform=rgb_proc_pipe)\n",
    "\n",
    "ds_val = DataFeed(path_to_img=rgb_val_dir,\n",
    "                  path_to_gps=gps_val_dir,\n",
    "                  path_to_lidar=lidar_val_dir,\n",
    "                  transform=rgb_proc_pipe)\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(ds_val, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZUgKQEZkGiH3"
   },
   "outputs": [],
   "source": [
    "# Updated model initialization\n",
    "num_classes=65\n",
    "num_features = 256\n",
    "num_heads = 8\n",
    "\n",
    "# RGB encoder uses 2D convolutions\n",
    "rgb_encoder = PointTransformerV3(num_classes=num_features)\n",
    "\n",
    "# LiDAR encoder remains unchanged\n",
    "lidar_encoder = PointTransformerV3(num_classes=num_features)\n",
    "\n",
    "# Fusion model stays the same\n",
    "fusion_model = FusionModel(rgb_encoder, lidar_encoder, num_features, num_heads).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5H0SzAaGqHu",
    "outputId": "03568110-5ece-4e78-bb3c-52d2edfd2c9e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```````````````````````````````````````````````````````\n",
      "Training size is 1\n",
      "Epoch No. 1\n",
      "Training-Batch No.10\n",
      "Loss = 3.9231643676757812\n",
      "Training-Batch No.20\n",
      "Loss = 3.928873062133789\n",
      "Training-Batch No.30\n",
      "Loss = 3.4139981269836426\n",
      "Training-Batch No.40\n",
      "Loss = 3.282966136932373\n",
      "Training-Batch No.50\n",
      "Loss = 3.220262289047241\n",
      "Training-Batch No.60\n",
      "Loss = 4.931697845458984\n",
      "Training-Batch No.70\n",
      "Loss = 3.398538589477539\n",
      "Training-Batch No.80\n",
      "Loss = 3.5917744636535645\n",
      "Training-Batch No.90\n",
      "Loss = 4.089653491973877\n",
      "Training-Batch No.100\n",
      "Loss = 4.372869491577148\n",
      "Training-Batch No.110\n",
      "Loss = 3.7989606857299805\n",
      "Training-Batch No.120\n",
      "Loss = 3.4989125728607178\n",
      "Training-Batch No.130\n",
      "Loss = 3.8647618293762207\n",
      "Training-Batch No.140\n",
      "Loss = 3.4492440223693848\n",
      "Training-Batch No.150\n",
      "Loss = 3.559380292892456\n",
      "Training-Batch No.160\n",
      "Loss = 3.9555492401123047\n",
      "Training-Batch No.170\n",
      "Loss = 3.7864601612091064\n",
      "Training-Batch No.180\n",
      "Loss = 4.264975070953369\n",
      "Training-Batch No.190\n",
      "Loss = 3.372161865234375\n",
      "Training-Batch No.200\n",
      "Loss = 4.661329746246338\n",
      "Training-Batch No.210\n",
      "Loss = 3.1085364818573\n",
      "Training-Batch No.220\n",
      "Loss = 4.008208274841309\n",
      "Training-Batch No.230\n",
      "Loss = 3.622365951538086\n",
      "Training-Batch No.240\n",
      "Loss = 4.516214370727539\n",
      "Training-Batch No.250\n",
      "Loss = 3.124229907989502\n",
      "Training-Batch No.260\n",
      "Loss = 3.7977023124694824\n",
      "Training-Batch No.270\n",
      "Loss = 4.519236087799072\n",
      "Training-Batch No.280\n",
      "Loss = 3.311880111694336\n",
      "Training-Batch No.290\n",
      "Loss = 4.433751106262207\n",
      "Training-Batch No.300\n",
      "Loss = 4.191895484924316\n",
      "Training-Batch No.310\n",
      "Loss = 4.013217449188232\n",
      "Training-Batch No.320\n",
      "Loss = 3.8881235122680664\n",
      "Training-Batch No.330\n",
      "Loss = 3.803548812866211\n",
      "Training-Batch No.340\n",
      "Loss = 3.9243979454040527\n",
      "Training-Batch No.350\n",
      "Loss = 3.7483696937561035\n",
      "Training-Batch No.360\n",
      "Loss = 3.5138039588928223\n",
      "Training-Batch No.370\n",
      "Loss = 5.801234245300293\n",
      "Training-Batch No.380\n",
      "Loss = 3.850647449493408\n",
      "Training-Batch No.390\n",
      "Loss = 3.5917139053344727\n",
      "Training-Batch No.400\n",
      "Loss = 3.6196935176849365\n",
      "Training-Batch No.410\n",
      "Loss = 3.514070749282837\n",
      "Training-Batch No.420\n",
      "Loss = 3.379054546356201\n",
      "Training-Batch No.430\n",
      "Loss = 3.636084794998169\n",
      "Training-Batch No.440\n",
      "Loss = 5.010481357574463\n",
      "Training-Batch No.450\n",
      "Loss = 3.754639148712158\n",
      "Training-Batch No.460\n",
      "Loss = 3.3567392826080322\n",
      "Training-Batch No.470\n",
      "Loss = 4.093300819396973\n",
      "Training-Batch No.480\n",
      "Loss = 3.6675658226013184\n",
      "Training-Batch No.490\n",
      "Loss = 4.179147720336914\n",
      "Training-Batch No.500\n",
      "Loss = 3.945603609085083\n",
      "Training-Batch No.510\n",
      "Loss = 3.857759475708008\n",
      "Training-Batch No.520\n",
      "Loss = 2.9861443042755127\n",
      "Training-Batch No.530\n",
      "Loss = 3.4752821922302246\n",
      "Training-Batch No.540\n",
      "Loss = 3.4193310737609863\n",
      "Training-Batch No.550\n",
      "Loss = 3.6447670459747314\n",
      "Training-Batch No.560\n",
      "Loss = 3.4318418502807617\n",
      "Training-Batch No.570\n",
      "Loss = 5.412158012390137\n",
      "Training-Batch No.580\n",
      "Loss = 3.411324977874756\n",
      "Training-Batch No.590\n",
      "Loss = 3.633902072906494\n",
      "Training-Batch No.600\n",
      "Loss = 3.630805492401123\n",
      "Training-Batch No.610\n",
      "Loss = 4.078795433044434\n",
      "Training-Batch No.620\n",
      "Loss = 3.431367874145508\n",
      "Training-Batch No.630\n",
      "Loss = 3.2831451892852783\n",
      "Training-Batch No.640\n",
      "Loss = 3.6890387535095215\n",
      "Training-Batch No.650\n",
      "Loss = 3.5191457271575928\n",
      "Training-Batch No.660\n",
      "Loss = 3.3385086059570312\n",
      "Training-Batch No.670\n",
      "Loss = 3.6764025688171387\n",
      "Training-Batch No.680\n",
      "Loss = 3.5698728561401367\n",
      "Training-Batch No.690\n",
      "Loss = 3.4362311363220215\n",
      "Training-Batch No.700\n",
      "Loss = 3.1492669582366943\n",
      "Training-Batch No.710\n",
      "Loss = 4.152827739715576\n",
      "Training-Batch No.720\n",
      "Loss = 3.3648972511291504\n",
      "Training-Batch No.730\n",
      "Loss = 4.20271110534668\n",
      "Training-Batch No.740\n",
      "Loss = 3.689206123352051\n",
      "Training-Batch No.750\n",
      "Loss = 4.135717391967773\n",
      "Training-Batch No.760\n",
      "Loss = 3.279615879058838\n",
      "Training-Batch No.770\n",
      "Loss = 3.3167924880981445\n",
      "Training-Batch No.780\n",
      "Loss = 4.519908905029297\n",
      "Training-Batch No.790\n",
      "Loss = 3.3324971199035645\n",
      "Training-Batch No.800\n",
      "Loss = 3.9219956398010254\n",
      "Training-Batch No.810\n",
      "Loss = 3.073551654815674\n",
      "Training-Batch No.820\n",
      "Loss = 3.2763028144836426\n",
      "Training-Batch No.830\n",
      "Loss = 3.7418816089630127\n",
      "Training-Batch No.840\n",
      "Loss = 3.6853227615356445\n",
      "Training-Batch No.850\n",
      "Loss = 3.481685161590576\n",
      "Training-Batch No.860\n",
      "Loss = 4.248011112213135\n",
      "Training-Batch No.870\n",
      "Loss = 3.8892197608947754\n",
      "Training-Batch No.880\n",
      "Loss = 3.058403730392456\n",
      "Training-Batch No.890\n",
      "Loss = 4.223028182983398\n",
      "Training-Batch No.900\n",
      "Loss = 3.6462759971618652\n",
      "Training-Batch No.910\n",
      "Loss = 3.433835029602051\n",
      "Training-Batch No.920\n",
      "Loss = 3.931406021118164\n",
      "Training-Batch No.930\n",
      "Loss = 3.2952051162719727\n",
      "Training-Batch No.940\n",
      "Loss = 3.1825456619262695\n",
      "Training-Batch No.950\n",
      "Loss = 3.303992748260498\n",
      "Training-Batch No.960\n",
      "Loss = 4.316810607910156\n",
      "Training-Batch No.970\n",
      "Loss = 3.3423194885253906\n",
      "Training-Batch No.980\n",
      "Loss = 3.118502616882324\n",
      "Training-Batch No.990\n",
      "Loss = 3.479123115539551\n",
      "Training-Batch No.1000\n",
      "Loss = 2.546445846557617\n",
      "Training-Batch No.1010\n",
      "Loss = 3.321357250213623\n",
      "Training-Batch No.1020\n",
      "Loss = 3.737802028656006\n",
      "Training-Batch No.1030\n",
      "Loss = 3.4297170639038086\n",
      "Training-Batch No.1040\n",
      "Loss = 4.0856804847717285\n",
      "Training-Batch No.1050\n",
      "Loss = 4.065243721008301\n",
      "Epoch 1 Training Loss: 3.7652\n",
      "Start validation\n",
      "Epoch 1 Validation Loss: 8.5110\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.060865430337612936\n",
      "Average Top-3 accuracy 0.15834522111269614\n",
      "Average Top-5 accuracy 0.2501188777936281\n",
      "Average Top-7 accuracy 0.357584403233476\n",
      "Average Top-9 accuracy 0.4560152163575844\n",
      "Average Top-11 accuracy 0.5263908701854494\n",
      "Average Top-13 accuracy 0.5872563005230623\n",
      "Average Top-15 accuracy 0.6405135520684736\n",
      "current acc 0.060865430337612936\n",
      "best acc 0\n",
      "Saving the best model\n",
      "updated best accuracy 0.060865430337612936\n",
      "Epoch No. 2\n",
      "Training-Batch No.1060\n",
      "Loss = 3.633939266204834\n",
      "Training-Batch No.1070\n",
      "Loss = 3.2640433311462402\n",
      "Training-Batch No.1080\n",
      "Loss = 3.3983163833618164\n",
      "Training-Batch No.1090\n",
      "Loss = 3.1762375831604004\n",
      "Training-Batch No.1100\n",
      "Loss = 4.157519340515137\n",
      "Training-Batch No.1110\n",
      "Loss = 3.9680893421173096\n",
      "Training-Batch No.1120\n",
      "Loss = 3.3647966384887695\n",
      "Training-Batch No.1130\n",
      "Loss = 5.089290618896484\n",
      "Training-Batch No.1140\n",
      "Loss = 3.419680595397949\n",
      "Training-Batch No.1150\n",
      "Loss = 3.5922393798828125\n",
      "Training-Batch No.1160\n",
      "Loss = 4.073019027709961\n",
      "Training-Batch No.1170\n",
      "Loss = 3.556533098220825\n",
      "Training-Batch No.1180\n",
      "Loss = 3.655994415283203\n",
      "Training-Batch No.1190\n",
      "Loss = 3.6000967025756836\n",
      "Training-Batch No.1200\n",
      "Loss = 3.617720127105713\n",
      "Training-Batch No.1210\n",
      "Loss = 4.164371967315674\n",
      "Training-Batch No.1220\n",
      "Loss = 3.706895351409912\n",
      "Training-Batch No.1230\n",
      "Loss = 4.605215072631836\n",
      "Training-Batch No.1240\n",
      "Loss = 4.376579761505127\n",
      "Training-Batch No.1250\n",
      "Loss = 3.958961009979248\n",
      "Training-Batch No.1260\n",
      "Loss = 3.9833450317382812\n",
      "Training-Batch No.1270\n",
      "Loss = 3.448604106903076\n",
      "Training-Batch No.1280\n",
      "Loss = 3.779308319091797\n",
      "Training-Batch No.1290\n",
      "Loss = 3.6695899963378906\n",
      "Training-Batch No.1300\n",
      "Loss = 3.9971024990081787\n",
      "Training-Batch No.1310\n",
      "Loss = 4.636782646179199\n",
      "Training-Batch No.1320\n",
      "Loss = 3.2564566135406494\n",
      "Training-Batch No.1330\n",
      "Loss = 3.762619733810425\n",
      "Training-Batch No.1340\n",
      "Loss = 3.1756696701049805\n",
      "Training-Batch No.1350\n",
      "Loss = 3.7608642578125\n",
      "Training-Batch No.1360\n",
      "Loss = 3.801743507385254\n",
      "Training-Batch No.1370\n",
      "Loss = 3.333588123321533\n",
      "Training-Batch No.1380\n",
      "Loss = 3.731471538543701\n",
      "Training-Batch No.1390\n",
      "Loss = 3.6190717220306396\n",
      "Training-Batch No.1400\n",
      "Loss = 3.548171281814575\n",
      "Training-Batch No.1410\n",
      "Loss = 3.283264398574829\n",
      "Training-Batch No.1420\n",
      "Loss = 3.6232380867004395\n",
      "Training-Batch No.1430\n",
      "Loss = 5.296535968780518\n",
      "Training-Batch No.1440\n",
      "Loss = 3.937162160873413\n",
      "Training-Batch No.1450\n",
      "Loss = 3.7140161991119385\n",
      "Training-Batch No.1460\n",
      "Loss = 3.5875566005706787\n",
      "Training-Batch No.1470\n",
      "Loss = 3.5385665893554688\n",
      "Training-Batch No.1480\n",
      "Loss = 2.9643375873565674\n",
      "Training-Batch No.1490\n",
      "Loss = 3.530914306640625\n",
      "Training-Batch No.1500\n",
      "Loss = 3.6090497970581055\n",
      "Training-Batch No.1510\n",
      "Loss = 3.6215267181396484\n",
      "Training-Batch No.1520\n",
      "Loss = 3.890359401702881\n",
      "Training-Batch No.1530\n",
      "Loss = 3.017674207687378\n",
      "Training-Batch No.1540\n",
      "Loss = 3.1411986351013184\n",
      "Training-Batch No.1550\n",
      "Loss = 3.1583447456359863\n",
      "Training-Batch No.1560\n",
      "Loss = 3.148730516433716\n",
      "Training-Batch No.1570\n",
      "Loss = 3.2192578315734863\n",
      "Training-Batch No.1580\n",
      "Loss = 3.1363911628723145\n",
      "Training-Batch No.1590\n",
      "Loss = 3.0457897186279297\n",
      "Training-Batch No.1600\n",
      "Loss = 3.548729181289673\n",
      "Training-Batch No.1610\n",
      "Loss = 5.537513732910156\n",
      "Training-Batch No.1620\n",
      "Loss = 3.841920852661133\n",
      "Training-Batch No.1630\n",
      "Loss = 3.470590829849243\n",
      "Training-Batch No.1640\n",
      "Loss = 3.5770931243896484\n",
      "Training-Batch No.1650\n",
      "Loss = 3.9586563110351562\n",
      "Training-Batch No.1660\n",
      "Loss = 3.696226119995117\n",
      "Training-Batch No.1670\n",
      "Loss = 3.199125051498413\n",
      "Training-Batch No.1680\n",
      "Loss = 2.8682475090026855\n",
      "Training-Batch No.1690\n",
      "Loss = 3.685774326324463\n",
      "Training-Batch No.1700\n",
      "Loss = 3.914646625518799\n",
      "Training-Batch No.1710\n",
      "Loss = 3.452648878097534\n",
      "Training-Batch No.1720\n",
      "Loss = 3.4913477897644043\n",
      "Training-Batch No.1730\n",
      "Loss = 3.494227170944214\n",
      "Training-Batch No.1740\n",
      "Loss = 4.12748908996582\n",
      "Training-Batch No.1750\n",
      "Loss = 3.196690320968628\n",
      "Training-Batch No.1760\n",
      "Loss = 3.9039206504821777\n",
      "Training-Batch No.1770\n",
      "Loss = 3.4452970027923584\n",
      "Training-Batch No.1780\n",
      "Loss = 3.2686614990234375\n",
      "Training-Batch No.1790\n",
      "Loss = 3.3482227325439453\n",
      "Training-Batch No.1800\n",
      "Loss = 3.5651872158050537\n",
      "Training-Batch No.1810\n",
      "Loss = 3.3342556953430176\n",
      "Training-Batch No.1820\n",
      "Loss = 2.9227168560028076\n",
      "Training-Batch No.1830\n",
      "Loss = 3.717653751373291\n",
      "Training-Batch No.1840\n",
      "Loss = 3.6021811962127686\n",
      "Training-Batch No.1850\n",
      "Loss = 3.9186935424804688\n",
      "Training-Batch No.1860\n",
      "Loss = 3.463253974914551\n",
      "Training-Batch No.1870\n",
      "Loss = 3.422335624694824\n",
      "Training-Batch No.1880\n",
      "Loss = 3.591372489929199\n",
      "Training-Batch No.1890\n",
      "Loss = 2.9550580978393555\n",
      "Training-Batch No.1900\n",
      "Loss = 4.3615641593933105\n",
      "Training-Batch No.1910\n",
      "Loss = 4.151529788970947\n",
      "Training-Batch No.1920\n",
      "Loss = 2.8401925563812256\n",
      "Training-Batch No.1930\n",
      "Loss = 3.919412612915039\n",
      "Training-Batch No.1940\n",
      "Loss = 2.837700128555298\n",
      "Training-Batch No.1950\n",
      "Loss = 3.6354637145996094\n",
      "Training-Batch No.1960\n",
      "Loss = 3.4142556190490723\n",
      "Training-Batch No.1970\n",
      "Loss = 3.715467929840088\n",
      "Training-Batch No.1980\n",
      "Loss = 3.240837574005127\n",
      "Training-Batch No.1990\n",
      "Loss = 3.3902692794799805\n",
      "Training-Batch No.2000\n",
      "Loss = 3.4782209396362305\n",
      "Training-Batch No.2010\n",
      "Loss = 3.0329971313476562\n",
      "Training-Batch No.2020\n",
      "Loss = 3.200657606124878\n",
      "Training-Batch No.2030\n",
      "Loss = 3.257869243621826\n",
      "Training-Batch No.2040\n",
      "Loss = 4.1128740310668945\n",
      "Training-Batch No.2050\n",
      "Loss = 3.6814026832580566\n",
      "Training-Batch No.2060\n",
      "Loss = 3.554068088531494\n",
      "Training-Batch No.2070\n",
      "Loss = 3.1670444011688232\n",
      "Training-Batch No.2080\n",
      "Loss = 3.4775099754333496\n",
      "Training-Batch No.2090\n",
      "Loss = 3.4656779766082764\n",
      "Training-Batch No.2100\n",
      "Loss = 3.4444751739501953\n",
      "Epoch 2 Training Loss: 3.6698\n",
      "Start validation\n",
      "Epoch 2 Validation Loss: 6.6599\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.060865430337612936\n",
      "Average Top-3 accuracy 0.15216357584403234\n",
      "Average Top-5 accuracy 0.24964336661911554\n",
      "Average Top-7 accuracy 0.3775558725630052\n",
      "Average Top-9 accuracy 0.4617213504517356\n",
      "Average Top-11 accuracy 0.519733713742273\n",
      "Average Top-13 accuracy 0.5739419876367094\n",
      "Average Top-15 accuracy 0.6405135520684736\n",
      "current acc 0.060865430337612936\n",
      "best acc 0.060865430337612936\n",
      "updated best accuracy 0.060865430337612936\n",
      "Epoch No. 3\n",
      "Training-Batch No.2110\n",
      "Loss = 3.546265125274658\n",
      "Training-Batch No.2120\n",
      "Loss = 3.560133457183838\n",
      "Training-Batch No.2130\n",
      "Loss = 2.8386526107788086\n",
      "Training-Batch No.2140\n",
      "Loss = 4.762625694274902\n",
      "Training-Batch No.2150\n",
      "Loss = 3.174569606781006\n",
      "Training-Batch No.2160\n",
      "Loss = 3.2463760375976562\n",
      "Training-Batch No.2170\n",
      "Loss = 4.03823184967041\n",
      "Training-Batch No.2180\n",
      "Loss = 2.6625237464904785\n",
      "Training-Batch No.2190\n",
      "Loss = 3.2236878871917725\n",
      "Training-Batch No.2200\n",
      "Loss = 3.500412940979004\n",
      "Training-Batch No.2210\n",
      "Loss = 3.573606491088867\n",
      "Training-Batch No.2220\n",
      "Loss = 3.282254219055176\n",
      "Training-Batch No.2230\n",
      "Loss = 2.9473729133605957\n",
      "Training-Batch No.2240\n",
      "Loss = 4.233179569244385\n",
      "Training-Batch No.2250\n",
      "Loss = 3.3062760829925537\n",
      "Training-Batch No.2260\n",
      "Loss = 3.2348310947418213\n",
      "Training-Batch No.2270\n",
      "Loss = 2.746638298034668\n",
      "Training-Batch No.2280\n",
      "Loss = 4.772125244140625\n",
      "Training-Batch No.2290\n",
      "Loss = 4.373295307159424\n",
      "Training-Batch No.2300\n",
      "Loss = 3.4737935066223145\n",
      "Training-Batch No.2310\n",
      "Loss = 4.110764503479004\n",
      "Training-Batch No.2320\n",
      "Loss = 3.5610921382904053\n",
      "Training-Batch No.2330\n",
      "Loss = 3.308107614517212\n",
      "Training-Batch No.2340\n",
      "Loss = 3.293541431427002\n",
      "Training-Batch No.2350\n",
      "Loss = 2.937833309173584\n",
      "Training-Batch No.2360\n",
      "Loss = 3.713200092315674\n",
      "Training-Batch No.2370\n",
      "Loss = 3.921633243560791\n",
      "Training-Batch No.2380\n",
      "Loss = 3.0189905166625977\n",
      "Training-Batch No.2390\n",
      "Loss = 3.6747026443481445\n",
      "Training-Batch No.2400\n",
      "Loss = 5.435853004455566\n",
      "Training-Batch No.2410\n",
      "Loss = 3.6093692779541016\n",
      "Training-Batch No.2420\n",
      "Loss = 3.3300869464874268\n",
      "Training-Batch No.2430\n",
      "Loss = 3.9432802200317383\n",
      "Training-Batch No.2440\n",
      "Loss = 3.608224630355835\n",
      "Training-Batch No.2450\n",
      "Loss = 3.8804757595062256\n",
      "Training-Batch No.2460\n",
      "Loss = 3.3832144737243652\n",
      "Training-Batch No.2470\n",
      "Loss = 3.5118496417999268\n",
      "Training-Batch No.2480\n",
      "Loss = 4.04023551940918\n",
      "Training-Batch No.2490\n",
      "Loss = 4.19486665725708\n",
      "Training-Batch No.2500\n",
      "Loss = 4.069308757781982\n",
      "Training-Batch No.2510\n",
      "Loss = 4.062239170074463\n",
      "Training-Batch No.2520\n",
      "Loss = 3.3904712200164795\n",
      "Training-Batch No.2530\n",
      "Loss = 3.729370594024658\n",
      "Training-Batch No.2540\n",
      "Loss = 3.3139872550964355\n",
      "Training-Batch No.2550\n",
      "Loss = 3.2590763568878174\n",
      "Training-Batch No.2560\n",
      "Loss = 3.6168124675750732\n",
      "Training-Batch No.2570\n",
      "Loss = 3.324885845184326\n",
      "Training-Batch No.2580\n",
      "Loss = 3.552232265472412\n",
      "Training-Batch No.2590\n",
      "Loss = 3.4663047790527344\n",
      "Training-Batch No.2600\n",
      "Loss = 3.8033127784729004\n",
      "Training-Batch No.2610\n",
      "Loss = 3.886303424835205\n",
      "Training-Batch No.2620\n",
      "Loss = 3.9355673789978027\n",
      "Training-Batch No.2630\n",
      "Loss = 3.374321937561035\n",
      "Training-Batch No.2640\n",
      "Loss = 3.814594268798828\n",
      "Training-Batch No.2650\n",
      "Loss = 3.5123910903930664\n",
      "Training-Batch No.2660\n",
      "Loss = 3.6095705032348633\n",
      "Training-Batch No.2670\n",
      "Loss = 3.116255044937134\n",
      "Training-Batch No.2680\n",
      "Loss = 3.0770068168640137\n",
      "Training-Batch No.2690\n",
      "Loss = 3.781149387359619\n",
      "Training-Batch No.2700\n",
      "Loss = 3.069443702697754\n",
      "Training-Batch No.2710\n",
      "Loss = 2.969043254852295\n",
      "Training-Batch No.2720\n",
      "Loss = 2.883523464202881\n",
      "Training-Batch No.2730\n",
      "Loss = 3.617875814437866\n",
      "Training-Batch No.2740\n",
      "Loss = 3.4371256828308105\n",
      "Training-Batch No.2750\n",
      "Loss = 2.7735557556152344\n",
      "Training-Batch No.2760\n",
      "Loss = 3.0074305534362793\n",
      "Training-Batch No.2770\n",
      "Loss = 3.4961557388305664\n",
      "Training-Batch No.2780\n",
      "Loss = 3.212369441986084\n",
      "Training-Batch No.2790\n",
      "Loss = 3.401350498199463\n",
      "Training-Batch No.2800\n",
      "Loss = 3.6602888107299805\n",
      "Training-Batch No.2810\n",
      "Loss = 5.7678117752075195\n",
      "Training-Batch No.2820\n",
      "Loss = 3.3275294303894043\n",
      "Training-Batch No.2830\n",
      "Loss = 3.7844691276550293\n",
      "Training-Batch No.2840\n",
      "Loss = 3.431222915649414\n",
      "Training-Batch No.2850\n",
      "Loss = 4.322292327880859\n",
      "Training-Batch No.2860\n",
      "Loss = 3.6063287258148193\n",
      "Training-Batch No.2870\n",
      "Loss = 3.57769513130188\n",
      "Training-Batch No.2880\n",
      "Loss = 5.014608383178711\n",
      "Training-Batch No.2890\n",
      "Loss = 3.9849038124084473\n",
      "Training-Batch No.2900\n",
      "Loss = 3.2804200649261475\n",
      "Training-Batch No.2910\n",
      "Loss = 2.888108015060425\n",
      "Training-Batch No.2920\n",
      "Loss = 3.6611833572387695\n",
      "Training-Batch No.2930\n",
      "Loss = 3.5668296813964844\n",
      "Training-Batch No.2940\n",
      "Loss = 2.906879425048828\n",
      "Training-Batch No.2950\n",
      "Loss = 3.9010581970214844\n",
      "Training-Batch No.2960\n",
      "Loss = 3.088888168334961\n",
      "Training-Batch No.2970\n",
      "Loss = 3.8082354068756104\n",
      "Training-Batch No.2980\n",
      "Loss = 3.052177667617798\n",
      "Training-Batch No.2990\n",
      "Loss = 2.8078339099884033\n",
      "Training-Batch No.3000\n",
      "Loss = 3.631077527999878\n",
      "Training-Batch No.3010\n",
      "Loss = 2.820225954055786\n",
      "Training-Batch No.3020\n",
      "Loss = 2.9509854316711426\n",
      "Training-Batch No.3030\n",
      "Loss = 3.2686285972595215\n",
      "Training-Batch No.3040\n",
      "Loss = 2.992215394973755\n",
      "Training-Batch No.3050\n",
      "Loss = 3.4908270835876465\n",
      "Training-Batch No.3060\n",
      "Loss = 3.2264435291290283\n",
      "Training-Batch No.3070\n",
      "Loss = 3.395552635192871\n",
      "Training-Batch No.3080\n",
      "Loss = 3.2235991954803467\n",
      "Training-Batch No.3090\n",
      "Loss = 2.7962677478790283\n",
      "Training-Batch No.3100\n",
      "Loss = 3.2167911529541016\n",
      "Training-Batch No.3110\n",
      "Loss = 2.8468680381774902\n",
      "Training-Batch No.3120\n",
      "Loss = 4.063081741333008\n",
      "Training-Batch No.3130\n",
      "Loss = 3.2306578159332275\n",
      "Training-Batch No.3140\n",
      "Loss = 3.4631190299987793\n",
      "Training-Batch No.3150\n",
      "Loss = 4.104107856750488\n",
      "Epoch 3 Training Loss: 3.5456\n",
      "Start validation\n",
      "Epoch 3 Validation Loss: 4.3183\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.03756538278649548\n",
      "Average Top-3 accuracy 0.15596766524013314\n",
      "Average Top-5 accuracy 0.3338088445078459\n",
      "Average Top-7 accuracy 0.3723252496433666\n",
      "Average Top-9 accuracy 0.4921540656205421\n",
      "Average Top-11 accuracy 0.555397051830718\n",
      "Average Top-13 accuracy 0.6134094151212554\n",
      "Average Top-15 accuracy 0.6557299096528768\n",
      "current acc 0.03756538278649548\n",
      "best acc 0.060865430337612936\n",
      "updated best accuracy 0.060865430337612936\n",
      "Epoch No. 4\n",
      "Training-Batch No.3160\n",
      "Loss = 3.4660305976867676\n",
      "Training-Batch No.3170\n",
      "Loss = 3.194756031036377\n",
      "Training-Batch No.3180\n",
      "Loss = 3.6987853050231934\n",
      "Training-Batch No.3190\n",
      "Loss = 4.00549840927124\n",
      "Training-Batch No.3200\n",
      "Loss = 3.880218982696533\n",
      "Training-Batch No.3210\n",
      "Loss = 3.373241662979126\n",
      "Training-Batch No.3220\n",
      "Loss = 3.1023130416870117\n",
      "Training-Batch No.3230\n",
      "Loss = 4.6223344802856445\n",
      "Training-Batch No.3240\n",
      "Loss = 3.743699550628662\n",
      "Training-Batch No.3250\n",
      "Loss = 3.2614452838897705\n",
      "Training-Batch No.3260\n",
      "Loss = 3.0697011947631836\n",
      "Training-Batch No.3270\n",
      "Loss = 3.1480674743652344\n",
      "Training-Batch No.3280\n",
      "Loss = 3.020995855331421\n",
      "Training-Batch No.3290\n",
      "Loss = 5.467573165893555\n",
      "Training-Batch No.3300\n",
      "Loss = 3.0271804332733154\n",
      "Training-Batch No.3310\n",
      "Loss = 2.905712604522705\n",
      "Training-Batch No.3320\n",
      "Loss = 4.784801483154297\n",
      "Training-Batch No.3330\n",
      "Loss = 3.62954044342041\n",
      "Training-Batch No.3340\n",
      "Loss = 4.099783897399902\n",
      "Training-Batch No.3350\n",
      "Loss = 4.2004194259643555\n",
      "Training-Batch No.3360\n",
      "Loss = 3.0484867095947266\n",
      "Training-Batch No.3370\n",
      "Loss = 3.619689464569092\n",
      "Training-Batch No.3380\n",
      "Loss = 3.560054063796997\n",
      "Training-Batch No.3390\n",
      "Loss = 3.0537006855010986\n",
      "Training-Batch No.3400\n",
      "Loss = 3.0291452407836914\n",
      "Training-Batch No.3410\n",
      "Loss = 3.201749324798584\n",
      "Training-Batch No.3420\n",
      "Loss = 2.92665433883667\n",
      "Training-Batch No.3430\n",
      "Loss = 3.666670799255371\n",
      "Training-Batch No.3440\n",
      "Loss = 4.313659191131592\n",
      "Training-Batch No.3450\n",
      "Loss = 4.035284996032715\n",
      "Training-Batch No.3460\n",
      "Loss = 3.4100773334503174\n",
      "Training-Batch No.3470\n",
      "Loss = 3.36268949508667\n",
      "Training-Batch No.3480\n",
      "Loss = 3.522571086883545\n",
      "Training-Batch No.3490\n",
      "Loss = 2.820040702819824\n",
      "Training-Batch No.3500\n",
      "Loss = 3.4300312995910645\n",
      "Training-Batch No.3510\n",
      "Loss = 3.0088558197021484\n",
      "Training-Batch No.3520\n",
      "Loss = 3.2499213218688965\n",
      "Training-Batch No.3530\n",
      "Loss = 3.0991833209991455\n",
      "Training-Batch No.3540\n",
      "Loss = 4.1617913246154785\n",
      "Training-Batch No.3550\n",
      "Loss = 3.024099826812744\n",
      "Training-Batch No.3560\n",
      "Loss = 2.736171245574951\n",
      "Training-Batch No.3570\n",
      "Loss = 3.3344435691833496\n",
      "Training-Batch No.3580\n",
      "Loss = 3.6796417236328125\n",
      "Training-Batch No.3590\n",
      "Loss = 3.001642942428589\n",
      "Training-Batch No.3600\n",
      "Loss = 2.369597911834717\n",
      "Training-Batch No.3610\n",
      "Loss = 3.7863128185272217\n",
      "Training-Batch No.3620\n",
      "Loss = 3.308006525039673\n",
      "Training-Batch No.3630\n",
      "Loss = 3.812814712524414\n",
      "Training-Batch No.3640\n",
      "Loss = 2.387401580810547\n",
      "Training-Batch No.3650\n",
      "Loss = 3.2781012058258057\n",
      "Training-Batch No.3660\n",
      "Loss = 3.2144880294799805\n",
      "Training-Batch No.3670\n",
      "Loss = 3.142615795135498\n",
      "Training-Batch No.3680\n",
      "Loss = 3.3349173069000244\n",
      "Training-Batch No.3690\n",
      "Loss = 2.702176094055176\n",
      "Training-Batch No.3700\n",
      "Loss = 2.6593620777130127\n",
      "Training-Batch No.3710\n",
      "Loss = 3.77335786819458\n",
      "Training-Batch No.3720\n",
      "Loss = 3.232905387878418\n",
      "Training-Batch No.3730\n",
      "Loss = 5.750027179718018\n",
      "Training-Batch No.3740\n",
      "Loss = 2.5609309673309326\n",
      "Training-Batch No.3750\n",
      "Loss = 3.6862785816192627\n",
      "Training-Batch No.3760\n",
      "Loss = 2.942978858947754\n",
      "Training-Batch No.3770\n",
      "Loss = 2.853362560272217\n",
      "Training-Batch No.3780\n",
      "Loss = 3.951681613922119\n",
      "Training-Batch No.3790\n",
      "Loss = 3.5782251358032227\n",
      "Training-Batch No.3800\n",
      "Loss = 3.6040186882019043\n",
      "Training-Batch No.3810\n",
      "Loss = 3.5670547485351562\n",
      "Training-Batch No.3820\n",
      "Loss = 3.616396903991699\n",
      "Training-Batch No.3830\n",
      "Loss = 3.0863125324249268\n",
      "Training-Batch No.3840\n",
      "Loss = 3.245025157928467\n",
      "Training-Batch No.3850\n",
      "Loss = 5.167029857635498\n",
      "Training-Batch No.3860\n",
      "Loss = 2.7343480587005615\n",
      "Training-Batch No.3870\n",
      "Loss = 3.1249680519104004\n",
      "Training-Batch No.3880\n",
      "Loss = 2.849348545074463\n",
      "Training-Batch No.3890\n",
      "Loss = 2.693120241165161\n",
      "Training-Batch No.3900\n",
      "Loss = 2.95184588432312\n",
      "Training-Batch No.3910\n",
      "Loss = 3.3400354385375977\n",
      "Training-Batch No.3920\n",
      "Loss = 3.275728702545166\n",
      "Training-Batch No.3930\n",
      "Loss = 2.6172003746032715\n",
      "Training-Batch No.3940\n",
      "Loss = 2.7969093322753906\n",
      "Training-Batch No.3950\n",
      "Loss = 2.851719379425049\n",
      "Training-Batch No.3960\n",
      "Loss = 3.0461971759796143\n",
      "Training-Batch No.3970\n",
      "Loss = 3.382709503173828\n",
      "Training-Batch No.3980\n",
      "Loss = 2.7318217754364014\n",
      "Training-Batch No.3990\n",
      "Loss = 2.7250406742095947\n",
      "Training-Batch No.4000\n",
      "Loss = 3.303755044937134\n",
      "Training-Batch No.4010\n",
      "Loss = 2.1462864875793457\n",
      "Training-Batch No.4020\n",
      "Loss = 3.5687241554260254\n",
      "Training-Batch No.4030\n",
      "Loss = 4.585230350494385\n",
      "Training-Batch No.4040\n",
      "Loss = 3.2388408184051514\n",
      "Training-Batch No.4050\n",
      "Loss = 3.282505989074707\n",
      "Training-Batch No.4060\n",
      "Loss = 4.181632041931152\n",
      "Training-Batch No.4070\n",
      "Loss = 3.1479766368865967\n",
      "Training-Batch No.4080\n",
      "Loss = 4.10430908203125\n",
      "Training-Batch No.4090\n",
      "Loss = 3.203558921813965\n",
      "Training-Batch No.4100\n",
      "Loss = 3.536026954650879\n",
      "Training-Batch No.4110\n",
      "Loss = 2.7683663368225098\n",
      "Training-Batch No.4120\n",
      "Loss = 4.0744733810424805\n",
      "Training-Batch No.4130\n",
      "Loss = 3.2085795402526855\n",
      "Training-Batch No.4140\n",
      "Loss = 3.2940211296081543\n",
      "Training-Batch No.4150\n",
      "Loss = 2.7063398361206055\n",
      "Training-Batch No.4160\n",
      "Loss = 4.775240898132324\n",
      "Training-Batch No.4170\n",
      "Loss = 3.180434465408325\n",
      "Training-Batch No.4180\n",
      "Loss = 3.2607340812683105\n",
      "Training-Batch No.4190\n",
      "Loss = 3.196878433227539\n",
      "Training-Batch No.4200\n",
      "Loss = 2.682006359100342\n",
      "Epoch 4 Training Loss: 3.3005\n",
      "Start validation\n",
      "Epoch 4 Validation Loss: 4.4027\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.0451735615786971\n",
      "Average Top-3 accuracy 0.1445553970518307\n",
      "Average Top-5 accuracy 0.21493105087969566\n",
      "Average Top-7 accuracy 0.2719923918212078\n",
      "Average Top-9 accuracy 0.3385639562529719\n",
      "Average Top-11 accuracy 0.4503090822634332\n",
      "Average Top-13 accuracy 0.5192582025677603\n",
      "Average Top-15 accuracy 0.553019495958155\n",
      "current acc 0.0451735615786971\n",
      "best acc 0.060865430337612936\n",
      "updated best accuracy 0.060865430337612936\n",
      "Epoch No. 5\n",
      "Training-Batch No.4210\n",
      "Loss = 2.643690586090088\n",
      "Training-Batch No.4220\n",
      "Loss = 3.9318456649780273\n",
      "Training-Batch No.4230\n",
      "Loss = 2.6332380771636963\n",
      "Training-Batch No.4240\n",
      "Loss = 6.45037841796875\n",
      "Training-Batch No.4250\n",
      "Loss = 2.788694143295288\n",
      "Training-Batch No.4260\n",
      "Loss = 2.9952163696289062\n",
      "Training-Batch No.4270\n",
      "Loss = 2.8050668239593506\n",
      "Training-Batch No.4280\n",
      "Loss = 2.3970232009887695\n",
      "Training-Batch No.4290\n",
      "Loss = 3.600128650665283\n",
      "Training-Batch No.4300\n",
      "Loss = 3.7252211570739746\n",
      "Training-Batch No.4310\n",
      "Loss = 3.144204616546631\n",
      "Training-Batch No.4320\n",
      "Loss = 3.257053852081299\n",
      "Training-Batch No.4330\n",
      "Loss = 2.908496618270874\n",
      "Training-Batch No.4340\n",
      "Loss = 3.1297645568847656\n",
      "Training-Batch No.4350\n",
      "Loss = 3.8421108722686768\n",
      "Training-Batch No.4360\n",
      "Loss = 2.589670181274414\n",
      "Training-Batch No.4370\n",
      "Loss = 3.2239341735839844\n",
      "Training-Batch No.4380\n",
      "Loss = 2.801966905593872\n",
      "Training-Batch No.4390\n",
      "Loss = 2.260887622833252\n",
      "Training-Batch No.4400\n",
      "Loss = 3.0574989318847656\n",
      "Training-Batch No.4410\n",
      "Loss = 3.648447036743164\n",
      "Training-Batch No.4420\n",
      "Loss = 4.091031074523926\n",
      "Training-Batch No.4430\n",
      "Loss = 3.545844554901123\n",
      "Training-Batch No.4440\n",
      "Loss = 2.447824239730835\n",
      "Training-Batch No.4450\n",
      "Loss = 3.010932445526123\n",
      "Training-Batch No.4460\n",
      "Loss = 2.5666379928588867\n",
      "Training-Batch No.4470\n",
      "Loss = 3.905369997024536\n",
      "Training-Batch No.4480\n",
      "Loss = 7.814163684844971\n",
      "Training-Batch No.4490\n",
      "Loss = 3.042649745941162\n",
      "Training-Batch No.4500\n",
      "Loss = 2.52449893951416\n",
      "Training-Batch No.4510\n",
      "Loss = 3.7010159492492676\n",
      "Training-Batch No.4520\n",
      "Loss = 3.10137939453125\n",
      "Training-Batch No.4530\n",
      "Loss = 3.061807155609131\n",
      "Training-Batch No.4540\n",
      "Loss = 2.7708983421325684\n",
      "Training-Batch No.4550\n",
      "Loss = 3.416656494140625\n",
      "Training-Batch No.4560\n",
      "Loss = 3.6030962467193604\n",
      "Training-Batch No.4570\n",
      "Loss = 2.538231611251831\n",
      "Training-Batch No.4580\n",
      "Loss = 3.727173328399658\n",
      "Training-Batch No.4590\n",
      "Loss = 3.5383081436157227\n",
      "Training-Batch No.4600\n",
      "Loss = 3.016183376312256\n",
      "Training-Batch No.4610\n",
      "Loss = 4.27847146987915\n",
      "Training-Batch No.4620\n",
      "Loss = 3.2875754833221436\n",
      "Training-Batch No.4630\n",
      "Loss = 2.7169229984283447\n",
      "Training-Batch No.4640\n",
      "Loss = 3.740250587463379\n",
      "Training-Batch No.4650\n",
      "Loss = 3.3768320083618164\n",
      "Training-Batch No.4660\n",
      "Loss = 2.90130615234375\n",
      "Training-Batch No.4670\n",
      "Loss = 3.830601215362549\n",
      "Training-Batch No.4680\n",
      "Loss = 2.6291344165802\n",
      "Training-Batch No.4690\n",
      "Loss = 3.8125557899475098\n",
      "Training-Batch No.4700\n",
      "Loss = 2.542196273803711\n",
      "Training-Batch No.4710\n",
      "Loss = 3.5728321075439453\n",
      "Training-Batch No.4720\n",
      "Loss = 2.939438581466675\n",
      "Training-Batch No.4730\n",
      "Loss = 2.301828384399414\n",
      "Training-Batch No.4740\n",
      "Loss = 3.6833994388580322\n",
      "Training-Batch No.4750\n",
      "Loss = 3.1995766162872314\n",
      "Training-Batch No.4760\n",
      "Loss = 2.0436902046203613\n",
      "Training-Batch No.4770\n",
      "Loss = 3.243793487548828\n",
      "Training-Batch No.4780\n",
      "Loss = 3.021627902984619\n",
      "Training-Batch No.4790\n",
      "Loss = 2.3240416049957275\n",
      "Training-Batch No.4800\n",
      "Loss = 3.155636787414551\n",
      "Training-Batch No.4810\n",
      "Loss = 2.6018142700195312\n",
      "Training-Batch No.4820\n",
      "Loss = 3.044015407562256\n",
      "Training-Batch No.4830\n",
      "Loss = 2.7806105613708496\n",
      "Training-Batch No.4840\n",
      "Loss = 2.8613333702087402\n",
      "Training-Batch No.4850\n",
      "Loss = 2.890172004699707\n",
      "Training-Batch No.4860\n",
      "Loss = 4.516713619232178\n",
      "Training-Batch No.4870\n",
      "Loss = 2.895031452178955\n",
      "Training-Batch No.4880\n",
      "Loss = 3.521507740020752\n",
      "Training-Batch No.4890\n",
      "Loss = 3.0954480171203613\n",
      "Training-Batch No.4900\n",
      "Loss = 2.26652455329895\n",
      "Training-Batch No.4910\n",
      "Loss = 2.595022439956665\n",
      "Training-Batch No.4920\n",
      "Loss = 3.4216909408569336\n",
      "Training-Batch No.4930\n",
      "Loss = 4.092785358428955\n",
      "Training-Batch No.4940\n",
      "Loss = 2.4504590034484863\n",
      "Training-Batch No.4950\n",
      "Loss = 2.4972915649414062\n",
      "Training-Batch No.4960\n",
      "Loss = 2.6182615756988525\n",
      "Training-Batch No.4970\n",
      "Loss = 2.1765336990356445\n",
      "Training-Batch No.4980\n",
      "Loss = 2.1388964653015137\n",
      "Training-Batch No.4990\n",
      "Loss = 3.8882153034210205\n",
      "Training-Batch No.5000\n",
      "Loss = 2.8131165504455566\n",
      "Training-Batch No.5010\n",
      "Loss = 3.0333969593048096\n",
      "Training-Batch No.5020\n",
      "Loss = 2.9609668254852295\n",
      "Training-Batch No.5030\n",
      "Loss = 2.6642682552337646\n",
      "Training-Batch No.5040\n",
      "Loss = 2.3342485427856445\n",
      "Training-Batch No.5050\n",
      "Loss = 3.153916358947754\n",
      "Training-Batch No.5060\n",
      "Loss = 4.521456718444824\n",
      "Training-Batch No.5070\n",
      "Loss = 2.3215718269348145\n",
      "Training-Batch No.5080\n",
      "Loss = 2.599560022354126\n",
      "Training-Batch No.5090\n",
      "Loss = 2.56699800491333\n",
      "Training-Batch No.5100\n",
      "Loss = 3.1715192794799805\n",
      "Training-Batch No.5110\n",
      "Loss = 2.759047746658325\n",
      "Training-Batch No.5120\n",
      "Loss = 2.0720927715301514\n",
      "Training-Batch No.5130\n",
      "Loss = 3.4259450435638428\n",
      "Training-Batch No.5140\n",
      "Loss = 2.799379587173462\n",
      "Training-Batch No.5150\n",
      "Loss = 1.9916603565216064\n",
      "Training-Batch No.5160\n",
      "Loss = 2.712527275085449\n",
      "Training-Batch No.5170\n",
      "Loss = 3.242440700531006\n",
      "Training-Batch No.5180\n",
      "Loss = 2.634681463241577\n",
      "Training-Batch No.5190\n",
      "Loss = 2.7905983924865723\n",
      "Training-Batch No.5200\n",
      "Loss = 2.132709264755249\n",
      "Training-Batch No.5210\n",
      "Loss = 2.3101136684417725\n",
      "Training-Batch No.5220\n",
      "Loss = 2.64363431930542\n",
      "Training-Batch No.5230\n",
      "Loss = 3.1560707092285156\n",
      "Training-Batch No.5240\n",
      "Loss = 2.741366386413574\n",
      "Training-Batch No.5250\n",
      "Loss = 1.834625482559204\n",
      "Training-Batch No.5260\n",
      "Loss = 2.392810821533203\n",
      "Epoch 5 Training Loss: 3.0013\n",
      "Start validation\n",
      "Epoch 5 Validation Loss: 3.2191\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.09272467902995721\n",
      "Average Top-3 accuracy 0.27722301474084643\n",
      "Average Top-5 accuracy 0.442225392296719\n",
      "Average Top-7 accuracy 0.5701378982406087\n",
      "Average Top-9 accuracy 0.6666666666666666\n",
      "Average Top-11 accuracy 0.7494056110318592\n",
      "Average Top-13 accuracy 0.8107465525439848\n",
      "Average Top-15 accuracy 0.8464098906324299\n",
      "current acc 0.09272467902995721\n",
      "best acc 0.060865430337612936\n",
      "Saving the best model\n",
      "updated best accuracy 0.09272467902995721\n",
      "Epoch No. 6\n",
      "Training-Batch No.5270\n",
      "Loss = 4.8310394287109375\n",
      "Training-Batch No.5280\n",
      "Loss = 3.3085150718688965\n",
      "Training-Batch No.5290\n",
      "Loss = 2.803852081298828\n",
      "Training-Batch No.5300\n",
      "Loss = 1.6692028045654297\n",
      "Training-Batch No.5310\n",
      "Loss = 2.272930860519409\n",
      "Training-Batch No.5320\n",
      "Loss = 2.716768264770508\n",
      "Training-Batch No.5330\n",
      "Loss = 1.9318360090255737\n",
      "Training-Batch No.5340\n",
      "Loss = 2.4654924869537354\n",
      "Training-Batch No.5350\n",
      "Loss = 2.8927338123321533\n",
      "Training-Batch No.5360\n",
      "Loss = 2.571150779724121\n",
      "Training-Batch No.5370\n",
      "Loss = 2.683025598526001\n",
      "Training-Batch No.5380\n",
      "Loss = 4.336456298828125\n",
      "Training-Batch No.5390\n",
      "Loss = 2.2135848999023438\n",
      "Training-Batch No.5400\n",
      "Loss = 2.79571533203125\n",
      "Training-Batch No.5410\n",
      "Loss = 2.8202881813049316\n",
      "Training-Batch No.5420\n",
      "Loss = 2.716046094894409\n",
      "Training-Batch No.5430\n",
      "Loss = 2.355998992919922\n",
      "Training-Batch No.5440\n",
      "Loss = 3.162870168685913\n",
      "Training-Batch No.5450\n",
      "Loss = 2.4396138191223145\n",
      "Training-Batch No.5460\n",
      "Loss = 4.409642696380615\n",
      "Training-Batch No.5470\n",
      "Loss = 2.2428574562072754\n",
      "Training-Batch No.5480\n",
      "Loss = 2.6465203762054443\n",
      "Training-Batch No.5490\n",
      "Loss = 2.331045627593994\n",
      "Training-Batch No.5500\n",
      "Loss = 2.9860033988952637\n",
      "Training-Batch No.5510\n",
      "Loss = 2.471653461456299\n",
      "Training-Batch No.5520\n",
      "Loss = 2.626776933670044\n",
      "Training-Batch No.5530\n",
      "Loss = 3.2431015968322754\n",
      "Training-Batch No.5540\n",
      "Loss = 3.474886417388916\n",
      "Training-Batch No.5550\n",
      "Loss = 2.948237180709839\n",
      "Training-Batch No.5560\n",
      "Loss = 2.516777515411377\n",
      "Training-Batch No.5570\n",
      "Loss = 2.9438772201538086\n",
      "Training-Batch No.5580\n",
      "Loss = 2.8967080116271973\n",
      "Training-Batch No.5590\n",
      "Loss = 2.8994579315185547\n",
      "Training-Batch No.5600\n",
      "Loss = 3.4660987854003906\n",
      "Training-Batch No.5610\n",
      "Loss = 2.384613513946533\n",
      "Training-Batch No.5620\n",
      "Loss = 2.1697170734405518\n",
      "Training-Batch No.5630\n",
      "Loss = 3.5879719257354736\n",
      "Training-Batch No.5640\n",
      "Loss = 2.3911898136138916\n",
      "Training-Batch No.5650\n",
      "Loss = 2.4182960987091064\n",
      "Training-Batch No.5660\n",
      "Loss = 2.7004165649414062\n",
      "Training-Batch No.5670\n",
      "Loss = 2.5209507942199707\n",
      "Training-Batch No.5680\n",
      "Loss = 2.0678930282592773\n",
      "Training-Batch No.5690\n",
      "Loss = 2.486281394958496\n",
      "Training-Batch No.5700\n",
      "Loss = 4.138375282287598\n",
      "Training-Batch No.5710\n",
      "Loss = 2.150449752807617\n",
      "Training-Batch No.5720\n",
      "Loss = 1.7482635974884033\n",
      "Training-Batch No.5730\n",
      "Loss = 2.8987302780151367\n",
      "Training-Batch No.5740\n",
      "Loss = 2.8137269020080566\n",
      "Training-Batch No.5750\n",
      "Loss = 3.217005729675293\n",
      "Training-Batch No.5760\n",
      "Loss = 2.050358772277832\n",
      "Training-Batch No.5770\n",
      "Loss = 2.408449649810791\n",
      "Training-Batch No.5780\n",
      "Loss = 2.1772964000701904\n",
      "Training-Batch No.5790\n",
      "Loss = 2.6742420196533203\n",
      "Training-Batch No.5800\n",
      "Loss = 2.5242886543273926\n",
      "Training-Batch No.5810\n",
      "Loss = 2.688760280609131\n",
      "Training-Batch No.5820\n",
      "Loss = 2.3419253826141357\n",
      "Training-Batch No.5830\n",
      "Loss = 3.5161495208740234\n",
      "Training-Batch No.5840\n",
      "Loss = 2.2747368812561035\n",
      "Training-Batch No.5850\n",
      "Loss = 2.4770498275756836\n",
      "Training-Batch No.5860\n",
      "Loss = 2.3976545333862305\n",
      "Training-Batch No.5870\n",
      "Loss = 2.8313236236572266\n",
      "Training-Batch No.5880\n",
      "Loss = 2.1788229942321777\n",
      "Training-Batch No.5890\n",
      "Loss = 2.343923807144165\n",
      "Training-Batch No.5900\n",
      "Loss = 2.527390956878662\n",
      "Training-Batch No.5910\n",
      "Loss = 2.3958780765533447\n",
      "Training-Batch No.5920\n",
      "Loss = 2.001619338989258\n",
      "Training-Batch No.5930\n",
      "Loss = 2.6578118801116943\n",
      "Training-Batch No.5940\n",
      "Loss = 2.48593807220459\n",
      "Training-Batch No.5950\n",
      "Loss = 1.770350456237793\n",
      "Training-Batch No.5960\n",
      "Loss = 1.67970609664917\n",
      "Training-Batch No.5970\n",
      "Loss = 2.9301950931549072\n",
      "Training-Batch No.5980\n",
      "Loss = 3.0399765968322754\n",
      "Training-Batch No.5990\n",
      "Loss = 2.619579315185547\n",
      "Training-Batch No.6000\n",
      "Loss = 2.7939867973327637\n",
      "Training-Batch No.6010\n",
      "Loss = 3.3515028953552246\n",
      "Training-Batch No.6020\n",
      "Loss = 2.7753593921661377\n",
      "Training-Batch No.6030\n",
      "Loss = 3.2212090492248535\n",
      "Training-Batch No.6040\n",
      "Loss = 3.487509250640869\n",
      "Training-Batch No.6050\n",
      "Loss = 1.9201464653015137\n",
      "Training-Batch No.6060\n",
      "Loss = 2.8084545135498047\n",
      "Training-Batch No.6070\n",
      "Loss = 2.0641121864318848\n",
      "Training-Batch No.6080\n",
      "Loss = 2.2857048511505127\n",
      "Training-Batch No.6090\n",
      "Loss = 5.481296539306641\n",
      "Training-Batch No.6100\n",
      "Loss = 2.5672030448913574\n",
      "Training-Batch No.6110\n",
      "Loss = 2.8677358627319336\n",
      "Training-Batch No.6120\n",
      "Loss = 6.040012359619141\n",
      "Training-Batch No.6130\n",
      "Loss = 2.3348400592803955\n",
      "Training-Batch No.6140\n",
      "Loss = 1.9614992141723633\n",
      "Training-Batch No.6150\n",
      "Loss = 3.5560684204101562\n",
      "Training-Batch No.6160\n",
      "Loss = 2.115588665008545\n",
      "Training-Batch No.6170\n",
      "Loss = 2.398613214492798\n",
      "Training-Batch No.6180\n",
      "Loss = 2.544243335723877\n",
      "Training-Batch No.6190\n",
      "Loss = 2.0202622413635254\n",
      "Training-Batch No.6200\n",
      "Loss = 1.6489254236221313\n",
      "Training-Batch No.6210\n",
      "Loss = 2.1565864086151123\n",
      "Training-Batch No.6220\n",
      "Loss = 2.5788304805755615\n",
      "Training-Batch No.6230\n",
      "Loss = 2.364866018295288\n",
      "Training-Batch No.6240\n",
      "Loss = 1.9188294410705566\n",
      "Training-Batch No.6250\n",
      "Loss = 1.8662725687026978\n",
      "Training-Batch No.6260\n",
      "Loss = 1.7829831838607788\n",
      "Training-Batch No.6270\n",
      "Loss = 2.1175315380096436\n",
      "Training-Batch No.6280\n",
      "Loss = 2.6478331089019775\n",
      "Training-Batch No.6290\n",
      "Loss = 2.0256338119506836\n",
      "Training-Batch No.6300\n",
      "Loss = 2.3600969314575195\n",
      "Training-Batch No.6310\n",
      "Loss = 2.9600718021392822\n",
      "Epoch 6 Training Loss: 2.7268\n",
      "Start validation\n",
      "Epoch 6 Validation Loss: 3.4241\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.09605325725154541\n",
      "Average Top-3 accuracy 0.33475986685687115\n",
      "Average Top-5 accuracy 0.4845458868283405\n",
      "Average Top-7 accuracy 0.6034236804564908\n",
      "Average Top-9 accuracy 0.7051830718021873\n",
      "Average Top-11 accuracy 0.7560627674750356\n",
      "Average Top-13 accuracy 0.8055159296243461\n",
      "Average Top-15 accuracy 0.8435568235853542\n",
      "current acc 0.09605325725154541\n",
      "best acc 0.09272467902995721\n",
      "Saving the best model\n",
      "updated best accuracy 0.09605325725154541\n",
      "Epoch No. 7\n",
      "Training-Batch No.6320\n",
      "Loss = 2.496793270111084\n",
      "Training-Batch No.6330\n",
      "Loss = 1.9013862609863281\n",
      "Training-Batch No.6340\n",
      "Loss = 2.4034247398376465\n",
      "Training-Batch No.6350\n",
      "Loss = 1.7611875534057617\n",
      "Training-Batch No.6360\n",
      "Loss = 2.502218008041382\n",
      "Training-Batch No.6370\n",
      "Loss = 2.8492279052734375\n",
      "Training-Batch No.6380\n",
      "Loss = 2.025172710418701\n",
      "Training-Batch No.6390\n",
      "Loss = 4.071139335632324\n",
      "Training-Batch No.6400\n",
      "Loss = 1.6143550872802734\n",
      "Training-Batch No.6410\n",
      "Loss = 1.7079274654388428\n",
      "Training-Batch No.6420\n",
      "Loss = 6.777333736419678\n",
      "Training-Batch No.6430\n",
      "Loss = 1.950987696647644\n",
      "Training-Batch No.6440\n",
      "Loss = 1.9516280889511108\n",
      "Training-Batch No.6450\n",
      "Loss = 2.7310659885406494\n",
      "Training-Batch No.6460\n",
      "Loss = 2.1115617752075195\n",
      "Training-Batch No.6470\n",
      "Loss = 2.8720250129699707\n",
      "Training-Batch No.6480\n",
      "Loss = 2.4397122859954834\n",
      "Training-Batch No.6490\n",
      "Loss = 3.2501959800720215\n",
      "Training-Batch No.6500\n",
      "Loss = 3.8359274864196777\n",
      "Training-Batch No.6510\n",
      "Loss = 2.9181673526763916\n",
      "Training-Batch No.6520\n",
      "Loss = 2.6958529949188232\n",
      "Training-Batch No.6530\n",
      "Loss = 2.3970727920532227\n",
      "Training-Batch No.6540\n",
      "Loss = 2.4879705905914307\n",
      "Training-Batch No.6550\n",
      "Loss = 3.071593999862671\n",
      "Training-Batch No.6560\n",
      "Loss = 2.930631160736084\n",
      "Training-Batch No.6570\n",
      "Loss = 2.6147265434265137\n",
      "Training-Batch No.6580\n",
      "Loss = 1.9197216033935547\n",
      "Training-Batch No.6590\n",
      "Loss = 2.02048921585083\n",
      "Training-Batch No.6600\n",
      "Loss = 1.8108108043670654\n",
      "Training-Batch No.6610\n",
      "Loss = 2.1729655265808105\n",
      "Training-Batch No.6620\n",
      "Loss = 2.2544822692871094\n",
      "Training-Batch No.6630\n",
      "Loss = 2.1676905155181885\n",
      "Training-Batch No.6640\n",
      "Loss = 2.428680658340454\n",
      "Training-Batch No.6650\n",
      "Loss = 1.8912869691848755\n",
      "Training-Batch No.6660\n",
      "Loss = 2.3106837272644043\n",
      "Training-Batch No.6670\n",
      "Loss = 1.9055876731872559\n",
      "Training-Batch No.6680\n",
      "Loss = 1.973567008972168\n",
      "Training-Batch No.6690\n",
      "Loss = 10.714200019836426\n",
      "Training-Batch No.6700\n",
      "Loss = 2.7490053176879883\n",
      "Training-Batch No.6710\n",
      "Loss = 2.6681132316589355\n",
      "Training-Batch No.6720\n",
      "Loss = 2.783690929412842\n",
      "Training-Batch No.6730\n",
      "Loss = 2.4486312866210938\n",
      "Training-Batch No.6740\n",
      "Loss = 1.868236780166626\n",
      "Training-Batch No.6750\n",
      "Loss = 2.6216113567352295\n",
      "Training-Batch No.6760\n",
      "Loss = 2.4212281703948975\n",
      "Training-Batch No.6770\n",
      "Loss = 2.1197898387908936\n",
      "Training-Batch No.6780\n",
      "Loss = 3.090005874633789\n",
      "Training-Batch No.6790\n",
      "Loss = 1.7052717208862305\n",
      "Training-Batch No.6800\n",
      "Loss = 2.459237575531006\n",
      "Training-Batch No.6810\n",
      "Loss = 1.6153587102890015\n",
      "Training-Batch No.6820\n",
      "Loss = 1.6905853748321533\n",
      "Training-Batch No.6830\n",
      "Loss = 1.65559983253479\n",
      "Training-Batch No.6840\n",
      "Loss = 2.255523681640625\n",
      "Training-Batch No.6850\n",
      "Loss = 1.9663196802139282\n",
      "Training-Batch No.6860\n",
      "Loss = 2.453432559967041\n",
      "Training-Batch No.6870\n",
      "Loss = 4.011302471160889\n",
      "Training-Batch No.6880\n",
      "Loss = 6.473516464233398\n",
      "Training-Batch No.6890\n",
      "Loss = 2.0577168464660645\n",
      "Training-Batch No.6900\n",
      "Loss = 2.5092227458953857\n",
      "Training-Batch No.6910\n",
      "Loss = 2.657061815261841\n",
      "Training-Batch No.6920\n",
      "Loss = 2.5977511405944824\n",
      "Training-Batch No.6930\n",
      "Loss = 2.1129164695739746\n",
      "Training-Batch No.6940\n",
      "Loss = 2.0054962635040283\n",
      "Training-Batch No.6950\n",
      "Loss = 3.055058240890503\n",
      "Training-Batch No.6960\n",
      "Loss = 3.239013195037842\n",
      "Training-Batch No.6970\n",
      "Loss = 2.043912887573242\n",
      "Training-Batch No.6980\n",
      "Loss = 2.514003038406372\n",
      "Training-Batch No.6990\n",
      "Loss = 2.6929564476013184\n",
      "Training-Batch No.7000\n",
      "Loss = 2.880596160888672\n",
      "Training-Batch No.7010\n",
      "Loss = 1.8037875890731812\n",
      "Training-Batch No.7020\n",
      "Loss = 2.726227283477783\n",
      "Training-Batch No.7030\n",
      "Loss = 1.9087064266204834\n",
      "Training-Batch No.7040\n",
      "Loss = 2.6261284351348877\n",
      "Training-Batch No.7050\n",
      "Loss = 2.760531425476074\n",
      "Training-Batch No.7060\n",
      "Loss = 2.6074955463409424\n",
      "Training-Batch No.7070\n",
      "Loss = 2.298985481262207\n",
      "Training-Batch No.7080\n",
      "Loss = 1.5374813079833984\n",
      "Training-Batch No.7090\n",
      "Loss = 2.764389991760254\n",
      "Training-Batch No.7100\n",
      "Loss = 2.4773526191711426\n",
      "Training-Batch No.7110\n",
      "Loss = 2.752316474914551\n",
      "Training-Batch No.7120\n",
      "Loss = 2.001112222671509\n",
      "Training-Batch No.7130\n",
      "Loss = 2.007845878601074\n",
      "Training-Batch No.7140\n",
      "Loss = 2.5971832275390625\n",
      "Training-Batch No.7150\n",
      "Loss = 1.7486093044281006\n",
      "Training-Batch No.7160\n",
      "Loss = 3.0509867668151855\n",
      "Training-Batch No.7170\n",
      "Loss = 3.211866855621338\n",
      "Training-Batch No.7180\n",
      "Loss = 1.505143404006958\n",
      "Training-Batch No.7190\n",
      "Loss = 2.902278423309326\n",
      "Training-Batch No.7200\n",
      "Loss = 1.5204371213912964\n",
      "Training-Batch No.7210\n",
      "Loss = 2.475389242172241\n",
      "Training-Batch No.7220\n",
      "Loss = 2.201205015182495\n",
      "Training-Batch No.7230\n",
      "Loss = 2.509270191192627\n",
      "Training-Batch No.7240\n",
      "Loss = 2.3366899490356445\n",
      "Training-Batch No.7250\n",
      "Loss = 2.948650360107422\n",
      "Training-Batch No.7260\n",
      "Loss = 2.2357773780822754\n",
      "Training-Batch No.7270\n",
      "Loss = 1.765010118484497\n",
      "Training-Batch No.7280\n",
      "Loss = 2.2311341762542725\n",
      "Training-Batch No.7290\n",
      "Loss = 2.429631233215332\n",
      "Training-Batch No.7300\n",
      "Loss = 2.6142659187316895\n",
      "Training-Batch No.7310\n",
      "Loss = 2.1274752616882324\n",
      "Training-Batch No.7320\n",
      "Loss = 1.616056203842163\n",
      "Training-Batch No.7330\n",
      "Loss = 2.638296127319336\n",
      "Training-Batch No.7340\n",
      "Loss = 2.3094401359558105\n",
      "Training-Batch No.7350\n",
      "Loss = 2.1824045181274414\n",
      "Training-Batch No.7360\n",
      "Loss = 1.7308354377746582\n",
      "Epoch 7 Training Loss: 2.6065\n",
      "Start validation\n",
      "Epoch 7 Validation Loss: 3.5763\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.10033285782215882\n",
      "Average Top-3 accuracy 0.3119353304802663\n",
      "Average Top-5 accuracy 0.47218259629101283\n",
      "Average Top-7 accuracy 0.5287684260580123\n",
      "Average Top-9 accuracy 0.6252971944840704\n",
      "Average Top-11 accuracy 0.7018544935805991\n",
      "Average Top-13 accuracy 0.7337137422729434\n",
      "Average Top-15 accuracy 0.7541607227769852\n",
      "current acc 0.10033285782215882\n",
      "best acc 0.09605325725154541\n",
      "Saving the best model\n",
      "updated best accuracy 0.10033285782215882\n",
      "Epoch No. 8\n",
      "Training-Batch No.7370\n",
      "Loss = 2.720093250274658\n",
      "Training-Batch No.7380\n",
      "Loss = 2.3181660175323486\n",
      "Training-Batch No.7390\n",
      "Loss = 1.7323778867721558\n",
      "Training-Batch No.7400\n",
      "Loss = 2.241302967071533\n",
      "Training-Batch No.7410\n",
      "Loss = 2.0014448165893555\n",
      "Training-Batch No.7420\n",
      "Loss = 2.4565272331237793\n",
      "Training-Batch No.7430\n",
      "Loss = 4.268261432647705\n",
      "Training-Batch No.7440\n",
      "Loss = 1.5440826416015625\n",
      "Training-Batch No.7450\n",
      "Loss = 2.056554079055786\n",
      "Training-Batch No.7460\n",
      "Loss = 2.8955514430999756\n",
      "Training-Batch No.7470\n",
      "Loss = 2.467212677001953\n",
      "Training-Batch No.7480\n",
      "Loss = 1.6422579288482666\n",
      "Training-Batch No.7490\n",
      "Loss = 3.233447551727295\n",
      "Training-Batch No.7500\n",
      "Loss = 2.686856269836426\n",
      "Training-Batch No.7510\n",
      "Loss = 2.3167972564697266\n",
      "Training-Batch No.7520\n",
      "Loss = 2.0469586849212646\n",
      "Training-Batch No.7530\n",
      "Loss = 1.7621653079986572\n",
      "Training-Batch No.7540\n",
      "Loss = 2.1387906074523926\n",
      "Training-Batch No.7550\n",
      "Loss = 6.628873825073242\n",
      "Training-Batch No.7560\n",
      "Loss = 2.237205982208252\n",
      "Training-Batch No.7570\n",
      "Loss = 3.005833864212036\n",
      "Training-Batch No.7580\n",
      "Loss = 2.469364881515503\n",
      "Training-Batch No.7590\n",
      "Loss = 1.6856213808059692\n",
      "Training-Batch No.7600\n",
      "Loss = 2.1039061546325684\n",
      "Training-Batch No.7610\n",
      "Loss = 1.8020273447036743\n",
      "Training-Batch No.7620\n",
      "Loss = 2.2650225162506104\n",
      "Training-Batch No.7630\n",
      "Loss = 2.2077982425689697\n",
      "Training-Batch No.7640\n",
      "Loss = 2.033637285232544\n",
      "Training-Batch No.7650\n",
      "Loss = 1.8971638679504395\n",
      "Training-Batch No.7660\n",
      "Loss = 5.812124252319336\n",
      "Training-Batch No.7670\n",
      "Loss = 2.349952220916748\n",
      "Training-Batch No.7680\n",
      "Loss = 2.3817901611328125\n",
      "Training-Batch No.7690\n",
      "Loss = 2.2820241451263428\n",
      "Training-Batch No.7700\n",
      "Loss = 2.4948506355285645\n",
      "Training-Batch No.7710\n",
      "Loss = 2.9648795127868652\n",
      "Training-Batch No.7720\n",
      "Loss = 2.2359488010406494\n",
      "Training-Batch No.7730\n",
      "Loss = 2.289666175842285\n",
      "Training-Batch No.7740\n",
      "Loss = 2.9574506282806396\n",
      "Training-Batch No.7750\n",
      "Loss = 2.8961853981018066\n",
      "Training-Batch No.7760\n",
      "Loss = 6.294360160827637\n",
      "Training-Batch No.7770\n",
      "Loss = 2.628417730331421\n",
      "Training-Batch No.7780\n",
      "Loss = 2.1519510746002197\n",
      "Training-Batch No.7790\n",
      "Loss = 2.39241361618042\n",
      "Training-Batch No.7800\n",
      "Loss = 1.9973021745681763\n",
      "Training-Batch No.7810\n",
      "Loss = 1.982298731803894\n",
      "Training-Batch No.7820\n",
      "Loss = 2.53924560546875\n",
      "Training-Batch No.7830\n",
      "Loss = 1.9182871580123901\n",
      "Training-Batch No.7840\n",
      "Loss = 2.251674175262451\n",
      "Training-Batch No.7850\n",
      "Loss = 1.831275224685669\n",
      "Training-Batch No.7860\n",
      "Loss = 1.9942444562911987\n",
      "Training-Batch No.7870\n",
      "Loss = 2.345545768737793\n",
      "Training-Batch No.7880\n",
      "Loss = 2.533966541290283\n",
      "Training-Batch No.7890\n",
      "Loss = 1.903512954711914\n",
      "Training-Batch No.7900\n",
      "Loss = 2.620166778564453\n",
      "Training-Batch No.7910\n",
      "Loss = 1.6165387630462646\n",
      "Training-Batch No.7920\n",
      "Loss = 2.019235134124756\n",
      "Training-Batch No.7930\n",
      "Loss = 1.852522611618042\n",
      "Training-Batch No.7940\n",
      "Loss = 1.673560619354248\n",
      "Training-Batch No.7950\n",
      "Loss = 2.005312919616699\n",
      "Training-Batch No.7960\n",
      "Loss = 1.820119023323059\n",
      "Training-Batch No.7970\n",
      "Loss = 2.058215618133545\n",
      "Training-Batch No.7980\n",
      "Loss = 1.6063263416290283\n",
      "Training-Batch No.7990\n",
      "Loss = 2.2506823539733887\n",
      "Training-Batch No.8000\n",
      "Loss = 2.343921422958374\n",
      "Training-Batch No.8010\n",
      "Loss = 1.7442325353622437\n",
      "Training-Batch No.8020\n",
      "Loss = 1.7528544664382935\n",
      "Training-Batch No.8030\n",
      "Loss = 2.897491931915283\n",
      "Training-Batch No.8040\n",
      "Loss = 1.9782838821411133\n",
      "Training-Batch No.8050\n",
      "Loss = 2.004826545715332\n",
      "Training-Batch No.8060\n",
      "Loss = 2.5288374423980713\n",
      "Training-Batch No.8070\n",
      "Loss = 8.041903495788574\n",
      "Training-Batch No.8080\n",
      "Loss = 2.615180730819702\n",
      "Training-Batch No.8090\n",
      "Loss = 2.4479563236236572\n",
      "Training-Batch No.8100\n",
      "Loss = 1.6587222814559937\n",
      "Training-Batch No.8110\n",
      "Loss = 2.8943374156951904\n",
      "Training-Batch No.8120\n",
      "Loss = 2.503408670425415\n",
      "Training-Batch No.8130\n",
      "Loss = 2.5857815742492676\n",
      "Training-Batch No.8140\n",
      "Loss = 3.772822380065918\n",
      "Training-Batch No.8150\n",
      "Loss = 2.3781590461730957\n",
      "Training-Batch No.8160\n",
      "Loss = 1.7589393854141235\n",
      "Training-Batch No.8170\n",
      "Loss = 1.7914808988571167\n",
      "Training-Batch No.8180\n",
      "Loss = 2.9856643676757812\n",
      "Training-Batch No.8190\n",
      "Loss = 2.3550243377685547\n",
      "Training-Batch No.8200\n",
      "Loss = 2.1168830394744873\n",
      "Training-Batch No.8210\n",
      "Loss = 2.8091819286346436\n",
      "Training-Batch No.8220\n",
      "Loss = 2.2957794666290283\n",
      "Training-Batch No.8230\n",
      "Loss = 4.6819281578063965\n",
      "Training-Batch No.8240\n",
      "Loss = 1.7482471466064453\n",
      "Training-Batch No.8250\n",
      "Loss = 1.5861115455627441\n",
      "Training-Batch No.8260\n",
      "Loss = 2.655287504196167\n",
      "Training-Batch No.8270\n",
      "Loss = 1.938058853149414\n",
      "Training-Batch No.8280\n",
      "Loss = 2.197409152984619\n",
      "Training-Batch No.8290\n",
      "Loss = 2.045208692550659\n",
      "Training-Batch No.8300\n",
      "Loss = 1.9037970304489136\n",
      "Training-Batch No.8310\n",
      "Loss = 2.6225037574768066\n",
      "Training-Batch No.8320\n",
      "Loss = 1.4503616094589233\n",
      "Training-Batch No.8330\n",
      "Loss = 2.52231764793396\n",
      "Training-Batch No.8340\n",
      "Loss = 2.6689581871032715\n",
      "Training-Batch No.8350\n",
      "Loss = 1.8169119358062744\n",
      "Training-Batch No.8360\n",
      "Loss = 1.9896751642227173\n",
      "Training-Batch No.8370\n",
      "Loss = 2.1510677337646484\n",
      "Training-Batch No.8380\n",
      "Loss = 2.6300883293151855\n",
      "Training-Batch No.8390\n",
      "Loss = 2.0555288791656494\n",
      "Training-Batch No.8400\n",
      "Loss = 2.34067964553833\n",
      "Training-Batch No.8410\n",
      "Loss = 2.920207977294922\n",
      "Epoch 8 Training Loss: 2.5372\n",
      "Start validation\n",
      "Epoch 8 Validation Loss: 2.5889\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.3176414645744175\n",
      "Average Top-3 accuracy 0.5810746552543985\n",
      "Average Top-5 accuracy 0.7494056110318592\n",
      "Average Top-7 accuracy 0.8193057536852116\n",
      "Average Top-9 accuracy 0.854493580599144\n",
      "Average Top-11 accuracy 0.8877793628150261\n",
      "Average Top-13 accuracy 0.9139324774132193\n",
      "Average Top-15 accuracy 0.936757013789824\n",
      "current acc 0.3176414645744175\n",
      "best acc 0.10033285782215882\n",
      "Saving the best model\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 9\n",
      "Training-Batch No.8420\n",
      "Loss = 2.3989756107330322\n",
      "Training-Batch No.8430\n",
      "Loss = 2.6620688438415527\n",
      "Training-Batch No.8440\n",
      "Loss = 2.2622010707855225\n",
      "Training-Batch No.8450\n",
      "Loss = 2.245988368988037\n",
      "Training-Batch No.8460\n",
      "Loss = 2.9224982261657715\n",
      "Training-Batch No.8470\n",
      "Loss = 2.6221818923950195\n",
      "Training-Batch No.8480\n",
      "Loss = 2.0422136783599854\n",
      "Training-Batch No.8490\n",
      "Loss = 3.333885908126831\n",
      "Training-Batch No.8500\n",
      "Loss = 2.68047833442688\n",
      "Training-Batch No.8510\n",
      "Loss = 2.409385919570923\n",
      "Training-Batch No.8520\n",
      "Loss = 2.199192523956299\n",
      "Training-Batch No.8530\n",
      "Loss = 1.898653268814087\n",
      "Training-Batch No.8540\n",
      "Loss = 2.100767135620117\n",
      "Training-Batch No.8550\n",
      "Loss = 5.409090042114258\n",
      "Training-Batch No.8560\n",
      "Loss = 2.1433560848236084\n",
      "Training-Batch No.8570\n",
      "Loss = 2.0571908950805664\n",
      "Training-Batch No.8580\n",
      "Loss = 4.393132209777832\n",
      "Training-Batch No.8590\n",
      "Loss = 2.3176045417785645\n",
      "Training-Batch No.8600\n",
      "Loss = 2.632148504257202\n",
      "Training-Batch No.8610\n",
      "Loss = 4.006478309631348\n",
      "Training-Batch No.8620\n",
      "Loss = 2.762923002243042\n",
      "Training-Batch No.8630\n",
      "Loss = 2.634639263153076\n",
      "Training-Batch No.8640\n",
      "Loss = 2.9523215293884277\n",
      "Training-Batch No.8650\n",
      "Loss = 1.7078311443328857\n",
      "Training-Batch No.8660\n",
      "Loss = 2.198024272918701\n",
      "Training-Batch No.8670\n",
      "Loss = 1.9341081380844116\n",
      "Training-Batch No.8680\n",
      "Loss = 1.6580066680908203\n",
      "Training-Batch No.8690\n",
      "Loss = 2.8819189071655273\n",
      "Training-Batch No.8700\n",
      "Loss = 5.243202209472656\n",
      "Training-Batch No.8710\n",
      "Loss = 5.343193054199219\n",
      "Training-Batch No.8720\n",
      "Loss = 2.2522404193878174\n",
      "Training-Batch No.8730\n",
      "Loss = 2.7217702865600586\n",
      "Training-Batch No.8740\n",
      "Loss = 2.5222179889678955\n",
      "Training-Batch No.8750\n",
      "Loss = 1.8931448459625244\n",
      "Training-Batch No.8760\n",
      "Loss = 2.2631969451904297\n",
      "Training-Batch No.8770\n",
      "Loss = 2.38393497467041\n",
      "Training-Batch No.8780\n",
      "Loss = 2.295255184173584\n",
      "Training-Batch No.8790\n",
      "Loss = 2.010772228240967\n",
      "Training-Batch No.8800\n",
      "Loss = 3.1813952922821045\n",
      "Training-Batch No.8810\n",
      "Loss = 2.1376256942749023\n",
      "Training-Batch No.8820\n",
      "Loss = 2.434999704360962\n",
      "Training-Batch No.8830\n",
      "Loss = 2.1731276512145996\n",
      "Training-Batch No.8840\n",
      "Loss = 2.5378451347351074\n",
      "Training-Batch No.8850\n",
      "Loss = 1.9024040699005127\n",
      "Training-Batch No.8860\n",
      "Loss = 1.4252287149429321\n",
      "Training-Batch No.8870\n",
      "Loss = 2.4013571739196777\n",
      "Training-Batch No.8880\n",
      "Loss = 2.6972720623016357\n",
      "Training-Batch No.8890\n",
      "Loss = 2.6214656829833984\n",
      "Training-Batch No.8900\n",
      "Loss = 1.793845772743225\n",
      "Training-Batch No.8910\n",
      "Loss = 2.275646924972534\n",
      "Training-Batch No.8920\n",
      "Loss = 2.1677703857421875\n",
      "Training-Batch No.8930\n",
      "Loss = 2.34173846244812\n",
      "Training-Batch No.8940\n",
      "Loss = 2.5380446910858154\n",
      "Training-Batch No.8950\n",
      "Loss = 2.157559871673584\n",
      "Training-Batch No.8960\n",
      "Loss = 1.6558783054351807\n",
      "Training-Batch No.8970\n",
      "Loss = 5.876458168029785\n",
      "Training-Batch No.8980\n",
      "Loss = 3.099132537841797\n",
      "Training-Batch No.8990\n",
      "Loss = 6.35482931137085\n",
      "Training-Batch No.9000\n",
      "Loss = 1.6731480360031128\n",
      "Training-Batch No.9010\n",
      "Loss = 2.625187397003174\n",
      "Training-Batch No.9020\n",
      "Loss = 1.8595516681671143\n",
      "Training-Batch No.9030\n",
      "Loss = 1.960658311843872\n",
      "Training-Batch No.9040\n",
      "Loss = 3.1097512245178223\n",
      "Training-Batch No.9050\n",
      "Loss = 2.8966541290283203\n",
      "Training-Batch No.9060\n",
      "Loss = 2.148108959197998\n",
      "Training-Batch No.9070\n",
      "Loss = 2.673038959503174\n",
      "Training-Batch No.9080\n",
      "Loss = 2.795945644378662\n",
      "Training-Batch No.9090\n",
      "Loss = 2.50240421295166\n",
      "Training-Batch No.9100\n",
      "Loss = 2.3909354209899902\n",
      "Training-Batch No.9110\n",
      "Loss = 5.834033489227295\n",
      "Training-Batch No.9120\n",
      "Loss = 1.6642024517059326\n",
      "Training-Batch No.9130\n",
      "Loss = 1.8446913957595825\n",
      "Training-Batch No.9140\n",
      "Loss = 2.3323326110839844\n",
      "Training-Batch No.9150\n",
      "Loss = 2.8476686477661133\n",
      "Training-Batch No.9160\n",
      "Loss = 3.199629545211792\n",
      "Training-Batch No.9170\n",
      "Loss = 2.1086537837982178\n",
      "Training-Batch No.9180\n",
      "Loss = 2.0564208030700684\n",
      "Training-Batch No.9190\n",
      "Loss = 1.9809207916259766\n",
      "Training-Batch No.9200\n",
      "Loss = 1.8764140605926514\n",
      "Training-Batch No.9210\n",
      "Loss = 1.9161992073059082\n",
      "Training-Batch No.9220\n",
      "Loss = 2.621295928955078\n",
      "Training-Batch No.9230\n",
      "Loss = 2.4963035583496094\n",
      "Training-Batch No.9240\n",
      "Loss = 1.6903711557388306\n",
      "Training-Batch No.9250\n",
      "Loss = 1.811527967453003\n",
      "Training-Batch No.9260\n",
      "Loss = 2.013188123703003\n",
      "Training-Batch No.9270\n",
      "Loss = 1.473602533340454\n",
      "Training-Batch No.9280\n",
      "Loss = 2.4708428382873535\n",
      "Training-Batch No.9290\n",
      "Loss = 4.673727512359619\n",
      "Training-Batch No.9300\n",
      "Loss = 2.5498461723327637\n",
      "Training-Batch No.9310\n",
      "Loss = 2.432910919189453\n",
      "Training-Batch No.9320\n",
      "Loss = 3.0114264488220215\n",
      "Training-Batch No.9330\n",
      "Loss = 2.2184579372406006\n",
      "Training-Batch No.9340\n",
      "Loss = 4.515589237213135\n",
      "Training-Batch No.9350\n",
      "Loss = 2.259242057800293\n",
      "Training-Batch No.9360\n",
      "Loss = 3.0608229637145996\n",
      "Training-Batch No.9370\n",
      "Loss = 1.8354218006134033\n",
      "Training-Batch No.9380\n",
      "Loss = 2.703205108642578\n",
      "Training-Batch No.9390\n",
      "Loss = 1.325642466545105\n",
      "Training-Batch No.9400\n",
      "Loss = 2.2592430114746094\n",
      "Training-Batch No.9410\n",
      "Loss = 1.7906928062438965\n",
      "Training-Batch No.9420\n",
      "Loss = 3.132953405380249\n",
      "Training-Batch No.9430\n",
      "Loss = 2.2050418853759766\n",
      "Training-Batch No.9440\n",
      "Loss = 2.443221092224121\n",
      "Training-Batch No.9450\n",
      "Loss = 2.1625099182128906\n",
      "Training-Batch No.9460\n",
      "Loss = 1.8628041744232178\n",
      "Epoch 9 Training Loss: 2.4912\n",
      "Start validation\n",
      "Epoch 9 Validation Loss: 4.7420\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.020446980504041846\n",
      "Average Top-3 accuracy 0.06229196386115074\n",
      "Average Top-5 accuracy 0.11079410366143605\n",
      "Average Top-7 accuracy 0.17165953399904899\n",
      "Average Top-9 accuracy 0.2230147408464099\n",
      "Average Top-11 accuracy 0.3223965763195435\n",
      "Average Top-13 accuracy 0.4160722776985259\n",
      "Average Top-15 accuracy 0.4945316214931051\n",
      "current acc 0.020446980504041846\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 10\n",
      "Training-Batch No.9470\n",
      "Loss = 1.8054678440093994\n",
      "Training-Batch No.9480\n",
      "Loss = 3.1729769706726074\n",
      "Training-Batch No.9490\n",
      "Loss = 1.8124035596847534\n",
      "Training-Batch No.9500\n",
      "Loss = 5.606911659240723\n",
      "Training-Batch No.9510\n",
      "Loss = 2.509333610534668\n",
      "Training-Batch No.9520\n",
      "Loss = 2.127556324005127\n",
      "Training-Batch No.9530\n",
      "Loss = 1.9298806190490723\n",
      "Training-Batch No.9540\n",
      "Loss = 1.5381336212158203\n",
      "Training-Batch No.9550\n",
      "Loss = 2.440488338470459\n",
      "Training-Batch No.9560\n",
      "Loss = 2.7807159423828125\n",
      "Training-Batch No.9570\n",
      "Loss = 2.524296522140503\n",
      "Training-Batch No.9580\n",
      "Loss = 2.3984620571136475\n",
      "Training-Batch No.9590\n",
      "Loss = 2.1654369831085205\n",
      "Training-Batch No.9600\n",
      "Loss = 2.2753348350524902\n",
      "Training-Batch No.9610\n",
      "Loss = 3.263791799545288\n",
      "Training-Batch No.9620\n",
      "Loss = 2.191767692565918\n",
      "Training-Batch No.9630\n",
      "Loss = 4.029212951660156\n",
      "Training-Batch No.9640\n",
      "Loss = 2.8129851818084717\n",
      "Training-Batch No.9650\n",
      "Loss = 1.8110886812210083\n",
      "Training-Batch No.9660\n",
      "Loss = 2.400346279144287\n",
      "Training-Batch No.9670\n",
      "Loss = 2.648963451385498\n",
      "Training-Batch No.9680\n",
      "Loss = 3.0793967247009277\n",
      "Training-Batch No.9690\n",
      "Loss = 3.117556095123291\n",
      "Training-Batch No.9700\n",
      "Loss = 1.7005231380462646\n",
      "Training-Batch No.9710\n",
      "Loss = 2.1891651153564453\n",
      "Training-Batch No.9720\n",
      "Loss = 2.1336941719055176\n",
      "Training-Batch No.9730\n",
      "Loss = 4.988176345825195\n",
      "Training-Batch No.9740\n",
      "Loss = 5.910968780517578\n",
      "Training-Batch No.9750\n",
      "Loss = 2.058279514312744\n",
      "Training-Batch No.9760\n",
      "Loss = 1.9060808420181274\n",
      "Training-Batch No.9770\n",
      "Loss = 2.569575071334839\n",
      "Training-Batch No.9780\n",
      "Loss = 2.12638521194458\n",
      "Training-Batch No.9790\n",
      "Loss = 1.6638859510421753\n",
      "Training-Batch No.9800\n",
      "Loss = 1.8023124933242798\n",
      "Training-Batch No.9810\n",
      "Loss = 2.6811389923095703\n",
      "Training-Batch No.9820\n",
      "Loss = 3.2305471897125244\n",
      "Training-Batch No.9830\n",
      "Loss = 1.7374053001403809\n",
      "Training-Batch No.9840\n",
      "Loss = 3.2333590984344482\n",
      "Training-Batch No.9850\n",
      "Loss = 2.962892770767212\n",
      "Training-Batch No.9860\n",
      "Loss = 2.939666271209717\n",
      "Training-Batch No.9870\n",
      "Loss = 5.970858573913574\n",
      "Training-Batch No.9880\n",
      "Loss = 2.1881728172302246\n",
      "Training-Batch No.9890\n",
      "Loss = 2.12187123298645\n",
      "Training-Batch No.9900\n",
      "Loss = 2.610189199447632\n",
      "Training-Batch No.9910\n",
      "Loss = 2.7469797134399414\n",
      "Training-Batch No.9920\n",
      "Loss = 2.5425071716308594\n",
      "Training-Batch No.9930\n",
      "Loss = 6.691905498504639\n",
      "Training-Batch No.9940\n",
      "Loss = 1.7699671983718872\n",
      "Training-Batch No.9950\n",
      "Loss = 4.027705192565918\n",
      "Training-Batch No.9960\n",
      "Loss = 2.038534164428711\n",
      "Training-Batch No.9970\n",
      "Loss = 2.8404839038848877\n",
      "Training-Batch No.9980\n",
      "Loss = 1.9248095750808716\n",
      "Training-Batch No.9990\n",
      "Loss = 1.804945468902588\n",
      "Training-Batch No.10000\n",
      "Loss = 2.6024372577667236\n",
      "Training-Batch No.10010\n",
      "Loss = 2.5107734203338623\n",
      "Training-Batch No.10020\n",
      "Loss = 1.5247514247894287\n",
      "Training-Batch No.10030\n",
      "Loss = 2.417363405227661\n",
      "Training-Batch No.10040\n",
      "Loss = 2.6223742961883545\n",
      "Training-Batch No.10050\n",
      "Loss = 1.3975880146026611\n",
      "Training-Batch No.10060\n",
      "Loss = 3.437133312225342\n",
      "Training-Batch No.10070\n",
      "Loss = 1.7979602813720703\n",
      "Training-Batch No.10080\n",
      "Loss = 2.4036786556243896\n",
      "Training-Batch No.10090\n",
      "Loss = 2.3195745944976807\n",
      "Training-Batch No.10100\n",
      "Loss = 2.7153353691101074\n",
      "Training-Batch No.10110\n",
      "Loss = 2.138420581817627\n",
      "Training-Batch No.10120\n",
      "Loss = 3.6722962856292725\n",
      "Training-Batch No.10130\n",
      "Loss = 2.3548600673675537\n",
      "Training-Batch No.10140\n",
      "Loss = 2.416224479675293\n",
      "Training-Batch No.10150\n",
      "Loss = 2.7469964027404785\n",
      "Training-Batch No.10160\n",
      "Loss = 1.6410185098648071\n",
      "Training-Batch No.10170\n",
      "Loss = 1.8341559171676636\n",
      "Training-Batch No.10180\n",
      "Loss = 2.988224983215332\n",
      "Training-Batch No.10190\n",
      "Loss = 5.807518482208252\n",
      "Training-Batch No.10200\n",
      "Loss = 1.8013930320739746\n",
      "Training-Batch No.10210\n",
      "Loss = 2.2690210342407227\n",
      "Training-Batch No.10220\n",
      "Loss = 2.053201675415039\n",
      "Training-Batch No.10230\n",
      "Loss = 1.5834157466888428\n",
      "Training-Batch No.10240\n",
      "Loss = 1.6063156127929688\n",
      "Training-Batch No.10250\n",
      "Loss = 3.673444986343384\n",
      "Training-Batch No.10260\n",
      "Loss = 2.1434426307678223\n",
      "Training-Batch No.10270\n",
      "Loss = 2.4542014598846436\n",
      "Training-Batch No.10280\n",
      "Loss = 2.4532816410064697\n",
      "Training-Batch No.10290\n",
      "Loss = 1.8317995071411133\n",
      "Training-Batch No.10300\n",
      "Loss = 1.6298725605010986\n",
      "Training-Batch No.10310\n",
      "Loss = 2.7195253372192383\n",
      "Training-Batch No.10320\n",
      "Loss = 5.050790309906006\n",
      "Training-Batch No.10330\n",
      "Loss = 2.0926692485809326\n",
      "Training-Batch No.10340\n",
      "Loss = 2.0864572525024414\n",
      "Training-Batch No.10350\n",
      "Loss = 2.3012943267822266\n",
      "Training-Batch No.10360\n",
      "Loss = 2.663574695587158\n",
      "Training-Batch No.10370\n",
      "Loss = 2.3208518028259277\n",
      "Training-Batch No.10380\n",
      "Loss = 1.8301069736480713\n",
      "Training-Batch No.10390\n",
      "Loss = 2.818577289581299\n",
      "Training-Batch No.10400\n",
      "Loss = 2.6072356700897217\n",
      "Training-Batch No.10410\n",
      "Loss = 1.6629774570465088\n",
      "Training-Batch No.10420\n",
      "Loss = 2.097019910812378\n",
      "Training-Batch No.10430\n",
      "Loss = 2.4861671924591064\n",
      "Training-Batch No.10440\n",
      "Loss = 2.0253865718841553\n",
      "Training-Batch No.10450\n",
      "Loss = 2.3464877605438232\n",
      "Training-Batch No.10460\n",
      "Loss = 1.3742337226867676\n",
      "Training-Batch No.10470\n",
      "Loss = 1.917328953742981\n",
      "Training-Batch No.10480\n",
      "Loss = 2.2453806400299072\n",
      "Training-Batch No.10490\n",
      "Loss = 2.825730800628662\n",
      "Training-Batch No.10500\n",
      "Loss = 2.1500351428985596\n",
      "Training-Batch No.10510\n",
      "Loss = 1.334404706954956\n",
      "Training-Batch No.10520\n",
      "Loss = 1.9257875680923462\n",
      "Epoch 10 Training Loss: 2.4820\n",
      "Start validation\n",
      "Epoch 10 Validation Loss: 3.4073\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.05848787446504993\n",
      "Average Top-3 accuracy 0.2648597242035188\n",
      "Average Top-5 accuracy 0.4507845934379458\n",
      "Average Top-7 accuracy 0.5706134094151213\n",
      "Average Top-9 accuracy 0.6604850213980028\n",
      "Average Top-11 accuracy 0.72943414170233\n",
      "Average Top-13 accuracy 0.7845934379457917\n",
      "Average Top-15 accuracy 0.8250118877793629\n",
      "current acc 0.05848787446504993\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 11\n",
      "Training-Batch No.10530\n",
      "Loss = 6.585460662841797\n",
      "Training-Batch No.10540\n",
      "Loss = 2.788109302520752\n",
      "Training-Batch No.10550\n",
      "Loss = 2.504929304122925\n",
      "Training-Batch No.10560\n",
      "Loss = 1.6425918340682983\n",
      "Training-Batch No.10570\n",
      "Loss = 1.8877606391906738\n",
      "Training-Batch No.10580\n",
      "Loss = 2.176476001739502\n",
      "Training-Batch No.10590\n",
      "Loss = 1.9404022693634033\n",
      "Training-Batch No.10600\n",
      "Loss = 2.1788041591644287\n",
      "Training-Batch No.10610\n",
      "Loss = 2.433363437652588\n",
      "Training-Batch No.10620\n",
      "Loss = 2.2630136013031006\n",
      "Training-Batch No.10630\n",
      "Loss = 2.1533074378967285\n",
      "Training-Batch No.10640\n",
      "Loss = 5.418929576873779\n",
      "Training-Batch No.10650\n",
      "Loss = 2.0748918056488037\n",
      "Training-Batch No.10660\n",
      "Loss = 2.292431592941284\n",
      "Training-Batch No.10670\n",
      "Loss = 2.1721887588500977\n",
      "Training-Batch No.10680\n",
      "Loss = 2.312161922454834\n",
      "Training-Batch No.10690\n",
      "Loss = 1.8401256799697876\n",
      "Training-Batch No.10700\n",
      "Loss = 2.928393840789795\n",
      "Training-Batch No.10710\n",
      "Loss = 2.200751304626465\n",
      "Training-Batch No.10720\n",
      "Loss = 3.73306941986084\n",
      "Training-Batch No.10730\n",
      "Loss = 1.5238906145095825\n",
      "Training-Batch No.10740\n",
      "Loss = 2.3585057258605957\n",
      "Training-Batch No.10750\n",
      "Loss = 2.02032732963562\n",
      "Training-Batch No.10760\n",
      "Loss = 2.5307717323303223\n",
      "Training-Batch No.10770\n",
      "Loss = 2.0259451866149902\n",
      "Training-Batch No.10780\n",
      "Loss = 2.2848033905029297\n",
      "Training-Batch No.10790\n",
      "Loss = 2.944488763809204\n",
      "Training-Batch No.10800\n",
      "Loss = 4.241201400756836\n",
      "Training-Batch No.10810\n",
      "Loss = 2.2674336433410645\n",
      "Training-Batch No.10820\n",
      "Loss = 1.93439519405365\n",
      "Training-Batch No.10830\n",
      "Loss = 2.4919354915618896\n",
      "Training-Batch No.10840\n",
      "Loss = 2.4349074363708496\n",
      "Training-Batch No.10850\n",
      "Loss = 2.401301383972168\n",
      "Training-Batch No.10860\n",
      "Loss = 3.1257028579711914\n",
      "Training-Batch No.10870\n",
      "Loss = 2.051147937774658\n",
      "Training-Batch No.10880\n",
      "Loss = 1.6590169668197632\n",
      "Training-Batch No.10890\n",
      "Loss = 3.239330768585205\n",
      "Training-Batch No.10900\n",
      "Loss = 2.3442130088806152\n",
      "Training-Batch No.10910\n",
      "Loss = 1.5443787574768066\n",
      "Training-Batch No.10920\n",
      "Loss = 2.7830684185028076\n",
      "Training-Batch No.10930\n",
      "Loss = 2.0993940830230713\n",
      "Training-Batch No.10940\n",
      "Loss = 1.7532655000686646\n",
      "Training-Batch No.10950\n",
      "Loss = 2.0582077503204346\n",
      "Training-Batch No.10960\n",
      "Loss = 3.9033336639404297\n",
      "Training-Batch No.10970\n",
      "Loss = 1.7779643535614014\n",
      "Training-Batch No.10980\n",
      "Loss = 1.358574628829956\n",
      "Training-Batch No.10990\n",
      "Loss = 2.4678049087524414\n",
      "Training-Batch No.11000\n",
      "Loss = 2.4917521476745605\n",
      "Training-Batch No.11010\n",
      "Loss = 2.8555188179016113\n",
      "Training-Batch No.11020\n",
      "Loss = 1.604691505432129\n",
      "Training-Batch No.11030\n",
      "Loss = 1.9186850786209106\n",
      "Training-Batch No.11040\n",
      "Loss = 2.2235710620880127\n",
      "Training-Batch No.11050\n",
      "Loss = 2.133211135864258\n",
      "Training-Batch No.11060\n",
      "Loss = 2.102158308029175\n",
      "Training-Batch No.11070\n",
      "Loss = 2.439298152923584\n",
      "Training-Batch No.11080\n",
      "Loss = 2.152573585510254\n",
      "Training-Batch No.11090\n",
      "Loss = 3.0224125385284424\n",
      "Training-Batch No.11100\n",
      "Loss = 2.038806676864624\n",
      "Training-Batch No.11110\n",
      "Loss = 2.2613325119018555\n",
      "Training-Batch No.11120\n",
      "Loss = 2.3649802207946777\n",
      "Training-Batch No.11130\n",
      "Loss = 3.0030245780944824\n",
      "Training-Batch No.11140\n",
      "Loss = 1.864212989807129\n",
      "Training-Batch No.11150\n",
      "Loss = 2.0947699546813965\n",
      "Training-Batch No.11160\n",
      "Loss = 2.127962589263916\n",
      "Training-Batch No.11170\n",
      "Loss = 1.9871809482574463\n",
      "Training-Batch No.11180\n",
      "Loss = 1.6517584323883057\n",
      "Training-Batch No.11190\n",
      "Loss = 2.5678369998931885\n",
      "Training-Batch No.11200\n",
      "Loss = 2.310990810394287\n",
      "Training-Batch No.11210\n",
      "Loss = 1.375524878501892\n",
      "Training-Batch No.11220\n",
      "Loss = 1.3569769859313965\n",
      "Training-Batch No.11230\n",
      "Loss = 2.341043710708618\n",
      "Training-Batch No.11240\n",
      "Loss = 4.451495170593262\n",
      "Training-Batch No.11250\n",
      "Loss = 2.052244186401367\n",
      "Training-Batch No.11260\n",
      "Loss = 2.634134292602539\n",
      "Training-Batch No.11270\n",
      "Loss = 2.7980422973632812\n",
      "Training-Batch No.11280\n",
      "Loss = 3.34645676612854\n",
      "Training-Batch No.11290\n",
      "Loss = 3.0940260887145996\n",
      "Training-Batch No.11300\n",
      "Loss = 2.711305618286133\n",
      "Training-Batch No.11310\n",
      "Loss = 1.614620327949524\n",
      "Training-Batch No.11320\n",
      "Loss = 2.570115089416504\n",
      "Training-Batch No.11330\n",
      "Loss = 1.7448810338974\n",
      "Training-Batch No.11340\n",
      "Loss = 1.820878267288208\n",
      "Training-Batch No.11350\n",
      "Loss = 7.329103946685791\n",
      "Training-Batch No.11360\n",
      "Loss = 2.4508299827575684\n",
      "Training-Batch No.11370\n",
      "Loss = 2.406172275543213\n",
      "Training-Batch No.11380\n",
      "Loss = 4.86599588394165\n",
      "Training-Batch No.11390\n",
      "Loss = 2.0884180068969727\n",
      "Training-Batch No.11400\n",
      "Loss = 1.7324707508087158\n",
      "Training-Batch No.11410\n",
      "Loss = 3.3458478450775146\n",
      "Training-Batch No.11420\n",
      "Loss = 1.7338883876800537\n",
      "Training-Batch No.11430\n",
      "Loss = 2.1686148643493652\n",
      "Training-Batch No.11440\n",
      "Loss = 2.3901751041412354\n",
      "Training-Batch No.11450\n",
      "Loss = 1.6044464111328125\n",
      "Training-Batch No.11460\n",
      "Loss = 1.4784233570098877\n",
      "Training-Batch No.11470\n",
      "Loss = 1.6652095317840576\n",
      "Training-Batch No.11480\n",
      "Loss = 2.4466066360473633\n",
      "Training-Batch No.11490\n",
      "Loss = 2.0198652744293213\n",
      "Training-Batch No.11500\n",
      "Loss = 2.556788206100464\n",
      "Training-Batch No.11510\n",
      "Loss = 2.4477500915527344\n",
      "Training-Batch No.11520\n",
      "Loss = 1.3968005180358887\n",
      "Training-Batch No.11530\n",
      "Loss = 1.8641153573989868\n",
      "Training-Batch No.11540\n",
      "Loss = 2.359280824661255\n",
      "Training-Batch No.11550\n",
      "Loss = 1.6191670894622803\n",
      "Training-Batch No.11560\n",
      "Loss = 2.2544944286346436\n",
      "Training-Batch No.11570\n",
      "Loss = 2.3744094371795654\n",
      "Epoch 11 Training Loss: 2.4373\n",
      "Start validation\n",
      "Epoch 11 Validation Loss: 2.5526\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.24964336661911554\n",
      "Average Top-3 accuracy 0.5653827864954827\n",
      "Average Top-5 accuracy 0.7669995244888255\n",
      "Average Top-7 accuracy 0.8359486447931527\n",
      "Average Top-9 accuracy 0.8987161198288159\n",
      "Average Top-11 accuracy 0.9196386115073705\n",
      "Average Top-13 accuracy 0.9353304802662863\n",
      "Average Top-15 accuracy 0.9548264384213029\n",
      "current acc 0.24964336661911554\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 12\n",
      "Training-Batch No.11580\n",
      "Loss = 2.4328908920288086\n",
      "Training-Batch No.11590\n",
      "Loss = 1.6480298042297363\n",
      "Training-Batch No.11600\n",
      "Loss = 1.918900728225708\n",
      "Training-Batch No.11610\n",
      "Loss = 1.6494560241699219\n",
      "Training-Batch No.11620\n",
      "Loss = 2.228151559829712\n",
      "Training-Batch No.11630\n",
      "Loss = 2.7070443630218506\n",
      "Training-Batch No.11640\n",
      "Loss = 1.901921272277832\n",
      "Training-Batch No.11650\n",
      "Loss = 3.343637704849243\n",
      "Training-Batch No.11660\n",
      "Loss = 1.6795547008514404\n",
      "Training-Batch No.11670\n",
      "Loss = 1.550506830215454\n",
      "Training-Batch No.11680\n",
      "Loss = 6.542219161987305\n",
      "Training-Batch No.11690\n",
      "Loss = 1.7102329730987549\n",
      "Training-Batch No.11700\n",
      "Loss = 1.9909800291061401\n",
      "Training-Batch No.11710\n",
      "Loss = 2.2812435626983643\n",
      "Training-Batch No.11720\n",
      "Loss = 1.766136646270752\n",
      "Training-Batch No.11730\n",
      "Loss = 2.694122076034546\n",
      "Training-Batch No.11740\n",
      "Loss = 2.0534615516662598\n",
      "Training-Batch No.11750\n",
      "Loss = 2.9006240367889404\n",
      "Training-Batch No.11760\n",
      "Loss = 3.540027141571045\n",
      "Training-Batch No.11770\n",
      "Loss = 2.6257681846618652\n",
      "Training-Batch No.11780\n",
      "Loss = 2.372823715209961\n",
      "Training-Batch No.11790\n",
      "Loss = 1.868391752243042\n",
      "Training-Batch No.11800\n",
      "Loss = 2.014977216720581\n",
      "Training-Batch No.11810\n",
      "Loss = 2.562650203704834\n",
      "Training-Batch No.11820\n",
      "Loss = 3.366702079772949\n",
      "Training-Batch No.11830\n",
      "Loss = 2.165067672729492\n",
      "Training-Batch No.11840\n",
      "Loss = 1.6806937456130981\n",
      "Training-Batch No.11850\n",
      "Loss = 1.7567427158355713\n",
      "Training-Batch No.11860\n",
      "Loss = 1.6488274335861206\n",
      "Training-Batch No.11870\n",
      "Loss = 1.9671450853347778\n",
      "Training-Batch No.11880\n",
      "Loss = 2.1151013374328613\n",
      "Training-Batch No.11890\n",
      "Loss = 2.1488852500915527\n",
      "Training-Batch No.11900\n",
      "Loss = 2.0921943187713623\n",
      "Training-Batch No.11910\n",
      "Loss = 1.658190131187439\n",
      "Training-Batch No.11920\n",
      "Loss = 2.1553821563720703\n",
      "Training-Batch No.11930\n",
      "Loss = 1.7858322858810425\n",
      "Training-Batch No.11940\n",
      "Loss = 1.8797138929367065\n",
      "Training-Batch No.11950\n",
      "Loss = 5.756287097930908\n",
      "Training-Batch No.11960\n",
      "Loss = 2.432401657104492\n",
      "Training-Batch No.11970\n",
      "Loss = 2.2267494201660156\n",
      "Training-Batch No.11980\n",
      "Loss = 2.322047472000122\n",
      "Training-Batch No.11990\n",
      "Loss = 2.2107250690460205\n",
      "Training-Batch No.12000\n",
      "Loss = 1.8150001764297485\n",
      "Training-Batch No.12010\n",
      "Loss = 2.362588405609131\n",
      "Training-Batch No.12020\n",
      "Loss = 2.4547722339630127\n",
      "Training-Batch No.12030\n",
      "Loss = 1.7965211868286133\n",
      "Training-Batch No.12040\n",
      "Loss = 2.585493564605713\n",
      "Training-Batch No.12050\n",
      "Loss = 1.4456820487976074\n",
      "Training-Batch No.12060\n",
      "Loss = 2.321420669555664\n",
      "Training-Batch No.12070\n",
      "Loss = 1.4066863059997559\n",
      "Training-Batch No.12080\n",
      "Loss = 1.522679090499878\n",
      "Training-Batch No.12090\n",
      "Loss = 1.7150163650512695\n",
      "Training-Batch No.12100\n",
      "Loss = 1.992344617843628\n",
      "Training-Batch No.12110\n",
      "Loss = 1.7111469507217407\n",
      "Training-Batch No.12120\n",
      "Loss = 2.410295009613037\n",
      "Training-Batch No.12130\n",
      "Loss = 3.4774999618530273\n",
      "Training-Batch No.12140\n",
      "Loss = 6.7996649742126465\n",
      "Training-Batch No.12150\n",
      "Loss = 1.8264317512512207\n",
      "Training-Batch No.12160\n",
      "Loss = 2.3084325790405273\n",
      "Training-Batch No.12170\n",
      "Loss = 2.300945997238159\n",
      "Training-Batch No.12180\n",
      "Loss = 2.32848858833313\n",
      "Training-Batch No.12190\n",
      "Loss = 1.9295713901519775\n",
      "Training-Batch No.12200\n",
      "Loss = 1.8630285263061523\n",
      "Training-Batch No.12210\n",
      "Loss = 2.74360728263855\n",
      "Training-Batch No.12220\n",
      "Loss = 2.727057456970215\n",
      "Training-Batch No.12230\n",
      "Loss = 1.7747126817703247\n",
      "Training-Batch No.12240\n",
      "Loss = 2.179363250732422\n",
      "Training-Batch No.12250\n",
      "Loss = 2.5027072429656982\n",
      "Training-Batch No.12260\n",
      "Loss = 2.6058268547058105\n",
      "Training-Batch No.12270\n",
      "Loss = 1.5704703330993652\n",
      "Training-Batch No.12280\n",
      "Loss = 2.498901844024658\n",
      "Training-Batch No.12290\n",
      "Loss = 1.665615200996399\n",
      "Training-Batch No.12300\n",
      "Loss = 2.771047592163086\n",
      "Training-Batch No.12310\n",
      "Loss = 2.4401233196258545\n",
      "Training-Batch No.12320\n",
      "Loss = 2.331400156021118\n",
      "Training-Batch No.12330\n",
      "Loss = 2.6772642135620117\n",
      "Training-Batch No.12340\n",
      "Loss = 1.3607169389724731\n",
      "Training-Batch No.12350\n",
      "Loss = 2.6018989086151123\n",
      "Training-Batch No.12360\n",
      "Loss = 2.7153072357177734\n",
      "Training-Batch No.12370\n",
      "Loss = 2.6187307834625244\n",
      "Training-Batch No.12380\n",
      "Loss = 1.6108096837997437\n",
      "Training-Batch No.12390\n",
      "Loss = 1.7838468551635742\n",
      "Training-Batch No.12400\n",
      "Loss = 2.3671905994415283\n",
      "Training-Batch No.12410\n",
      "Loss = 1.7297512292861938\n",
      "Training-Batch No.12420\n",
      "Loss = 2.8708109855651855\n",
      "Training-Batch No.12430\n",
      "Loss = 3.2353079319000244\n",
      "Training-Batch No.12440\n",
      "Loss = 1.4184201955795288\n",
      "Training-Batch No.12450\n",
      "Loss = 2.521450996398926\n",
      "Training-Batch No.12460\n",
      "Loss = 1.352988600730896\n",
      "Training-Batch No.12470\n",
      "Loss = 2.2967374324798584\n",
      "Training-Batch No.12480\n",
      "Loss = 2.0089211463928223\n",
      "Training-Batch No.12490\n",
      "Loss = 2.286705493927002\n",
      "Training-Batch No.12500\n",
      "Loss = 2.0139098167419434\n",
      "Training-Batch No.12510\n",
      "Loss = 2.7652511596679688\n",
      "Training-Batch No.12520\n",
      "Loss = 2.1747121810913086\n",
      "Training-Batch No.12530\n",
      "Loss = 1.8440543413162231\n",
      "Training-Batch No.12540\n",
      "Loss = 1.950850248336792\n",
      "Training-Batch No.12550\n",
      "Loss = 2.1593270301818848\n",
      "Training-Batch No.12560\n",
      "Loss = 2.26340913772583\n",
      "Training-Batch No.12570\n",
      "Loss = 1.9736921787261963\n",
      "Training-Batch No.12580\n",
      "Loss = 1.3591833114624023\n",
      "Training-Batch No.12590\n",
      "Loss = 2.5569050312042236\n",
      "Training-Batch No.12600\n",
      "Loss = 2.069391965866089\n",
      "Training-Batch No.12610\n",
      "Loss = 1.9001270532608032\n",
      "Training-Batch No.12620\n",
      "Loss = 1.92777681350708\n",
      "Epoch 12 Training Loss: 2.3982\n",
      "Start validation\n",
      "Epoch 12 Validation Loss: 2.5358\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.29529243937232524\n",
      "Average Top-3 accuracy 0.6081787922016167\n",
      "Average Top-5 accuracy 0.7669995244888255\n",
      "Average Top-7 accuracy 0.8492629576795054\n",
      "Average Top-9 accuracy 0.874940561103186\n",
      "Average Top-11 accuracy 0.8930099857346647\n",
      "Average Top-13 accuracy 0.9305753685211603\n",
      "Average Top-15 accuracy 0.9429386590584878\n",
      "current acc 0.29529243937232524\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 13\n",
      "Training-Batch No.12630\n",
      "Loss = 2.530458688735962\n",
      "Training-Batch No.12640\n",
      "Loss = 2.0808045864105225\n",
      "Training-Batch No.12650\n",
      "Loss = 1.4419089555740356\n",
      "Training-Batch No.12660\n",
      "Loss = 1.9656445980072021\n",
      "Training-Batch No.12670\n",
      "Loss = 1.7308712005615234\n",
      "Training-Batch No.12680\n",
      "Loss = 2.241584300994873\n",
      "Training-Batch No.12690\n",
      "Loss = 4.559736251831055\n",
      "Training-Batch No.12700\n",
      "Loss = 1.4949424266815186\n",
      "Training-Batch No.12710\n",
      "Loss = 1.8148643970489502\n",
      "Training-Batch No.12720\n",
      "Loss = 3.125718832015991\n",
      "Training-Batch No.12730\n",
      "Loss = 2.176286220550537\n",
      "Training-Batch No.12740\n",
      "Loss = 1.380300760269165\n",
      "Training-Batch No.12750\n",
      "Loss = 3.4936437606811523\n",
      "Training-Batch No.12760\n",
      "Loss = 2.445495128631592\n",
      "Training-Batch No.12770\n",
      "Loss = 2.1886065006256104\n",
      "Training-Batch No.12780\n",
      "Loss = 1.9022032022476196\n",
      "Training-Batch No.12790\n",
      "Loss = 1.6581364870071411\n",
      "Training-Batch No.12800\n",
      "Loss = 1.9798250198364258\n",
      "Training-Batch No.12810\n",
      "Loss = 6.621126174926758\n",
      "Training-Batch No.12820\n",
      "Loss = 2.0184202194213867\n",
      "Training-Batch No.12830\n",
      "Loss = 2.8493282794952393\n",
      "Training-Batch No.12840\n",
      "Loss = 2.190068244934082\n",
      "Training-Batch No.12850\n",
      "Loss = 1.4482941627502441\n",
      "Training-Batch No.12860\n",
      "Loss = 2.0528154373168945\n",
      "Training-Batch No.12870\n",
      "Loss = 1.6744215488433838\n",
      "Training-Batch No.12880\n",
      "Loss = 2.0514676570892334\n",
      "Training-Batch No.12890\n",
      "Loss = 2.0527443885803223\n",
      "Training-Batch No.12900\n",
      "Loss = 1.835607886314392\n",
      "Training-Batch No.12910\n",
      "Loss = 1.890357494354248\n",
      "Training-Batch No.12920\n",
      "Loss = 5.011655330657959\n",
      "Training-Batch No.12930\n",
      "Loss = 2.560532569885254\n",
      "Training-Batch No.12940\n",
      "Loss = 2.1405234336853027\n",
      "Training-Batch No.12950\n",
      "Loss = 2.2048027515411377\n",
      "Training-Batch No.12960\n",
      "Loss = 2.3760831356048584\n",
      "Training-Batch No.12970\n",
      "Loss = 2.6819982528686523\n",
      "Training-Batch No.12980\n",
      "Loss = 1.913081169128418\n",
      "Training-Batch No.12990\n",
      "Loss = 2.099909782409668\n",
      "Training-Batch No.13000\n",
      "Loss = 2.7724339962005615\n",
      "Training-Batch No.13010\n",
      "Loss = 2.4742989540100098\n",
      "Training-Batch No.13020\n",
      "Loss = 5.657175540924072\n",
      "Training-Batch No.13030\n",
      "Loss = 2.3273611068725586\n",
      "Training-Batch No.13040\n",
      "Loss = 2.0716207027435303\n",
      "Training-Batch No.13050\n",
      "Loss = 2.3113324642181396\n",
      "Training-Batch No.13060\n",
      "Loss = 1.8645470142364502\n",
      "Training-Batch No.13070\n",
      "Loss = 1.7631419897079468\n",
      "Training-Batch No.13080\n",
      "Loss = 2.2829408645629883\n",
      "Training-Batch No.13090\n",
      "Loss = 1.8027094602584839\n",
      "Training-Batch No.13100\n",
      "Loss = 2.0974783897399902\n",
      "Training-Batch No.13110\n",
      "Loss = 1.561668038368225\n",
      "Training-Batch No.13120\n",
      "Loss = 1.8663384914398193\n",
      "Training-Batch No.13130\n",
      "Loss = 2.131211757659912\n",
      "Training-Batch No.13140\n",
      "Loss = 2.308972120285034\n",
      "Training-Batch No.13150\n",
      "Loss = 1.7301530838012695\n",
      "Training-Batch No.13160\n",
      "Loss = 2.331820487976074\n",
      "Training-Batch No.13170\n",
      "Loss = 1.3928096294403076\n",
      "Training-Batch No.13180\n",
      "Loss = 1.9192650318145752\n",
      "Training-Batch No.13190\n",
      "Loss = 1.6177771091461182\n",
      "Training-Batch No.13200\n",
      "Loss = 1.4534707069396973\n",
      "Training-Batch No.13210\n",
      "Loss = 1.6178505420684814\n",
      "Training-Batch No.13220\n",
      "Loss = 1.7436178922653198\n",
      "Training-Batch No.13230\n",
      "Loss = 1.8659974336624146\n",
      "Training-Batch No.13240\n",
      "Loss = 1.7286008596420288\n",
      "Training-Batch No.13250\n",
      "Loss = 2.1121816635131836\n",
      "Training-Batch No.13260\n",
      "Loss = 2.1670784950256348\n",
      "Training-Batch No.13270\n",
      "Loss = 1.8060330152511597\n",
      "Training-Batch No.13280\n",
      "Loss = 1.545349359512329\n",
      "Training-Batch No.13290\n",
      "Loss = 2.7293829917907715\n",
      "Training-Batch No.13300\n",
      "Loss = 1.7829008102416992\n",
      "Training-Batch No.13310\n",
      "Loss = 1.7954742908477783\n",
      "Training-Batch No.13320\n",
      "Loss = 2.336851119995117\n",
      "Training-Batch No.13330\n",
      "Loss = 6.015315532684326\n",
      "Training-Batch No.13340\n",
      "Loss = 2.5609307289123535\n",
      "Training-Batch No.13350\n",
      "Loss = 2.3465077877044678\n",
      "Training-Batch No.13360\n",
      "Loss = 1.460603952407837\n",
      "Training-Batch No.13370\n",
      "Loss = 2.7635245323181152\n",
      "Training-Batch No.13380\n",
      "Loss = 2.244231939315796\n",
      "Training-Batch No.13390\n",
      "Loss = 2.432919502258301\n",
      "Training-Batch No.13400\n",
      "Loss = 3.546693801879883\n",
      "Training-Batch No.13410\n",
      "Loss = 2.264699935913086\n",
      "Training-Batch No.13420\n",
      "Loss = 1.6595674753189087\n",
      "Training-Batch No.13430\n",
      "Loss = 1.6634763479232788\n",
      "Training-Batch No.13440\n",
      "Loss = 2.935558557510376\n",
      "Training-Batch No.13450\n",
      "Loss = 2.115988254547119\n",
      "Training-Batch No.13460\n",
      "Loss = 2.127683401107788\n",
      "Training-Batch No.13470\n",
      "Loss = 2.6102099418640137\n",
      "Training-Batch No.13480\n",
      "Loss = 2.1840553283691406\n",
      "Training-Batch No.13490\n",
      "Loss = 4.418282508850098\n",
      "Training-Batch No.13500\n",
      "Loss = 1.603230595588684\n",
      "Training-Batch No.13510\n",
      "Loss = 1.4272921085357666\n",
      "Training-Batch No.13520\n",
      "Loss = 2.7492127418518066\n",
      "Training-Batch No.13530\n",
      "Loss = 1.9024137258529663\n",
      "Training-Batch No.13540\n",
      "Loss = 2.0194079875946045\n",
      "Training-Batch No.13550\n",
      "Loss = 1.8688405752182007\n",
      "Training-Batch No.13560\n",
      "Loss = 2.0354340076446533\n",
      "Training-Batch No.13570\n",
      "Loss = 2.473562240600586\n",
      "Training-Batch No.13580\n",
      "Loss = 1.3157539367675781\n",
      "Training-Batch No.13590\n",
      "Loss = 2.4029574394226074\n",
      "Training-Batch No.13600\n",
      "Loss = 1.9045097827911377\n",
      "Training-Batch No.13610\n",
      "Loss = 1.6000418663024902\n",
      "Training-Batch No.13620\n",
      "Loss = 1.826728343963623\n",
      "Training-Batch No.13630\n",
      "Loss = 2.011223316192627\n",
      "Training-Batch No.13640\n",
      "Loss = 2.5342254638671875\n",
      "Training-Batch No.13650\n",
      "Loss = 1.8941341638565063\n",
      "Training-Batch No.13660\n",
      "Loss = 2.221039056777954\n",
      "Training-Batch No.13670\n",
      "Loss = 2.7401621341705322\n",
      "Epoch 13 Training Loss: 2.3785\n",
      "Start validation\n",
      "Epoch 13 Validation Loss: 2.9175\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.11412268188302425\n",
      "Average Top-3 accuracy 0.4136947218259629\n",
      "Average Top-5 accuracy 0.5844032334759867\n",
      "Average Top-7 accuracy 0.7232524964336662\n",
      "Average Top-9 accuracy 0.854493580599144\n",
      "Average Top-11 accuracy 0.9125059438896814\n",
      "Average Top-13 accuracy 0.9453162149310509\n",
      "Average Top-15 accuracy 0.9538754160722777\n",
      "current acc 0.11412268188302425\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 14\n",
      "Training-Batch No.13680\n",
      "Loss = 2.314758539199829\n",
      "Training-Batch No.13690\n",
      "Loss = 2.693169116973877\n",
      "Training-Batch No.13700\n",
      "Loss = 2.191222667694092\n",
      "Training-Batch No.13710\n",
      "Loss = 2.0554637908935547\n",
      "Training-Batch No.13720\n",
      "Loss = 2.759824514389038\n",
      "Training-Batch No.13730\n",
      "Loss = 2.509028673171997\n",
      "Training-Batch No.13740\n",
      "Loss = 1.8948581218719482\n",
      "Training-Batch No.13750\n",
      "Loss = 3.1522977352142334\n",
      "Training-Batch No.13760\n",
      "Loss = 2.5442278385162354\n",
      "Training-Batch No.13770\n",
      "Loss = 2.313028335571289\n",
      "Training-Batch No.13780\n",
      "Loss = 1.9495930671691895\n",
      "Training-Batch No.13790\n",
      "Loss = 1.793472170829773\n",
      "Training-Batch No.13800\n",
      "Loss = 1.5023269653320312\n",
      "Training-Batch No.13810\n",
      "Loss = 2.8330154418945312\n",
      "Training-Batch No.13820\n",
      "Loss = 1.860857367515564\n",
      "Training-Batch No.13830\n",
      "Loss = 1.6316808462142944\n",
      "Training-Batch No.13840\n",
      "Loss = 3.9245712757110596\n",
      "Training-Batch No.13850\n",
      "Loss = 2.1100211143493652\n",
      "Training-Batch No.13860\n",
      "Loss = 2.5512197017669678\n",
      "Training-Batch No.13870\n",
      "Loss = 3.7337279319763184\n",
      "Training-Batch No.13880\n",
      "Loss = 2.5793731212615967\n",
      "Training-Batch No.13890\n",
      "Loss = 2.5124478340148926\n",
      "Training-Batch No.13900\n",
      "Loss = 2.6104483604431152\n",
      "Training-Batch No.13910\n",
      "Loss = 1.6160435676574707\n",
      "Training-Batch No.13920\n",
      "Loss = 2.0265979766845703\n",
      "Training-Batch No.13930\n",
      "Loss = 1.7002418041229248\n",
      "Training-Batch No.13940\n",
      "Loss = 1.6068637371063232\n",
      "Training-Batch No.13950\n",
      "Loss = 2.6260876655578613\n",
      "Training-Batch No.13960\n",
      "Loss = 5.9119720458984375\n",
      "Training-Batch No.13970\n",
      "Loss = 4.757282257080078\n",
      "Training-Batch No.13980\n",
      "Loss = 2.120985746383667\n",
      "Training-Batch No.13990\n",
      "Loss = 2.4038805961608887\n",
      "Training-Batch No.14000\n",
      "Loss = 2.263162136077881\n",
      "Training-Batch No.14010\n",
      "Loss = 1.7453778982162476\n",
      "Training-Batch No.14020\n",
      "Loss = 2.100165367126465\n",
      "Training-Batch No.14030\n",
      "Loss = 2.3467531204223633\n",
      "Training-Batch No.14040\n",
      "Loss = 2.032456398010254\n",
      "Training-Batch No.14050\n",
      "Loss = 1.8821908235549927\n",
      "Training-Batch No.14060\n",
      "Loss = 3.001986026763916\n",
      "Training-Batch No.14070\n",
      "Loss = 1.9329355955123901\n",
      "Training-Batch No.14080\n",
      "Loss = 2.416992425918579\n",
      "Training-Batch No.14090\n",
      "Loss = 2.4776391983032227\n",
      "Training-Batch No.14100\n",
      "Loss = 2.355908155441284\n",
      "Training-Batch No.14110\n",
      "Loss = 1.7318000793457031\n",
      "Training-Batch No.14120\n",
      "Loss = 1.2379909753799438\n",
      "Training-Batch No.14130\n",
      "Loss = 2.138608455657959\n",
      "Training-Batch No.14140\n",
      "Loss = 2.495767116546631\n",
      "Training-Batch No.14150\n",
      "Loss = 2.4677796363830566\n",
      "Training-Batch No.14160\n",
      "Loss = 1.700618028640747\n",
      "Training-Batch No.14170\n",
      "Loss = 2.140125036239624\n",
      "Training-Batch No.14180\n",
      "Loss = 2.1190500259399414\n",
      "Training-Batch No.14190\n",
      "Loss = 2.1915154457092285\n",
      "Training-Batch No.14200\n",
      "Loss = 2.3702173233032227\n",
      "Training-Batch No.14210\n",
      "Loss = 2.062312364578247\n",
      "Training-Batch No.14220\n",
      "Loss = 1.5261019468307495\n",
      "Training-Batch No.14230\n",
      "Loss = 5.741034507751465\n",
      "Training-Batch No.14240\n",
      "Loss = 3.640336275100708\n",
      "Training-Batch No.14250\n",
      "Loss = 4.394284725189209\n",
      "Training-Batch No.14260\n",
      "Loss = 1.5995373725891113\n",
      "Training-Batch No.14270\n",
      "Loss = 2.4224863052368164\n",
      "Training-Batch No.14280\n",
      "Loss = 1.7067174911499023\n",
      "Training-Batch No.14290\n",
      "Loss = 1.8356585502624512\n",
      "Training-Batch No.14300\n",
      "Loss = 3.0275187492370605\n",
      "Training-Batch No.14310\n",
      "Loss = 2.737790822982788\n",
      "Training-Batch No.14320\n",
      "Loss = 2.246661424636841\n",
      "Training-Batch No.14330\n",
      "Loss = 2.7522289752960205\n",
      "Training-Batch No.14340\n",
      "Loss = 2.4645118713378906\n",
      "Training-Batch No.14350\n",
      "Loss = 2.3318443298339844\n",
      "Training-Batch No.14360\n",
      "Loss = 2.3008265495300293\n",
      "Training-Batch No.14370\n",
      "Loss = 4.897992134094238\n",
      "Training-Batch No.14380\n",
      "Loss = 1.6929391622543335\n",
      "Training-Batch No.14390\n",
      "Loss = 1.842188835144043\n",
      "Training-Batch No.14400\n",
      "Loss = 2.2662572860717773\n",
      "Training-Batch No.14410\n",
      "Loss = 2.968177556991577\n",
      "Training-Batch No.14420\n",
      "Loss = 3.280804395675659\n",
      "Training-Batch No.14430\n",
      "Loss = 1.7427926063537598\n",
      "Training-Batch No.14440\n",
      "Loss = 1.9295395612716675\n",
      "Training-Batch No.14450\n",
      "Loss = 1.8825548887252808\n",
      "Training-Batch No.14460\n",
      "Loss = 1.7089853286743164\n",
      "Training-Batch No.14470\n",
      "Loss = 1.7360118627548218\n",
      "Training-Batch No.14480\n",
      "Loss = 2.5592596530914307\n",
      "Training-Batch No.14490\n",
      "Loss = 2.4234461784362793\n",
      "Training-Batch No.14500\n",
      "Loss = 1.7195377349853516\n",
      "Training-Batch No.14510\n",
      "Loss = 1.7727266550064087\n",
      "Training-Batch No.14520\n",
      "Loss = 1.7320207357406616\n",
      "Training-Batch No.14530\n",
      "Loss = 1.2896060943603516\n",
      "Training-Batch No.14540\n",
      "Loss = 2.141644239425659\n",
      "Training-Batch No.14550\n",
      "Loss = 4.5103535652160645\n",
      "Training-Batch No.14560\n",
      "Loss = 2.3543171882629395\n",
      "Training-Batch No.14570\n",
      "Loss = 2.4043729305267334\n",
      "Training-Batch No.14580\n",
      "Loss = 2.7995445728302\n",
      "Training-Batch No.14590\n",
      "Loss = 2.1857755184173584\n",
      "Training-Batch No.14600\n",
      "Loss = 5.04674768447876\n",
      "Training-Batch No.14610\n",
      "Loss = 2.0301082134246826\n",
      "Training-Batch No.14620\n",
      "Loss = 3.004396438598633\n",
      "Training-Batch No.14630\n",
      "Loss = 1.8756049871444702\n",
      "Training-Batch No.14640\n",
      "Loss = 2.514601230621338\n",
      "Training-Batch No.14650\n",
      "Loss = 1.2282735109329224\n",
      "Training-Batch No.14660\n",
      "Loss = 2.001739740371704\n",
      "Training-Batch No.14670\n",
      "Loss = 1.6320223808288574\n",
      "Training-Batch No.14680\n",
      "Loss = 2.784977436065674\n",
      "Training-Batch No.14690\n",
      "Loss = 2.0675525665283203\n",
      "Training-Batch No.14700\n",
      "Loss = 2.3121612071990967\n",
      "Training-Batch No.14710\n",
      "Loss = 2.124802350997925\n",
      "Training-Batch No.14720\n",
      "Loss = 1.8348702192306519\n",
      "Epoch 14 Training Loss: 2.3532\n",
      "Start validation\n",
      "Epoch 14 Validation Loss: 7.0507\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.01759391345696624\n",
      "Average Top-3 accuracy 0.0461245839277223\n",
      "Average Top-5 accuracy 0.07560627674750357\n",
      "Average Top-7 accuracy 0.11554921540656206\n",
      "Average Top-9 accuracy 0.15691868758915833\n",
      "Average Top-11 accuracy 0.2239657631954351\n",
      "Average Top-13 accuracy 0.2743699476937708\n",
      "Average Top-15 accuracy 0.3147883975273419\n",
      "current acc 0.01759391345696624\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 15\n",
      "Training-Batch No.14730\n",
      "Loss = 1.7338347434997559\n",
      "Training-Batch No.14740\n",
      "Loss = 3.4408655166625977\n",
      "Training-Batch No.14750\n",
      "Loss = 1.6857028007507324\n",
      "Training-Batch No.14760\n",
      "Loss = 4.502303600311279\n",
      "Training-Batch No.14770\n",
      "Loss = 2.804595470428467\n",
      "Training-Batch No.14780\n",
      "Loss = 1.884881615638733\n",
      "Training-Batch No.14790\n",
      "Loss = 1.8147149085998535\n",
      "Training-Batch No.14800\n",
      "Loss = 1.442370891571045\n",
      "Training-Batch No.14810\n",
      "Loss = 2.2551751136779785\n",
      "Training-Batch No.14820\n",
      "Loss = 2.5970304012298584\n",
      "Training-Batch No.14830\n",
      "Loss = 2.3459291458129883\n",
      "Training-Batch No.14840\n",
      "Loss = 2.11702299118042\n",
      "Training-Batch No.14850\n",
      "Loss = 2.037816286087036\n",
      "Training-Batch No.14860\n",
      "Loss = 2.133739471435547\n",
      "Training-Batch No.14870\n",
      "Loss = 2.6186976432800293\n",
      "Training-Batch No.14880\n",
      "Loss = 2.054506778717041\n",
      "Training-Batch No.14890\n",
      "Loss = 4.189781665802002\n",
      "Training-Batch No.14900\n",
      "Loss = 2.281038522720337\n",
      "Training-Batch No.14910\n",
      "Loss = 1.4416319131851196\n",
      "Training-Batch No.14920\n",
      "Loss = 2.4261720180511475\n",
      "Training-Batch No.14930\n",
      "Loss = 2.628800868988037\n",
      "Training-Batch No.14940\n",
      "Loss = 3.1520392894744873\n",
      "Training-Batch No.14950\n",
      "Loss = 2.8590049743652344\n",
      "Training-Batch No.14960\n",
      "Loss = 1.5082132816314697\n",
      "Training-Batch No.14970\n",
      "Loss = 2.1202690601348877\n",
      "Training-Batch No.14980\n",
      "Loss = 2.116598606109619\n",
      "Training-Batch No.14990\n",
      "Loss = 5.907081604003906\n",
      "Training-Batch No.15000\n",
      "Loss = 3.9089534282684326\n",
      "Training-Batch No.15010\n",
      "Loss = 2.1333632469177246\n",
      "Training-Batch No.15020\n",
      "Loss = 1.844214916229248\n",
      "Training-Batch No.15030\n",
      "Loss = 3.0515360832214355\n",
      "Training-Batch No.15040\n",
      "Loss = 2.061552047729492\n",
      "Training-Batch No.15050\n",
      "Loss = 1.575088620185852\n",
      "Training-Batch No.15060\n",
      "Loss = 1.6266558170318604\n",
      "Training-Batch No.15070\n",
      "Loss = 2.5491538047790527\n",
      "Training-Batch No.15080\n",
      "Loss = 2.8915305137634277\n",
      "Training-Batch No.15090\n",
      "Loss = 1.6092674732208252\n",
      "Training-Batch No.15100\n",
      "Loss = 2.9769372940063477\n",
      "Training-Batch No.15110\n",
      "Loss = 2.7587900161743164\n",
      "Training-Batch No.15120\n",
      "Loss = 2.7056045532226562\n",
      "Training-Batch No.15130\n",
      "Loss = 6.34843111038208\n",
      "Training-Batch No.15140\n",
      "Loss = 1.9675326347351074\n",
      "Training-Batch No.15150\n",
      "Loss = 1.9734236001968384\n",
      "Training-Batch No.15160\n",
      "Loss = 2.4731130599975586\n",
      "Training-Batch No.15170\n",
      "Loss = 2.53342342376709\n",
      "Training-Batch No.15180\n",
      "Loss = 2.364504814147949\n",
      "Training-Batch No.15190\n",
      "Loss = 7.2382659912109375\n",
      "Training-Batch No.15200\n",
      "Loss = 1.6864298582077026\n",
      "Training-Batch No.15210\n",
      "Loss = 4.177409648895264\n",
      "Training-Batch No.15220\n",
      "Loss = 1.7400891780853271\n",
      "Training-Batch No.15230\n",
      "Loss = 2.6703858375549316\n",
      "Training-Batch No.15240\n",
      "Loss = 1.7823103666305542\n",
      "Training-Batch No.15250\n",
      "Loss = 1.688628911972046\n",
      "Training-Batch No.15260\n",
      "Loss = 2.4487903118133545\n",
      "Training-Batch No.15270\n",
      "Loss = 2.4269368648529053\n",
      "Training-Batch No.15280\n",
      "Loss = 1.5253816843032837\n",
      "Training-Batch No.15290\n",
      "Loss = 2.6343722343444824\n",
      "Training-Batch No.15300\n",
      "Loss = 2.544872760772705\n",
      "Training-Batch No.15310\n",
      "Loss = 1.2967361211776733\n",
      "Training-Batch No.15320\n",
      "Loss = 3.835986375808716\n",
      "Training-Batch No.15330\n",
      "Loss = 1.8123345375061035\n",
      "Training-Batch No.15340\n",
      "Loss = 2.2822482585906982\n",
      "Training-Batch No.15350\n",
      "Loss = 1.9886828660964966\n",
      "Training-Batch No.15360\n",
      "Loss = 2.577294111251831\n",
      "Training-Batch No.15370\n",
      "Loss = 1.969028353691101\n",
      "Training-Batch No.15380\n",
      "Loss = 3.5364620685577393\n",
      "Training-Batch No.15390\n",
      "Loss = 2.1837666034698486\n",
      "Training-Batch No.15400\n",
      "Loss = 2.4121954441070557\n",
      "Training-Batch No.15410\n",
      "Loss = 2.146355390548706\n",
      "Training-Batch No.15420\n",
      "Loss = 1.667755126953125\n",
      "Training-Batch No.15430\n",
      "Loss = 1.810302734375\n",
      "Training-Batch No.15440\n",
      "Loss = 2.6980557441711426\n",
      "Training-Batch No.15450\n",
      "Loss = 6.4763617515563965\n",
      "Training-Batch No.15460\n",
      "Loss = 1.774234652519226\n",
      "Training-Batch No.15470\n",
      "Loss = 1.9384952783584595\n",
      "Training-Batch No.15480\n",
      "Loss = 1.9372012615203857\n",
      "Training-Batch No.15490\n",
      "Loss = 1.6781284809112549\n",
      "Training-Batch No.15500\n",
      "Loss = 1.401138186454773\n",
      "Training-Batch No.15510\n",
      "Loss = 4.15568733215332\n",
      "Training-Batch No.15520\n",
      "Loss = 2.0307843685150146\n",
      "Training-Batch No.15530\n",
      "Loss = 2.2700462341308594\n",
      "Training-Batch No.15540\n",
      "Loss = 2.2039389610290527\n",
      "Training-Batch No.15550\n",
      "Loss = 1.8464454412460327\n",
      "Training-Batch No.15560\n",
      "Loss = 1.6358479261398315\n",
      "Training-Batch No.15570\n",
      "Loss = 2.542612314224243\n",
      "Training-Batch No.15580\n",
      "Loss = 4.2691569328308105\n",
      "Training-Batch No.15590\n",
      "Loss = 1.667363166809082\n",
      "Training-Batch No.15600\n",
      "Loss = 1.7843692302703857\n",
      "Training-Batch No.15610\n",
      "Loss = 1.8856308460235596\n",
      "Training-Batch No.15620\n",
      "Loss = 2.7387025356292725\n",
      "Training-Batch No.15630\n",
      "Loss = 2.1622262001037598\n",
      "Training-Batch No.15640\n",
      "Loss = 1.795281171798706\n",
      "Training-Batch No.15650\n",
      "Loss = 2.7185845375061035\n",
      "Training-Batch No.15660\n",
      "Loss = 2.4200820922851562\n",
      "Training-Batch No.15670\n",
      "Loss = 1.487659215927124\n",
      "Training-Batch No.15680\n",
      "Loss = 1.9169989824295044\n",
      "Training-Batch No.15690\n",
      "Loss = 2.3913588523864746\n",
      "Training-Batch No.15700\n",
      "Loss = 1.845420479774475\n",
      "Training-Batch No.15710\n",
      "Loss = 2.098580837249756\n",
      "Training-Batch No.15720\n",
      "Loss = 1.2675974369049072\n",
      "Training-Batch No.15730\n",
      "Loss = 2.0305309295654297\n",
      "Training-Batch No.15740\n",
      "Loss = 1.9012161493301392\n",
      "Training-Batch No.15750\n",
      "Loss = 2.6181771755218506\n",
      "Training-Batch No.15760\n",
      "Loss = 2.0852527618408203\n",
      "Training-Batch No.15770\n",
      "Loss = 1.2399526834487915\n",
      "Training-Batch No.15780\n",
      "Loss = 1.9122976064682007\n",
      "Epoch 15 Training Loss: 2.3431\n",
      "Start validation\n",
      "Epoch 15 Validation Loss: 2.9381\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.20922491678554447\n",
      "Average Top-3 accuracy 0.49595815501664287\n",
      "Average Top-5 accuracy 0.6457441749881122\n",
      "Average Top-7 accuracy 0.7308606752258678\n",
      "Average Top-9 accuracy 0.776034236804565\n",
      "Average Top-11 accuracy 0.8135996195910604\n",
      "Average Top-13 accuracy 0.8483119353304802\n",
      "Average Top-15 accuracy 0.8773181169757489\n",
      "current acc 0.20922491678554447\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 16\n",
      "Training-Batch No.15790\n",
      "Loss = 7.533236503601074\n",
      "Training-Batch No.15800\n",
      "Loss = 2.7700324058532715\n",
      "Training-Batch No.15810\n",
      "Loss = 2.429337739944458\n",
      "Training-Batch No.15820\n",
      "Loss = 1.5463112592697144\n",
      "Training-Batch No.15830\n",
      "Loss = 1.6941661834716797\n",
      "Training-Batch No.15840\n",
      "Loss = 2.1096062660217285\n",
      "Training-Batch No.15850\n",
      "Loss = 2.121622085571289\n",
      "Training-Batch No.15860\n",
      "Loss = 2.0836851596832275\n",
      "Training-Batch No.15870\n",
      "Loss = 2.5370593070983887\n",
      "Training-Batch No.15880\n",
      "Loss = 2.224055767059326\n",
      "Training-Batch No.15890\n",
      "Loss = 2.009387969970703\n",
      "Training-Batch No.15900\n",
      "Loss = 5.317590713500977\n",
      "Training-Batch No.15910\n",
      "Loss = 1.9854953289031982\n",
      "Training-Batch No.15920\n",
      "Loss = 2.108271598815918\n",
      "Training-Batch No.15930\n",
      "Loss = 2.0714433193206787\n",
      "Training-Batch No.15940\n",
      "Loss = 2.1978678703308105\n",
      "Training-Batch No.15950\n",
      "Loss = 1.8476991653442383\n",
      "Training-Batch No.15960\n",
      "Loss = 2.6599950790405273\n",
      "Training-Batch No.15970\n",
      "Loss = 2.0331368446350098\n",
      "Training-Batch No.15980\n",
      "Loss = 3.459137201309204\n",
      "Training-Batch No.15990\n",
      "Loss = 1.4021999835968018\n",
      "Training-Batch No.16000\n",
      "Loss = 2.173980236053467\n",
      "Training-Batch No.16010\n",
      "Loss = 1.9157721996307373\n",
      "Training-Batch No.16020\n",
      "Loss = 2.39735746383667\n",
      "Training-Batch No.16030\n",
      "Loss = 1.933206558227539\n",
      "Training-Batch No.16040\n",
      "Loss = 2.0609114170074463\n",
      "Training-Batch No.16050\n",
      "Loss = 2.6825902462005615\n",
      "Training-Batch No.16060\n",
      "Loss = 3.9897429943084717\n",
      "Training-Batch No.16070\n",
      "Loss = 2.4614036083221436\n",
      "Training-Batch No.16080\n",
      "Loss = 2.8179402351379395\n",
      "Training-Batch No.16090\n",
      "Loss = 2.447709798812866\n",
      "Training-Batch No.16100\n",
      "Loss = 2.417141914367676\n",
      "Training-Batch No.16110\n",
      "Loss = 2.496387004852295\n",
      "Training-Batch No.16120\n",
      "Loss = 8.117415428161621\n",
      "Training-Batch No.16130\n",
      "Loss = 2.6779367923736572\n",
      "Training-Batch No.16140\n",
      "Loss = 1.7628960609436035\n",
      "Training-Batch No.16150\n",
      "Loss = 2.8137683868408203\n",
      "Training-Batch No.16160\n",
      "Loss = 2.3693761825561523\n",
      "Training-Batch No.16170\n",
      "Loss = 1.4904768466949463\n",
      "Training-Batch No.16180\n",
      "Loss = 2.911630153656006\n",
      "Training-Batch No.16190\n",
      "Loss = 1.9705047607421875\n",
      "Training-Batch No.16200\n",
      "Loss = 1.6750133037567139\n",
      "Training-Batch No.16210\n",
      "Loss = 1.8840948343276978\n",
      "Training-Batch No.16220\n",
      "Loss = 3.646780490875244\n",
      "Training-Batch No.16230\n",
      "Loss = 1.6668144464492798\n",
      "Training-Batch No.16240\n",
      "Loss = 1.3175349235534668\n",
      "Training-Batch No.16250\n",
      "Loss = 2.4496214389801025\n",
      "Training-Batch No.16260\n",
      "Loss = 2.3196637630462646\n",
      "Training-Batch No.16270\n",
      "Loss = 2.695249080657959\n",
      "Training-Batch No.16280\n",
      "Loss = 1.576062798500061\n",
      "Training-Batch No.16290\n",
      "Loss = 1.7356910705566406\n",
      "Training-Batch No.16300\n",
      "Loss = 2.2556164264678955\n",
      "Training-Batch No.16310\n",
      "Loss = 2.0481581687927246\n",
      "Training-Batch No.16320\n",
      "Loss = 1.949958086013794\n",
      "Training-Batch No.16330\n",
      "Loss = 2.37308931350708\n",
      "Training-Batch No.16340\n",
      "Loss = 1.9761033058166504\n",
      "Training-Batch No.16350\n",
      "Loss = 2.8785290718078613\n",
      "Training-Batch No.16360\n",
      "Loss = 1.9668132066726685\n",
      "Training-Batch No.16370\n",
      "Loss = 2.2050118446350098\n",
      "Training-Batch No.16380\n",
      "Loss = 2.555695056915283\n",
      "Training-Batch No.16390\n",
      "Loss = 2.832335948944092\n",
      "Training-Batch No.16400\n",
      "Loss = 1.772979974746704\n",
      "Training-Batch No.16410\n",
      "Loss = 2.026812791824341\n",
      "Training-Batch No.16420\n",
      "Loss = 2.0810444355010986\n",
      "Training-Batch No.16430\n",
      "Loss = 2.0259203910827637\n",
      "Training-Batch No.16440\n",
      "Loss = 1.6170847415924072\n",
      "Training-Batch No.16450\n",
      "Loss = 2.7847628593444824\n",
      "Training-Batch No.16460\n",
      "Loss = 2.2917847633361816\n",
      "Training-Batch No.16470\n",
      "Loss = 1.4102025032043457\n",
      "Training-Batch No.16480\n",
      "Loss = 1.4048444032669067\n",
      "Training-Batch No.16490\n",
      "Loss = 2.151822566986084\n",
      "Training-Batch No.16500\n",
      "Loss = 5.235489845275879\n",
      "Training-Batch No.16510\n",
      "Loss = 2.210170269012451\n",
      "Training-Batch No.16520\n",
      "Loss = 2.5060291290283203\n",
      "Training-Batch No.16530\n",
      "Loss = 3.2727463245391846\n",
      "Training-Batch No.16540\n",
      "Loss = 3.146223783493042\n",
      "Training-Batch No.16550\n",
      "Loss = 3.0116848945617676\n",
      "Training-Batch No.16560\n",
      "Loss = 2.5323708057403564\n",
      "Training-Batch No.16570\n",
      "Loss = 1.5702974796295166\n",
      "Training-Batch No.16580\n",
      "Loss = 2.51894211769104\n",
      "Training-Batch No.16590\n",
      "Loss = 1.7719287872314453\n",
      "Training-Batch No.16600\n",
      "Loss = 1.7499281167984009\n",
      "Training-Batch No.16610\n",
      "Loss = 8.706050872802734\n",
      "Training-Batch No.16620\n",
      "Loss = 2.316540479660034\n",
      "Training-Batch No.16630\n",
      "Loss = 2.264256477355957\n",
      "Training-Batch No.16640\n",
      "Loss = 3.654804229736328\n",
      "Training-Batch No.16650\n",
      "Loss = 1.7231029272079468\n",
      "Training-Batch No.16660\n",
      "Loss = 1.5475473403930664\n",
      "Training-Batch No.16670\n",
      "Loss = 3.071430206298828\n",
      "Training-Batch No.16680\n",
      "Loss = 1.578416109085083\n",
      "Training-Batch No.16690\n",
      "Loss = 2.0730926990509033\n",
      "Training-Batch No.16700\n",
      "Loss = 2.344787836074829\n",
      "Training-Batch No.16710\n",
      "Loss = 1.498060941696167\n",
      "Training-Batch No.16720\n",
      "Loss = 1.5156234502792358\n",
      "Training-Batch No.16730\n",
      "Loss = 1.6448757648468018\n",
      "Training-Batch No.16740\n",
      "Loss = 2.15708327293396\n",
      "Training-Batch No.16750\n",
      "Loss = 1.8974575996398926\n",
      "Training-Batch No.16760\n",
      "Loss = 1.5177127122879028\n",
      "Training-Batch No.16770\n",
      "Loss = 1.558499813079834\n",
      "Training-Batch No.16780\n",
      "Loss = 1.3384668827056885\n",
      "Training-Batch No.16790\n",
      "Loss = 1.7893543243408203\n",
      "Training-Batch No.16800\n",
      "Loss = 2.2025084495544434\n",
      "Training-Batch No.16810\n",
      "Loss = 1.5209364891052246\n",
      "Training-Batch No.16820\n",
      "Loss = 2.0842766761779785\n",
      "Training-Batch No.16830\n",
      "Loss = 2.477466344833374\n",
      "Epoch 16 Training Loss: 2.3350\n",
      "Start validation\n",
      "Epoch 16 Validation Loss: 2.6688\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.19686162624821682\n",
      "Average Top-3 accuracy 0.543509272467903\n",
      "Average Top-5 accuracy 0.7213504517356157\n",
      "Average Top-7 accuracy 0.816452686638136\n",
      "Average Top-9 accuracy 0.8882548739895387\n",
      "Average Top-11 accuracy 0.91773656680932\n",
      "Average Top-13 accuracy 0.9467427484545887\n",
      "Average Top-15 accuracy 0.9557774607703281\n",
      "current acc 0.19686162624821682\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 17\n",
      "Training-Batch No.16840\n",
      "Loss = 2.3057262897491455\n",
      "Training-Batch No.16850\n",
      "Loss = 1.6023415327072144\n",
      "Training-Batch No.16860\n",
      "Loss = 1.8379936218261719\n",
      "Training-Batch No.16870\n",
      "Loss = 1.661694049835205\n",
      "Training-Batch No.16880\n",
      "Loss = 2.1785638332366943\n",
      "Training-Batch No.16890\n",
      "Loss = 2.426089286804199\n",
      "Training-Batch No.16900\n",
      "Loss = 1.944917917251587\n",
      "Training-Batch No.16910\n",
      "Loss = 3.2001006603240967\n",
      "Training-Batch No.16920\n",
      "Loss = 1.6375974416732788\n",
      "Training-Batch No.16930\n",
      "Loss = 1.35770583152771\n",
      "Training-Batch No.16940\n",
      "Loss = 6.424251079559326\n",
      "Training-Batch No.16950\n",
      "Loss = 1.5748369693756104\n",
      "Training-Batch No.16960\n",
      "Loss = 1.6134809255599976\n",
      "Training-Batch No.16970\n",
      "Loss = 2.1703062057495117\n",
      "Training-Batch No.16980\n",
      "Loss = 1.661210060119629\n",
      "Training-Batch No.16990\n",
      "Loss = 2.585188865661621\n",
      "Training-Batch No.17000\n",
      "Loss = 1.876375436782837\n",
      "Training-Batch No.17010\n",
      "Loss = 2.7875280380249023\n",
      "Training-Batch No.17020\n",
      "Loss = 3.43344783782959\n",
      "Training-Batch No.17030\n",
      "Loss = 2.4830737113952637\n",
      "Training-Batch No.17040\n",
      "Loss = 2.0532355308532715\n",
      "Training-Batch No.17050\n",
      "Loss = 1.725732684135437\n",
      "Training-Batch No.17060\n",
      "Loss = 1.8201286792755127\n",
      "Training-Batch No.17070\n",
      "Loss = 2.3938047885894775\n",
      "Training-Batch No.17080\n",
      "Loss = 3.518721342086792\n",
      "Training-Batch No.17090\n",
      "Loss = 2.0084266662597656\n",
      "Training-Batch No.17100\n",
      "Loss = 1.6556850671768188\n",
      "Training-Batch No.17110\n",
      "Loss = 1.703052043914795\n",
      "Training-Batch No.17120\n",
      "Loss = 1.5370604991912842\n",
      "Training-Batch No.17130\n",
      "Loss = 1.8326703310012817\n",
      "Training-Batch No.17140\n",
      "Loss = 1.933730125427246\n",
      "Training-Batch No.17150\n",
      "Loss = 1.8640153408050537\n",
      "Training-Batch No.17160\n",
      "Loss = 1.9975676536560059\n",
      "Training-Batch No.17170\n",
      "Loss = 1.5761725902557373\n",
      "Training-Batch No.17180\n",
      "Loss = 2.061483383178711\n",
      "Training-Batch No.17190\n",
      "Loss = 1.607602834701538\n",
      "Training-Batch No.17200\n",
      "Loss = 1.6535197496414185\n",
      "Training-Batch No.17210\n",
      "Loss = 2.9784445762634277\n",
      "Training-Batch No.17220\n",
      "Loss = 2.214632987976074\n",
      "Training-Batch No.17230\n",
      "Loss = 1.9940564632415771\n",
      "Training-Batch No.17240\n",
      "Loss = 2.6717052459716797\n",
      "Training-Batch No.17250\n",
      "Loss = 2.0124831199645996\n",
      "Training-Batch No.17260\n",
      "Loss = 1.6485798358917236\n",
      "Training-Batch No.17270\n",
      "Loss = 2.1575815677642822\n",
      "Training-Batch No.17280\n",
      "Loss = 2.617083787918091\n",
      "Training-Batch No.17290\n",
      "Loss = 1.64015793800354\n",
      "Training-Batch No.17300\n",
      "Loss = 2.459991455078125\n",
      "Training-Batch No.17310\n",
      "Loss = 1.4192990064620972\n",
      "Training-Batch No.17320\n",
      "Loss = 2.39372181892395\n",
      "Training-Batch No.17330\n",
      "Loss = 1.2051594257354736\n",
      "Training-Batch No.17340\n",
      "Loss = 1.4547386169433594\n",
      "Training-Batch No.17350\n",
      "Loss = 1.4957795143127441\n",
      "Training-Batch No.17360\n",
      "Loss = 1.9009120464324951\n",
      "Training-Batch No.17370\n",
      "Loss = 1.5746302604675293\n",
      "Training-Batch No.17380\n",
      "Loss = 2.254807710647583\n",
      "Training-Batch No.17390\n",
      "Loss = 3.2314045429229736\n",
      "Training-Batch No.17400\n",
      "Loss = 5.206262588500977\n",
      "Training-Batch No.17410\n",
      "Loss = 1.703978180885315\n",
      "Training-Batch No.17420\n",
      "Loss = 2.2855801582336426\n",
      "Training-Batch No.17430\n",
      "Loss = 2.2214040756225586\n",
      "Training-Batch No.17440\n",
      "Loss = 2.250715970993042\n",
      "Training-Batch No.17450\n",
      "Loss = 1.8922849893569946\n",
      "Training-Batch No.17460\n",
      "Loss = 1.6930248737335205\n",
      "Training-Batch No.17470\n",
      "Loss = 2.8424172401428223\n",
      "Training-Batch No.17480\n",
      "Loss = 2.683852434158325\n",
      "Training-Batch No.17490\n",
      "Loss = 1.753090500831604\n",
      "Training-Batch No.17500\n",
      "Loss = 2.330479621887207\n",
      "Training-Batch No.17510\n",
      "Loss = 2.2068235874176025\n",
      "Training-Batch No.17520\n",
      "Loss = 2.380603075027466\n",
      "Training-Batch No.17530\n",
      "Loss = 1.4618051052093506\n",
      "Training-Batch No.17540\n",
      "Loss = 2.335202217102051\n",
      "Training-Batch No.17550\n",
      "Loss = 1.8257627487182617\n",
      "Training-Batch No.17560\n",
      "Loss = 2.647067070007324\n",
      "Training-Batch No.17570\n",
      "Loss = 2.552124261856079\n",
      "Training-Batch No.17580\n",
      "Loss = 2.5600881576538086\n",
      "Training-Batch No.17590\n",
      "Loss = 2.0144190788269043\n",
      "Training-Batch No.17600\n",
      "Loss = 1.2363293170928955\n",
      "Training-Batch No.17610\n",
      "Loss = 2.340867519378662\n",
      "Training-Batch No.17620\n",
      "Loss = 2.4361536502838135\n",
      "Training-Batch No.17630\n",
      "Loss = 2.4907569885253906\n",
      "Training-Batch No.17640\n",
      "Loss = 1.7707834243774414\n",
      "Training-Batch No.17650\n",
      "Loss = 1.627307415008545\n",
      "Training-Batch No.17660\n",
      "Loss = 2.2966511249542236\n",
      "Training-Batch No.17670\n",
      "Loss = 1.5698010921478271\n",
      "Training-Batch No.17680\n",
      "Loss = 2.8289361000061035\n",
      "Training-Batch No.17690\n",
      "Loss = 3.244623899459839\n",
      "Training-Batch No.17700\n",
      "Loss = 1.5009565353393555\n",
      "Training-Batch No.17710\n",
      "Loss = 2.2913637161254883\n",
      "Training-Batch No.17720\n",
      "Loss = 1.326119065284729\n",
      "Training-Batch No.17730\n",
      "Loss = 2.0782556533813477\n",
      "Training-Batch No.17740\n",
      "Loss = 1.9075933694839478\n",
      "Training-Batch No.17750\n",
      "Loss = 2.1459624767303467\n",
      "Training-Batch No.17760\n",
      "Loss = 1.971886396408081\n",
      "Training-Batch No.17770\n",
      "Loss = 2.656240940093994\n",
      "Training-Batch No.17780\n",
      "Loss = 1.9966861009597778\n",
      "Training-Batch No.17790\n",
      "Loss = 1.7615917921066284\n",
      "Training-Batch No.17800\n",
      "Loss = 1.9051234722137451\n",
      "Training-Batch No.17810\n",
      "Loss = 1.963315486907959\n",
      "Training-Batch No.17820\n",
      "Loss = 2.1885204315185547\n",
      "Training-Batch No.17830\n",
      "Loss = 1.9211249351501465\n",
      "Training-Batch No.17840\n",
      "Loss = 1.266227126121521\n",
      "Training-Batch No.17850\n",
      "Loss = 2.5931813716888428\n",
      "Training-Batch No.17860\n",
      "Loss = 1.8784027099609375\n",
      "Training-Batch No.17870\n",
      "Loss = 1.798545241355896\n",
      "Training-Batch No.17880\n",
      "Loss = 1.8701744079589844\n",
      "Epoch 17 Training Loss: 2.2831\n",
      "Start validation\n",
      "Epoch 17 Validation Loss: 2.5512\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2891107941036614\n",
      "Average Top-3 accuracy 0.6191155492154066\n",
      "Average Top-5 accuracy 0.7574893009985735\n",
      "Average Top-7 accuracy 0.8278649548264384\n",
      "Average Top-9 accuracy 0.8773181169757489\n",
      "Average Top-11 accuracy 0.9229671897289586\n",
      "Average Top-13 accuracy 0.9429386590584878\n",
      "Average Top-15 accuracy 0.9514978601997147\n",
      "current acc 0.2891107941036614\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 18\n",
      "Training-Batch No.17890\n",
      "Loss = 2.372896671295166\n",
      "Training-Batch No.17900\n",
      "Loss = 2.041166305541992\n",
      "Training-Batch No.17910\n",
      "Loss = 1.4710502624511719\n",
      "Training-Batch No.17920\n",
      "Loss = 2.1427361965179443\n",
      "Training-Batch No.17930\n",
      "Loss = 1.5652793645858765\n",
      "Training-Batch No.17940\n",
      "Loss = 2.2161307334899902\n",
      "Training-Batch No.17950\n",
      "Loss = 5.773971080780029\n",
      "Training-Batch No.17960\n",
      "Loss = 1.4156689643859863\n",
      "Training-Batch No.17970\n",
      "Loss = 1.6795108318328857\n",
      "Training-Batch No.17980\n",
      "Loss = 2.9685230255126953\n",
      "Training-Batch No.17990\n",
      "Loss = 2.1112871170043945\n",
      "Training-Batch No.18000\n",
      "Loss = 1.2814884185791016\n",
      "Training-Batch No.18010\n",
      "Loss = 3.656485080718994\n",
      "Training-Batch No.18020\n",
      "Loss = 2.461571216583252\n",
      "Training-Batch No.18030\n",
      "Loss = 2.2462751865386963\n",
      "Training-Batch No.18040\n",
      "Loss = 1.7068355083465576\n",
      "Training-Batch No.18050\n",
      "Loss = 1.6268655061721802\n",
      "Training-Batch No.18060\n",
      "Loss = 1.9886168241500854\n",
      "Training-Batch No.18070\n",
      "Loss = 4.736020565032959\n",
      "Training-Batch No.18080\n",
      "Loss = 2.242269992828369\n",
      "Training-Batch No.18090\n",
      "Loss = 2.8811728954315186\n",
      "Training-Batch No.18100\n",
      "Loss = 2.043189287185669\n",
      "Training-Batch No.18110\n",
      "Loss = 1.3596858978271484\n",
      "Training-Batch No.18120\n",
      "Loss = 1.8480862379074097\n",
      "Training-Batch No.18130\n",
      "Loss = 1.6252074241638184\n",
      "Training-Batch No.18140\n",
      "Loss = 1.9806236028671265\n",
      "Training-Batch No.18150\n",
      "Loss = 1.8453487157821655\n",
      "Training-Batch No.18160\n",
      "Loss = 1.9020915031433105\n",
      "Training-Batch No.18170\n",
      "Loss = 1.632746934890747\n",
      "Training-Batch No.18180\n",
      "Loss = 4.483653545379639\n",
      "Training-Batch No.18190\n",
      "Loss = 1.987971305847168\n",
      "Training-Batch No.18200\n",
      "Loss = 2.029395580291748\n",
      "Training-Batch No.18210\n",
      "Loss = 1.8947458267211914\n",
      "Training-Batch No.18220\n",
      "Loss = 2.4783687591552734\n",
      "Training-Batch No.18230\n",
      "Loss = 2.861724853515625\n",
      "Training-Batch No.18240\n",
      "Loss = 1.9314899444580078\n",
      "Training-Batch No.18250\n",
      "Loss = 1.9767581224441528\n",
      "Training-Batch No.18260\n",
      "Loss = 2.584782600402832\n",
      "Training-Batch No.18270\n",
      "Loss = 2.4604787826538086\n",
      "Training-Batch No.18280\n",
      "Loss = 4.506687641143799\n",
      "Training-Batch No.18290\n",
      "Loss = 2.2662296295166016\n",
      "Training-Batch No.18300\n",
      "Loss = 1.8654696941375732\n",
      "Training-Batch No.18310\n",
      "Loss = 1.9449881315231323\n",
      "Training-Batch No.18320\n",
      "Loss = 1.5758092403411865\n",
      "Training-Batch No.18330\n",
      "Loss = 1.4875292778015137\n",
      "Training-Batch No.18340\n",
      "Loss = 2.143071174621582\n",
      "Training-Batch No.18350\n",
      "Loss = 1.891509771347046\n",
      "Training-Batch No.18360\n",
      "Loss = 2.109210729598999\n",
      "Training-Batch No.18370\n",
      "Loss = 1.6786766052246094\n",
      "Training-Batch No.18380\n",
      "Loss = 1.750685691833496\n",
      "Training-Batch No.18390\n",
      "Loss = 1.998874306678772\n",
      "Training-Batch No.18400\n",
      "Loss = 2.180978298187256\n",
      "Training-Batch No.18410\n",
      "Loss = 1.5842970609664917\n",
      "Training-Batch No.18420\n",
      "Loss = 2.1685895919799805\n",
      "Training-Batch No.18430\n",
      "Loss = 1.2923543453216553\n",
      "Training-Batch No.18440\n",
      "Loss = 1.771217703819275\n",
      "Training-Batch No.18450\n",
      "Loss = 1.6552764177322388\n",
      "Training-Batch No.18460\n",
      "Loss = 1.5661624670028687\n",
      "Training-Batch No.18470\n",
      "Loss = 1.560581922531128\n",
      "Training-Batch No.18480\n",
      "Loss = 1.6066863536834717\n",
      "Training-Batch No.18490\n",
      "Loss = 1.7961983680725098\n",
      "Training-Batch No.18500\n",
      "Loss = 1.7236169576644897\n",
      "Training-Batch No.18510\n",
      "Loss = 1.9220125675201416\n",
      "Training-Batch No.18520\n",
      "Loss = 2.1010520458221436\n",
      "Training-Batch No.18530\n",
      "Loss = 1.5282564163208008\n",
      "Training-Batch No.18540\n",
      "Loss = 1.4599827527999878\n",
      "Training-Batch No.18550\n",
      "Loss = 2.8286097049713135\n",
      "Training-Batch No.18560\n",
      "Loss = 1.6843862533569336\n",
      "Training-Batch No.18570\n",
      "Loss = 2.0154271125793457\n",
      "Training-Batch No.18580\n",
      "Loss = 2.2579233646392822\n",
      "Training-Batch No.18590\n",
      "Loss = 3.2995338439941406\n",
      "Training-Batch No.18600\n",
      "Loss = 2.861482858657837\n",
      "Training-Batch No.18610\n",
      "Loss = 2.3874080181121826\n",
      "Training-Batch No.18620\n",
      "Loss = 1.4148762226104736\n",
      "Training-Batch No.18630\n",
      "Loss = 2.4881227016448975\n",
      "Training-Batch No.18640\n",
      "Loss = 2.0368850231170654\n",
      "Training-Batch No.18650\n",
      "Loss = 2.204279899597168\n",
      "Training-Batch No.18660\n",
      "Loss = 3.3917555809020996\n",
      "Training-Batch No.18670\n",
      "Loss = 2.193096399307251\n",
      "Training-Batch No.18680\n",
      "Loss = 1.6345478296279907\n",
      "Training-Batch No.18690\n",
      "Loss = 1.7106828689575195\n",
      "Training-Batch No.18700\n",
      "Loss = 3.639315605163574\n",
      "Training-Batch No.18710\n",
      "Loss = 1.9982411861419678\n",
      "Training-Batch No.18720\n",
      "Loss = 2.005908966064453\n",
      "Training-Batch No.18730\n",
      "Loss = 2.470763921737671\n",
      "Training-Batch No.18740\n",
      "Loss = 2.1290483474731445\n",
      "Training-Batch No.18750\n",
      "Loss = 4.411622524261475\n",
      "Training-Batch No.18760\n",
      "Loss = 1.5345286130905151\n",
      "Training-Batch No.18770\n",
      "Loss = 1.3314785957336426\n",
      "Training-Batch No.18780\n",
      "Loss = 2.735950469970703\n",
      "Training-Batch No.18790\n",
      "Loss = 1.7154849767684937\n",
      "Training-Batch No.18800\n",
      "Loss = 1.8909814357757568\n",
      "Training-Batch No.18810\n",
      "Loss = 1.7404063940048218\n",
      "Training-Batch No.18820\n",
      "Loss = 1.8594872951507568\n",
      "Training-Batch No.18830\n",
      "Loss = 2.1487810611724854\n",
      "Training-Batch No.18840\n",
      "Loss = 1.279662013053894\n",
      "Training-Batch No.18850\n",
      "Loss = 2.053582191467285\n",
      "Training-Batch No.18860\n",
      "Loss = 1.9605340957641602\n",
      "Training-Batch No.18870\n",
      "Loss = 1.6039482355117798\n",
      "Training-Batch No.18880\n",
      "Loss = 1.5997211933135986\n",
      "Training-Batch No.18890\n",
      "Loss = 1.9465537071228027\n",
      "Training-Batch No.18900\n",
      "Loss = 2.55470609664917\n",
      "Training-Batch No.18910\n",
      "Loss = 1.7960009574890137\n",
      "Training-Batch No.18920\n",
      "Loss = 2.094954490661621\n",
      "Training-Batch No.18930\n",
      "Loss = 2.573923110961914\n",
      "Epoch 18 Training Loss: 2.2718\n",
      "Start validation\n",
      "Epoch 18 Validation Loss: 2.6892\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2315739419876367\n",
      "Average Top-3 accuracy 0.553019495958155\n",
      "Average Top-5 accuracy 0.6899667142177841\n",
      "Average Top-7 accuracy 0.7936281502615311\n",
      "Average Top-9 accuracy 0.8492629576795054\n",
      "Average Top-11 accuracy 0.889205896338564\n",
      "Average Top-13 accuracy 0.9125059438896814\n",
      "Average Top-15 accuracy 0.9324774132192106\n",
      "current acc 0.2315739419876367\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 19\n",
      "Training-Batch No.18940\n",
      "Loss = 2.21547532081604\n",
      "Training-Batch No.18950\n",
      "Loss = 2.6439905166625977\n",
      "Training-Batch No.18960\n",
      "Loss = 2.01393723487854\n",
      "Training-Batch No.18970\n",
      "Loss = 2.091482639312744\n",
      "Training-Batch No.18980\n",
      "Loss = 2.682772159576416\n",
      "Training-Batch No.18990\n",
      "Loss = 2.4306774139404297\n",
      "Training-Batch No.19000\n",
      "Loss = 1.8016799688339233\n",
      "Training-Batch No.19010\n",
      "Loss = 3.281651496887207\n",
      "Training-Batch No.19020\n",
      "Loss = 2.572941780090332\n",
      "Training-Batch No.19030\n",
      "Loss = 2.2859408855438232\n",
      "Training-Batch No.19040\n",
      "Loss = 1.9069480895996094\n",
      "Training-Batch No.19050\n",
      "Loss = 1.7383339405059814\n",
      "Training-Batch No.19060\n",
      "Loss = 1.5842756032943726\n",
      "Training-Batch No.19070\n",
      "Loss = 2.944934368133545\n",
      "Training-Batch No.19080\n",
      "Loss = 1.763789415359497\n",
      "Training-Batch No.19090\n",
      "Loss = 1.4642364978790283\n",
      "Training-Batch No.19100\n",
      "Loss = 3.8057069778442383\n",
      "Training-Batch No.19110\n",
      "Loss = 2.105055332183838\n",
      "Training-Batch No.19120\n",
      "Loss = 2.3750433921813965\n",
      "Training-Batch No.19130\n",
      "Loss = 3.6522397994995117\n",
      "Training-Batch No.19140\n",
      "Loss = 2.700648307800293\n",
      "Training-Batch No.19150\n",
      "Loss = 2.2109718322753906\n",
      "Training-Batch No.19160\n",
      "Loss = 2.267756938934326\n",
      "Training-Batch No.19170\n",
      "Loss = 1.5790375471115112\n",
      "Training-Batch No.19180\n",
      "Loss = 1.799263834953308\n",
      "Training-Batch No.19190\n",
      "Loss = 1.7110596895217896\n",
      "Training-Batch No.19200\n",
      "Loss = 1.5088839530944824\n",
      "Training-Batch No.19210\n",
      "Loss = 2.504690647125244\n",
      "Training-Batch No.19220\n",
      "Loss = 5.916456699371338\n",
      "Training-Batch No.19230\n",
      "Loss = 3.3993749618530273\n",
      "Training-Batch No.19240\n",
      "Loss = 2.1201024055480957\n",
      "Training-Batch No.19250\n",
      "Loss = 2.3114418983459473\n",
      "Training-Batch No.19260\n",
      "Loss = 2.232987403869629\n",
      "Training-Batch No.19270\n",
      "Loss = 1.6490895748138428\n",
      "Training-Batch No.19280\n",
      "Loss = 2.169074058532715\n",
      "Training-Batch No.19290\n",
      "Loss = 2.0371763706207275\n",
      "Training-Batch No.19300\n",
      "Loss = 2.2351906299591064\n",
      "Training-Batch No.19310\n",
      "Loss = 1.7728490829467773\n",
      "Training-Batch No.19320\n",
      "Loss = 3.5493476390838623\n",
      "Training-Batch No.19330\n",
      "Loss = 1.8132579326629639\n",
      "Training-Batch No.19340\n",
      "Loss = 2.19754958152771\n",
      "Training-Batch No.19350\n",
      "Loss = 2.507270336151123\n",
      "Training-Batch No.19360\n",
      "Loss = 2.1426713466644287\n",
      "Training-Batch No.19370\n",
      "Loss = 1.6408095359802246\n",
      "Training-Batch No.19380\n",
      "Loss = 1.2098723649978638\n",
      "Training-Batch No.19390\n",
      "Loss = 2.189927577972412\n",
      "Training-Batch No.19400\n",
      "Loss = 2.3046820163726807\n",
      "Training-Batch No.19410\n",
      "Loss = 2.2362170219421387\n",
      "Training-Batch No.19420\n",
      "Loss = 1.6319454908370972\n",
      "Training-Batch No.19430\n",
      "Loss = 1.9982777833938599\n",
      "Training-Batch No.19440\n",
      "Loss = 1.9721211194992065\n",
      "Training-Batch No.19450\n",
      "Loss = 2.1827473640441895\n",
      "Training-Batch No.19460\n",
      "Loss = 2.271690845489502\n",
      "Training-Batch No.19470\n",
      "Loss = 1.9528393745422363\n",
      "Training-Batch No.19480\n",
      "Loss = 1.4308967590332031\n",
      "Training-Batch No.19490\n",
      "Loss = 5.010833740234375\n",
      "Training-Batch No.19500\n",
      "Loss = 3.6056923866271973\n",
      "Training-Batch No.19510\n",
      "Loss = 4.154513359069824\n",
      "Training-Batch No.19520\n",
      "Loss = 1.44370436668396\n",
      "Training-Batch No.19530\n",
      "Loss = 2.3971922397613525\n",
      "Training-Batch No.19540\n",
      "Loss = 1.5769641399383545\n",
      "Training-Batch No.19550\n",
      "Loss = 1.8388961553573608\n",
      "Training-Batch No.19560\n",
      "Loss = 2.7550833225250244\n",
      "Training-Batch No.19570\n",
      "Loss = 2.622584819793701\n",
      "Training-Batch No.19580\n",
      "Loss = 2.020310878753662\n",
      "Training-Batch No.19590\n",
      "Loss = 2.326643466949463\n",
      "Training-Batch No.19600\n",
      "Loss = 2.352065086364746\n",
      "Training-Batch No.19610\n",
      "Loss = 2.1945371627807617\n",
      "Training-Batch No.19620\n",
      "Loss = 2.1269936561584473\n",
      "Training-Batch No.19630\n",
      "Loss = 3.4870214462280273\n",
      "Training-Batch No.19640\n",
      "Loss = 1.5979695320129395\n",
      "Training-Batch No.19650\n",
      "Loss = 1.5843989849090576\n",
      "Training-Batch No.19660\n",
      "Loss = 2.32488751411438\n",
      "Training-Batch No.19670\n",
      "Loss = 3.061938524246216\n",
      "Training-Batch No.19680\n",
      "Loss = 3.338202714920044\n",
      "Training-Batch No.19690\n",
      "Loss = 1.6699726581573486\n",
      "Training-Batch No.19700\n",
      "Loss = 1.8793480396270752\n",
      "Training-Batch No.19710\n",
      "Loss = 1.7468923330307007\n",
      "Training-Batch No.19720\n",
      "Loss = 1.56356680393219\n",
      "Training-Batch No.19730\n",
      "Loss = 1.5985162258148193\n",
      "Training-Batch No.19740\n",
      "Loss = 2.7539963722229004\n",
      "Training-Batch No.19750\n",
      "Loss = 2.237107276916504\n",
      "Training-Batch No.19760\n",
      "Loss = 1.5936553478240967\n",
      "Training-Batch No.19770\n",
      "Loss = 1.7386850118637085\n",
      "Training-Batch No.19780\n",
      "Loss = 1.6373200416564941\n",
      "Training-Batch No.19790\n",
      "Loss = 1.2284618616104126\n",
      "Training-Batch No.19800\n",
      "Loss = 2.022651433944702\n",
      "Training-Batch No.19810\n",
      "Loss = 4.06650447845459\n",
      "Training-Batch No.19820\n",
      "Loss = 2.141186475753784\n",
      "Training-Batch No.19830\n",
      "Loss = 2.3707189559936523\n",
      "Training-Batch No.19840\n",
      "Loss = 2.5824780464172363\n",
      "Training-Batch No.19850\n",
      "Loss = 2.146450996398926\n",
      "Training-Batch No.19860\n",
      "Loss = 5.082743167877197\n",
      "Training-Batch No.19870\n",
      "Loss = 1.9249755144119263\n",
      "Training-Batch No.19880\n",
      "Loss = 2.666206121444702\n",
      "Training-Batch No.19890\n",
      "Loss = 1.54973304271698\n",
      "Training-Batch No.19900\n",
      "Loss = 2.3853960037231445\n",
      "Training-Batch No.19910\n",
      "Loss = 1.283156156539917\n",
      "Training-Batch No.19920\n",
      "Loss = 1.845396876335144\n",
      "Training-Batch No.19930\n",
      "Loss = 1.523988962173462\n",
      "Training-Batch No.19940\n",
      "Loss = 2.681461811065674\n",
      "Training-Batch No.19950\n",
      "Loss = 2.097076654434204\n",
      "Training-Batch No.19960\n",
      "Loss = 2.235564708709717\n",
      "Training-Batch No.19970\n",
      "Loss = 1.904471516609192\n",
      "Training-Batch No.19980\n",
      "Loss = 1.7121186256408691\n",
      "Epoch 19 Training Loss: 2.2576\n",
      "Start validation\n",
      "Epoch 19 Validation Loss: 3.0035\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.11031859248692344\n",
      "Average Top-3 accuracy 0.39372325249643364\n",
      "Average Top-5 accuracy 0.6000951022349025\n",
      "Average Top-7 accuracy 0.7765097479790775\n",
      "Average Top-9 accuracy 0.8820732287208749\n",
      "Average Top-11 accuracy 0.9300998573466477\n",
      "Average Top-13 accuracy 0.9467427484545887\n",
      "Average Top-15 accuracy 0.9553019495958155\n",
      "current acc 0.11031859248692344\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 20\n",
      "Training-Batch No.19990\n",
      "Loss = 1.6423066854476929\n",
      "Training-Batch No.20000\n",
      "Loss = 3.071824789047241\n",
      "Training-Batch No.20010\n",
      "Loss = 1.6859090328216553\n",
      "Training-Batch No.20020\n",
      "Loss = 4.161616325378418\n",
      "Training-Batch No.20030\n",
      "Loss = 2.7774863243103027\n",
      "Training-Batch No.20040\n",
      "Loss = 1.8743934631347656\n",
      "Training-Batch No.20050\n",
      "Loss = 1.637460708618164\n",
      "Training-Batch No.20060\n",
      "Loss = 1.3667967319488525\n",
      "Training-Batch No.20070\n",
      "Loss = 2.3412985801696777\n",
      "Training-Batch No.20080\n",
      "Loss = 2.466742515563965\n",
      "Training-Batch No.20090\n",
      "Loss = 2.250525951385498\n",
      "Training-Batch No.20100\n",
      "Loss = 1.952685832977295\n",
      "Training-Batch No.20110\n",
      "Loss = 2.081662893295288\n",
      "Training-Batch No.20120\n",
      "Loss = 2.018428325653076\n",
      "Training-Batch No.20130\n",
      "Loss = 2.5253915786743164\n",
      "Training-Batch No.20140\n",
      "Loss = 2.2485597133636475\n",
      "Training-Batch No.20150\n",
      "Loss = 4.290846347808838\n",
      "Training-Batch No.20160\n",
      "Loss = 2.3560190200805664\n",
      "Training-Batch No.20170\n",
      "Loss = 1.3998446464538574\n",
      "Training-Batch No.20180\n",
      "Loss = 2.4288902282714844\n",
      "Training-Batch No.20190\n",
      "Loss = 2.604945659637451\n",
      "Training-Batch No.20200\n",
      "Loss = 2.9591851234436035\n",
      "Training-Batch No.20210\n",
      "Loss = 2.630465030670166\n",
      "Training-Batch No.20220\n",
      "Loss = 1.5349699258804321\n",
      "Training-Batch No.20230\n",
      "Loss = 2.0019917488098145\n",
      "Training-Batch No.20240\n",
      "Loss = 2.012230157852173\n",
      "Training-Batch No.20250\n",
      "Loss = 5.7693891525268555\n",
      "Training-Batch No.20260\n",
      "Loss = 3.515209436416626\n",
      "Training-Batch No.20270\n",
      "Loss = 1.9522058963775635\n",
      "Training-Batch No.20280\n",
      "Loss = 1.9169102907180786\n",
      "Training-Batch No.20290\n",
      "Loss = 3.1768946647644043\n",
      "Training-Batch No.20300\n",
      "Loss = 1.923206090927124\n",
      "Training-Batch No.20310\n",
      "Loss = 1.4865186214447021\n",
      "Training-Batch No.20320\n",
      "Loss = 1.5478254556655884\n",
      "Training-Batch No.20330\n",
      "Loss = 2.438624620437622\n",
      "Training-Batch No.20340\n",
      "Loss = 2.917048931121826\n",
      "Training-Batch No.20350\n",
      "Loss = 1.5497841835021973\n",
      "Training-Batch No.20360\n",
      "Loss = 2.4425344467163086\n",
      "Training-Batch No.20370\n",
      "Loss = 2.552682399749756\n",
      "Training-Batch No.20380\n",
      "Loss = 2.8432581424713135\n",
      "Training-Batch No.20390\n",
      "Loss = 7.3070597648620605\n",
      "Training-Batch No.20400\n",
      "Loss = 1.9612774848937988\n",
      "Training-Batch No.20410\n",
      "Loss = 1.8340544700622559\n",
      "Training-Batch No.20420\n",
      "Loss = 2.4361257553100586\n",
      "Training-Batch No.20430\n",
      "Loss = 2.5571117401123047\n",
      "Training-Batch No.20440\n",
      "Loss = 2.2643675804138184\n",
      "Training-Batch No.20450\n",
      "Loss = 8.224581718444824\n",
      "Training-Batch No.20460\n",
      "Loss = 1.5590177774429321\n",
      "Training-Batch No.20470\n",
      "Loss = 4.14281702041626\n",
      "Training-Batch No.20480\n",
      "Loss = 1.6884090900421143\n",
      "Training-Batch No.20490\n",
      "Loss = 2.5752627849578857\n",
      "Training-Batch No.20500\n",
      "Loss = 1.6435482501983643\n",
      "Training-Batch No.20510\n",
      "Loss = 1.5584803819656372\n",
      "Training-Batch No.20520\n",
      "Loss = 2.366551160812378\n",
      "Training-Batch No.20530\n",
      "Loss = 2.390009880065918\n",
      "Training-Batch No.20540\n",
      "Loss = 1.427123785018921\n",
      "Training-Batch No.20550\n",
      "Loss = 2.461954116821289\n",
      "Training-Batch No.20560\n",
      "Loss = 2.3562331199645996\n",
      "Training-Batch No.20570\n",
      "Loss = 1.390628457069397\n",
      "Training-Batch No.20580\n",
      "Loss = 3.8755087852478027\n",
      "Training-Batch No.20590\n",
      "Loss = 1.749929428100586\n",
      "Training-Batch No.20600\n",
      "Loss = 2.1220104694366455\n",
      "Training-Batch No.20610\n",
      "Loss = 1.91097891330719\n",
      "Training-Batch No.20620\n",
      "Loss = 2.2689642906188965\n",
      "Training-Batch No.20630\n",
      "Loss = 1.8679063320159912\n",
      "Training-Batch No.20640\n",
      "Loss = 3.7590091228485107\n",
      "Training-Batch No.20650\n",
      "Loss = 2.179112434387207\n",
      "Training-Batch No.20660\n",
      "Loss = 2.085566282272339\n",
      "Training-Batch No.20670\n",
      "Loss = 1.9934934377670288\n",
      "Training-Batch No.20680\n",
      "Loss = 1.605466365814209\n",
      "Training-Batch No.20690\n",
      "Loss = 1.8645433187484741\n",
      "Training-Batch No.20700\n",
      "Loss = 2.581055164337158\n",
      "Training-Batch No.20710\n",
      "Loss = 6.762584209442139\n",
      "Training-Batch No.20720\n",
      "Loss = 1.7853072881698608\n",
      "Training-Batch No.20730\n",
      "Loss = 1.9676560163497925\n",
      "Training-Batch No.20740\n",
      "Loss = 1.7854708433151245\n",
      "Training-Batch No.20750\n",
      "Loss = 1.4014897346496582\n",
      "Training-Batch No.20760\n",
      "Loss = 1.584434986114502\n",
      "Training-Batch No.20770\n",
      "Loss = 3.4791457653045654\n",
      "Training-Batch No.20780\n",
      "Loss = 2.086240530014038\n",
      "Training-Batch No.20790\n",
      "Loss = 2.2780885696411133\n",
      "Training-Batch No.20800\n",
      "Loss = 2.373522996902466\n",
      "Training-Batch No.20810\n",
      "Loss = 1.598353385925293\n",
      "Training-Batch No.20820\n",
      "Loss = 1.437408685684204\n",
      "Training-Batch No.20830\n",
      "Loss = 2.3512110710144043\n",
      "Training-Batch No.20840\n",
      "Loss = 4.960672855377197\n",
      "Training-Batch No.20850\n",
      "Loss = 1.7777578830718994\n",
      "Training-Batch No.20860\n",
      "Loss = 1.6970696449279785\n",
      "Training-Batch No.20870\n",
      "Loss = 1.6903806924819946\n",
      "Training-Batch No.20880\n",
      "Loss = 2.56131649017334\n",
      "Training-Batch No.20890\n",
      "Loss = 2.0583510398864746\n",
      "Training-Batch No.20900\n",
      "Loss = 1.8180185556411743\n",
      "Training-Batch No.20910\n",
      "Loss = 2.5403294563293457\n",
      "Training-Batch No.20920\n",
      "Loss = 2.588292121887207\n",
      "Training-Batch No.20930\n",
      "Loss = 1.3534739017486572\n",
      "Training-Batch No.20940\n",
      "Loss = 1.756690502166748\n",
      "Training-Batch No.20950\n",
      "Loss = 2.397676944732666\n",
      "Training-Batch No.20960\n",
      "Loss = 1.7854645252227783\n",
      "Training-Batch No.20970\n",
      "Loss = 1.8550233840942383\n",
      "Training-Batch No.20980\n",
      "Loss = 1.159268856048584\n",
      "Training-Batch No.20990\n",
      "Loss = 1.8498969078063965\n",
      "Training-Batch No.21000\n",
      "Loss = 1.7686212062835693\n",
      "Training-Batch No.21010\n",
      "Loss = 2.47784686088562\n",
      "Training-Batch No.21020\n",
      "Loss = 1.9208887815475464\n",
      "Training-Batch No.21030\n",
      "Loss = 1.3012919425964355\n",
      "Training-Batch No.21040\n",
      "Loss = 1.8098640441894531\n",
      "Epoch 20 Training Loss: 2.2456\n",
      "Start validation\n",
      "Epoch 20 Validation Loss: 3.9460\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.10508796956728483\n",
      "Average Top-3 accuracy 0.28815977175463625\n",
      "Average Top-5 accuracy 0.4427009034712316\n",
      "Average Top-7 accuracy 0.5078459343794579\n",
      "Average Top-9 accuracy 0.5734664764621968\n",
      "Average Top-11 accuracy 0.6110318592486923\n",
      "Average Top-13 accuracy 0.644793152639087\n",
      "Average Top-15 accuracy 0.7013789824060865\n",
      "current acc 0.10508796956728483\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 21\n",
      "Training-Batch No.21050\n",
      "Loss = 7.909650802612305\n",
      "Training-Batch No.21060\n",
      "Loss = 2.5514297485351562\n",
      "Training-Batch No.21070\n",
      "Loss = 2.2463881969451904\n",
      "Training-Batch No.21080\n",
      "Loss = 1.5272352695465088\n",
      "Training-Batch No.21090\n",
      "Loss = 1.6840035915374756\n",
      "Training-Batch No.21100\n",
      "Loss = 2.026827812194824\n",
      "Training-Batch No.21110\n",
      "Loss = 1.8512625694274902\n",
      "Training-Batch No.21120\n",
      "Loss = 2.1562347412109375\n",
      "Training-Batch No.21130\n",
      "Loss = 2.4043219089508057\n",
      "Training-Batch No.21140\n",
      "Loss = 2.0919458866119385\n",
      "Training-Batch No.21150\n",
      "Loss = 1.9867466688156128\n",
      "Training-Batch No.21160\n",
      "Loss = 4.22085428237915\n",
      "Training-Batch No.21170\n",
      "Loss = 1.7914776802062988\n",
      "Training-Batch No.21180\n",
      "Loss = 2.0749175548553467\n",
      "Training-Batch No.21190\n",
      "Loss = 1.82210111618042\n",
      "Training-Batch No.21200\n",
      "Loss = 2.047319173812866\n",
      "Training-Batch No.21210\n",
      "Loss = 1.798457384109497\n",
      "Training-Batch No.21220\n",
      "Loss = 2.5575695037841797\n",
      "Training-Batch No.21230\n",
      "Loss = 1.8978846073150635\n",
      "Training-Batch No.21240\n",
      "Loss = 3.2612407207489014\n",
      "Training-Batch No.21250\n",
      "Loss = 1.3953200578689575\n",
      "Training-Batch No.21260\n",
      "Loss = 2.170928716659546\n",
      "Training-Batch No.21270\n",
      "Loss = 2.0298924446105957\n",
      "Training-Batch No.21280\n",
      "Loss = 2.2289907932281494\n",
      "Training-Batch No.21290\n",
      "Loss = 1.8201680183410645\n",
      "Training-Batch No.21300\n",
      "Loss = 1.980866551399231\n",
      "Training-Batch No.21310\n",
      "Loss = 2.5599184036254883\n",
      "Training-Batch No.21320\n",
      "Loss = 3.391155958175659\n",
      "Training-Batch No.21330\n",
      "Loss = 2.221439838409424\n",
      "Training-Batch No.21340\n",
      "Loss = 1.7340039014816284\n",
      "Training-Batch No.21350\n",
      "Loss = 2.3100156784057617\n",
      "Training-Batch No.21360\n",
      "Loss = 2.1542868614196777\n",
      "Training-Batch No.21370\n",
      "Loss = 2.405258893966675\n",
      "Training-Batch No.21380\n",
      "Loss = 2.7249197959899902\n",
      "Training-Batch No.21390\n",
      "Loss = 1.8251757621765137\n",
      "Training-Batch No.21400\n",
      "Loss = 1.7684452533721924\n",
      "Training-Batch No.21410\n",
      "Loss = 2.5682811737060547\n",
      "Training-Batch No.21420\n",
      "Loss = 1.8755381107330322\n",
      "Training-Batch No.21430\n",
      "Loss = 1.2400798797607422\n",
      "Training-Batch No.21440\n",
      "Loss = 3.059434175491333\n",
      "Training-Batch No.21450\n",
      "Loss = 1.8523496389389038\n",
      "Training-Batch No.21460\n",
      "Loss = 1.804497241973877\n",
      "Training-Batch No.21470\n",
      "Loss = 1.7531278133392334\n",
      "Training-Batch No.21480\n",
      "Loss = 3.523914098739624\n",
      "Training-Batch No.21490\n",
      "Loss = 1.5767295360565186\n",
      "Training-Batch No.21500\n",
      "Loss = 1.2555451393127441\n",
      "Training-Batch No.21510\n",
      "Loss = 2.363157272338867\n",
      "Training-Batch No.21520\n",
      "Loss = 2.2714617252349854\n",
      "Training-Batch No.21530\n",
      "Loss = 2.605490207672119\n",
      "Training-Batch No.21540\n",
      "Loss = 1.4271442890167236\n",
      "Training-Batch No.21550\n",
      "Loss = 1.4763076305389404\n",
      "Training-Batch No.21560\n",
      "Loss = 2.221970796585083\n",
      "Training-Batch No.21570\n",
      "Loss = 2.0150115489959717\n",
      "Training-Batch No.21580\n",
      "Loss = 1.9002225399017334\n",
      "Training-Batch No.21590\n",
      "Loss = 2.4543569087982178\n",
      "Training-Batch No.21600\n",
      "Loss = 2.0866851806640625\n",
      "Training-Batch No.21610\n",
      "Loss = 2.8042588233947754\n",
      "Training-Batch No.21620\n",
      "Loss = 1.9902526140213013\n",
      "Training-Batch No.21630\n",
      "Loss = 2.0692625045776367\n",
      "Training-Batch No.21640\n",
      "Loss = 2.510307788848877\n",
      "Training-Batch No.21650\n",
      "Loss = 2.823495626449585\n",
      "Training-Batch No.21660\n",
      "Loss = 1.6991775035858154\n",
      "Training-Batch No.21670\n",
      "Loss = 1.9144304990768433\n",
      "Training-Batch No.21680\n",
      "Loss = 1.910014271736145\n",
      "Training-Batch No.21690\n",
      "Loss = 1.8091610670089722\n",
      "Training-Batch No.21700\n",
      "Loss = 1.5976133346557617\n",
      "Training-Batch No.21710\n",
      "Loss = 2.6360459327697754\n",
      "Training-Batch No.21720\n",
      "Loss = 1.9045631885528564\n",
      "Training-Batch No.21730\n",
      "Loss = 1.2751615047454834\n",
      "Training-Batch No.21740\n",
      "Loss = 1.3389338254928589\n",
      "Training-Batch No.21750\n",
      "Loss = 2.355708122253418\n",
      "Training-Batch No.21760\n",
      "Loss = 6.8016157150268555\n",
      "Training-Batch No.21770\n",
      "Loss = 2.0193309783935547\n",
      "Training-Batch No.21780\n",
      "Loss = 2.4257431030273438\n",
      "Training-Batch No.21790\n",
      "Loss = 2.616837978363037\n",
      "Training-Batch No.21800\n",
      "Loss = 3.543888568878174\n",
      "Training-Batch No.21810\n",
      "Loss = 3.4247090816497803\n",
      "Training-Batch No.21820\n",
      "Loss = 2.3121280670166016\n",
      "Training-Batch No.21830\n",
      "Loss = 1.5796326398849487\n",
      "Training-Batch No.21840\n",
      "Loss = 2.3793787956237793\n",
      "Training-Batch No.21850\n",
      "Loss = 1.5900365114212036\n",
      "Training-Batch No.21860\n",
      "Loss = 1.7214268445968628\n",
      "Training-Batch No.21870\n",
      "Loss = 9.14370059967041\n",
      "Training-Batch No.21880\n",
      "Loss = 2.3037109375\n",
      "Training-Batch No.21890\n",
      "Loss = 2.0688729286193848\n",
      "Training-Batch No.21900\n",
      "Loss = 3.4803919792175293\n",
      "Training-Batch No.21910\n",
      "Loss = 1.8211815357208252\n",
      "Training-Batch No.21920\n",
      "Loss = 1.5122841596603394\n",
      "Training-Batch No.21930\n",
      "Loss = 3.068488836288452\n",
      "Training-Batch No.21940\n",
      "Loss = 1.458240270614624\n",
      "Training-Batch No.21950\n",
      "Loss = 1.9893457889556885\n",
      "Training-Batch No.21960\n",
      "Loss = 2.3718819618225098\n",
      "Training-Batch No.21970\n",
      "Loss = 1.475679874420166\n",
      "Training-Batch No.21980\n",
      "Loss = 1.528179407119751\n",
      "Training-Batch No.21990\n",
      "Loss = 1.6244399547576904\n",
      "Training-Batch No.22000\n",
      "Loss = 2.0763611793518066\n",
      "Training-Batch No.22010\n",
      "Loss = 1.961648941040039\n",
      "Training-Batch No.22020\n",
      "Loss = 1.5365498065948486\n",
      "Training-Batch No.22030\n",
      "Loss = 1.3570427894592285\n",
      "Training-Batch No.22040\n",
      "Loss = 1.2778044939041138\n",
      "Training-Batch No.22050\n",
      "Loss = 1.6501365900039673\n",
      "Training-Batch No.22060\n",
      "Loss = 2.1529700756073\n",
      "Training-Batch No.22070\n",
      "Loss = 1.4650206565856934\n",
      "Training-Batch No.22080\n",
      "Loss = 2.021177291870117\n",
      "Training-Batch No.22090\n",
      "Loss = 2.2825193405151367\n",
      "Epoch 21 Training Loss: 2.2285\n",
      "Start validation\n",
      "Epoch 21 Validation Loss: 4.5747\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.07180218735140276\n",
      "Average Top-3 accuracy 0.24251069900142652\n",
      "Average Top-5 accuracy 0.39752734189253447\n",
      "Average Top-7 accuracy 0.4992867332382311\n",
      "Average Top-9 accuracy 0.5615786970993818\n",
      "Average Top-11 accuracy 0.6176890156918687\n",
      "Average Top-13 accuracy 0.6652401331431289\n",
      "Average Top-15 accuracy 0.6994769377080361\n",
      "current acc 0.07180218735140276\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 22\n",
      "Training-Batch No.22100\n",
      "Loss = 2.1533217430114746\n",
      "Training-Batch No.22110\n",
      "Loss = 1.874817132949829\n",
      "Training-Batch No.22120\n",
      "Loss = 1.660011649131775\n",
      "Training-Batch No.22130\n",
      "Loss = 1.8541269302368164\n",
      "Training-Batch No.22140\n",
      "Loss = 1.9835751056671143\n",
      "Training-Batch No.22150\n",
      "Loss = 2.254904270172119\n",
      "Training-Batch No.22160\n",
      "Loss = 1.7651880979537964\n",
      "Training-Batch No.22170\n",
      "Loss = 3.3283276557922363\n",
      "Training-Batch No.22180\n",
      "Loss = 1.7643016576766968\n",
      "Training-Batch No.22190\n",
      "Loss = 1.3677045106887817\n",
      "Training-Batch No.22200\n",
      "Loss = 5.948928356170654\n",
      "Training-Batch No.22210\n",
      "Loss = 1.585845708847046\n",
      "Training-Batch No.22220\n",
      "Loss = 1.6204054355621338\n",
      "Training-Batch No.22230\n",
      "Loss = 2.081864833831787\n",
      "Training-Batch No.22240\n",
      "Loss = 1.6562142372131348\n",
      "Training-Batch No.22250\n",
      "Loss = 2.4218862056732178\n",
      "Training-Batch No.22260\n",
      "Loss = 1.8085769414901733\n",
      "Training-Batch No.22270\n",
      "Loss = 2.8185882568359375\n",
      "Training-Batch No.22280\n",
      "Loss = 3.501612424850464\n",
      "Training-Batch No.22290\n",
      "Loss = 2.254423141479492\n",
      "Training-Batch No.22300\n",
      "Loss = 1.9273207187652588\n",
      "Training-Batch No.22310\n",
      "Loss = 1.6683778762817383\n",
      "Training-Batch No.22320\n",
      "Loss = 1.7357772588729858\n",
      "Training-Batch No.22330\n",
      "Loss = 2.3778815269470215\n",
      "Training-Batch No.22340\n",
      "Loss = 3.831834077835083\n",
      "Training-Batch No.22350\n",
      "Loss = 1.9440343379974365\n",
      "Training-Batch No.22360\n",
      "Loss = 1.6293439865112305\n",
      "Training-Batch No.22370\n",
      "Loss = 1.6093424558639526\n",
      "Training-Batch No.22380\n",
      "Loss = 1.7758756875991821\n",
      "Training-Batch No.22390\n",
      "Loss = 1.8807706832885742\n",
      "Training-Batch No.22400\n",
      "Loss = 1.9375942945480347\n",
      "Training-Batch No.22410\n",
      "Loss = 1.728124737739563\n",
      "Training-Batch No.22420\n",
      "Loss = 1.8446441888809204\n",
      "Training-Batch No.22430\n",
      "Loss = 1.4711419343948364\n",
      "Training-Batch No.22440\n",
      "Loss = 2.097896099090576\n",
      "Training-Batch No.22450\n",
      "Loss = 1.637272834777832\n",
      "Training-Batch No.22460\n",
      "Loss = 1.4750843048095703\n",
      "Training-Batch No.22470\n",
      "Loss = 2.669203758239746\n",
      "Training-Batch No.22480\n",
      "Loss = 2.1584320068359375\n",
      "Training-Batch No.22490\n",
      "Loss = 1.9304310083389282\n",
      "Training-Batch No.22500\n",
      "Loss = 2.149568557739258\n",
      "Training-Batch No.22510\n",
      "Loss = 1.7614840269088745\n",
      "Training-Batch No.22520\n",
      "Loss = 1.6337192058563232\n",
      "Training-Batch No.22530\n",
      "Loss = 2.143908739089966\n",
      "Training-Batch No.22540\n",
      "Loss = 3.0702126026153564\n",
      "Training-Batch No.22550\n",
      "Loss = 1.5804531574249268\n",
      "Training-Batch No.22560\n",
      "Loss = 2.4736812114715576\n",
      "Training-Batch No.22570\n",
      "Loss = 1.3961960077285767\n",
      "Training-Batch No.22580\n",
      "Loss = 2.16717791557312\n",
      "Training-Batch No.22590\n",
      "Loss = 1.2800687551498413\n",
      "Training-Batch No.22600\n",
      "Loss = 1.3519363403320312\n",
      "Training-Batch No.22610\n",
      "Loss = 1.6558254957199097\n",
      "Training-Batch No.22620\n",
      "Loss = 1.8148161172866821\n",
      "Training-Batch No.22630\n",
      "Loss = 1.5134756565093994\n",
      "Training-Batch No.22640\n",
      "Loss = 2.1201157569885254\n",
      "Training-Batch No.22650\n",
      "Loss = 3.2047340869903564\n",
      "Training-Batch No.22660\n",
      "Loss = 3.962893486022949\n",
      "Training-Batch No.22670\n",
      "Loss = 1.5470002889633179\n",
      "Training-Batch No.22680\n",
      "Loss = 2.2806944847106934\n",
      "Training-Batch No.22690\n",
      "Loss = 1.9623525142669678\n",
      "Training-Batch No.22700\n",
      "Loss = 2.0607359409332275\n",
      "Training-Batch No.22710\n",
      "Loss = 1.7415003776550293\n",
      "Training-Batch No.22720\n",
      "Loss = 1.757226586341858\n",
      "Training-Batch No.22730\n",
      "Loss = 2.757925510406494\n",
      "Training-Batch No.22740\n",
      "Loss = 2.358847141265869\n",
      "Training-Batch No.22750\n",
      "Loss = 1.759044885635376\n",
      "Training-Batch No.22760\n",
      "Loss = 1.9353662729263306\n",
      "Training-Batch No.22770\n",
      "Loss = 2.1263628005981445\n",
      "Training-Batch No.22780\n",
      "Loss = 2.468994140625\n",
      "Training-Batch No.22790\n",
      "Loss = 1.5238699913024902\n",
      "Training-Batch No.22800\n",
      "Loss = 2.282627582550049\n",
      "Training-Batch No.22810\n",
      "Loss = 1.2923377752304077\n",
      "Training-Batch No.22820\n",
      "Loss = 2.7198352813720703\n",
      "Training-Batch No.22830\n",
      "Loss = 2.264517307281494\n",
      "Training-Batch No.22840\n",
      "Loss = 2.235102653503418\n",
      "Training-Batch No.22850\n",
      "Loss = 2.005096673965454\n",
      "Training-Batch No.22860\n",
      "Loss = 1.160204529762268\n",
      "Training-Batch No.22870\n",
      "Loss = 2.1523115634918213\n",
      "Training-Batch No.22880\n",
      "Loss = 2.2953271865844727\n",
      "Training-Batch No.22890\n",
      "Loss = 2.3727715015411377\n",
      "Training-Batch No.22900\n",
      "Loss = 1.5819669961929321\n",
      "Training-Batch No.22910\n",
      "Loss = 1.5832403898239136\n",
      "Training-Batch No.22920\n",
      "Loss = 2.216588020324707\n",
      "Training-Batch No.22930\n",
      "Loss = 1.402847170829773\n",
      "Training-Batch No.22940\n",
      "Loss = 2.8134422302246094\n",
      "Training-Batch No.22950\n",
      "Loss = 3.035414218902588\n",
      "Training-Batch No.22960\n",
      "Loss = 1.394601583480835\n",
      "Training-Batch No.22970\n",
      "Loss = 2.2179715633392334\n",
      "Training-Batch No.22980\n",
      "Loss = 1.2835755348205566\n",
      "Training-Batch No.22990\n",
      "Loss = 1.924939751625061\n",
      "Training-Batch No.23000\n",
      "Loss = 1.9828301668167114\n",
      "Training-Batch No.23010\n",
      "Loss = 2.0906991958618164\n",
      "Training-Batch No.23020\n",
      "Loss = 1.8081235885620117\n",
      "Training-Batch No.23030\n",
      "Loss = 2.714852809906006\n",
      "Training-Batch No.23040\n",
      "Loss = 1.8363189697265625\n",
      "Training-Batch No.23050\n",
      "Loss = 1.711418628692627\n",
      "Training-Batch No.23060\n",
      "Loss = 1.8186204433441162\n",
      "Training-Batch No.23070\n",
      "Loss = 1.8963690996170044\n",
      "Training-Batch No.23080\n",
      "Loss = 2.012369155883789\n",
      "Training-Batch No.23090\n",
      "Loss = 1.9216257333755493\n",
      "Training-Batch No.23100\n",
      "Loss = 1.264380693435669\n",
      "Training-Batch No.23110\n",
      "Loss = 2.737454414367676\n",
      "Training-Batch No.23120\n",
      "Loss = 1.877091407775879\n",
      "Training-Batch No.23130\n",
      "Loss = 1.771790623664856\n",
      "Training-Batch No.23140\n",
      "Loss = 1.8191590309143066\n",
      "Epoch 22 Training Loss: 2.2169\n",
      "Start validation\n",
      "Epoch 22 Validation Loss: 5.3166\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.038040893961008085\n",
      "Average Top-3 accuracy 0.08273894436519258\n",
      "Average Top-5 accuracy 0.1355206847360913\n",
      "Average Top-7 accuracy 0.20161673799334284\n",
      "Average Top-9 accuracy 0.2691393247741322\n",
      "Average Top-11 accuracy 0.3204945316214931\n",
      "Average Top-13 accuracy 0.41844983357108895\n",
      "Average Top-15 accuracy 0.4902520209224917\n",
      "current acc 0.038040893961008085\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 23\n",
      "Training-Batch No.23150\n",
      "Loss = 2.2808799743652344\n",
      "Training-Batch No.23160\n",
      "Loss = 1.9750674962997437\n",
      "Training-Batch No.23170\n",
      "Loss = 1.4929330348968506\n",
      "Training-Batch No.23180\n",
      "Loss = 1.9848965406417847\n",
      "Training-Batch No.23190\n",
      "Loss = 1.4189426898956299\n",
      "Training-Batch No.23200\n",
      "Loss = 2.14785099029541\n",
      "Training-Batch No.23210\n",
      "Loss = 6.091506004333496\n",
      "Training-Batch No.23220\n",
      "Loss = 1.4147164821624756\n",
      "Training-Batch No.23230\n",
      "Loss = 1.6513980627059937\n",
      "Training-Batch No.23240\n",
      "Loss = 3.121708631515503\n",
      "Training-Batch No.23250\n",
      "Loss = 1.9961559772491455\n",
      "Training-Batch No.23260\n",
      "Loss = 1.2764629125595093\n",
      "Training-Batch No.23270\n",
      "Loss = 3.615476608276367\n",
      "Training-Batch No.23280\n",
      "Loss = 2.3968958854675293\n",
      "Training-Batch No.23290\n",
      "Loss = 1.9172890186309814\n",
      "Training-Batch No.23300\n",
      "Loss = 1.6067559719085693\n",
      "Training-Batch No.23310\n",
      "Loss = 1.455726146697998\n",
      "Training-Batch No.23320\n",
      "Loss = 1.85099458694458\n",
      "Training-Batch No.23330\n",
      "Loss = 4.295223712921143\n",
      "Training-Batch No.23340\n",
      "Loss = 1.9775745868682861\n",
      "Training-Batch No.23350\n",
      "Loss = 2.765190362930298\n",
      "Training-Batch No.23360\n",
      "Loss = 1.9153971672058105\n",
      "Training-Batch No.23370\n",
      "Loss = 1.2343932390213013\n",
      "Training-Batch No.23380\n",
      "Loss = 1.7458475828170776\n",
      "Training-Batch No.23390\n",
      "Loss = 1.5673670768737793\n",
      "Training-Batch No.23400\n",
      "Loss = 1.98086416721344\n",
      "Training-Batch No.23410\n",
      "Loss = 1.7980351448059082\n",
      "Training-Batch No.23420\n",
      "Loss = 1.8227548599243164\n",
      "Training-Batch No.23430\n",
      "Loss = 1.5746304988861084\n",
      "Training-Batch No.23440\n",
      "Loss = 4.415637016296387\n",
      "Training-Batch No.23450\n",
      "Loss = 1.917881965637207\n",
      "Training-Batch No.23460\n",
      "Loss = 1.965651273727417\n",
      "Training-Batch No.23470\n",
      "Loss = 1.7205522060394287\n",
      "Training-Batch No.23480\n",
      "Loss = 2.606199264526367\n",
      "Training-Batch No.23490\n",
      "Loss = 2.603898286819458\n",
      "Training-Batch No.23500\n",
      "Loss = 1.6363179683685303\n",
      "Training-Batch No.23510\n",
      "Loss = 1.8629662990570068\n",
      "Training-Batch No.23520\n",
      "Loss = 2.4653468132019043\n",
      "Training-Batch No.23530\n",
      "Loss = 2.2988357543945312\n",
      "Training-Batch No.23540\n",
      "Loss = 5.404820442199707\n",
      "Training-Batch No.23550\n",
      "Loss = 2.3117165565490723\n",
      "Training-Batch No.23560\n",
      "Loss = 1.7717726230621338\n",
      "Training-Batch No.23570\n",
      "Loss = 1.84126877784729\n",
      "Training-Batch No.23580\n",
      "Loss = 1.6749463081359863\n",
      "Training-Batch No.23590\n",
      "Loss = 1.4844759702682495\n",
      "Training-Batch No.23600\n",
      "Loss = 2.1503524780273438\n",
      "Training-Batch No.23610\n",
      "Loss = 1.7392264604568481\n",
      "Training-Batch No.23620\n",
      "Loss = 1.8388835191726685\n",
      "Training-Batch No.23630\n",
      "Loss = 1.3303990364074707\n",
      "Training-Batch No.23640\n",
      "Loss = 1.708461046218872\n",
      "Training-Batch No.23650\n",
      "Loss = 1.8789801597595215\n",
      "Training-Batch No.23660\n",
      "Loss = 2.1142923831939697\n",
      "Training-Batch No.23670\n",
      "Loss = 1.5130585432052612\n",
      "Training-Batch No.23680\n",
      "Loss = 2.1091299057006836\n",
      "Training-Batch No.23690\n",
      "Loss = 1.2840089797973633\n",
      "Training-Batch No.23700\n",
      "Loss = 1.7723811864852905\n",
      "Training-Batch No.23710\n",
      "Loss = 1.5559532642364502\n",
      "Training-Batch No.23720\n",
      "Loss = 1.4461616277694702\n",
      "Training-Batch No.23730\n",
      "Loss = 1.49799644947052\n",
      "Training-Batch No.23740\n",
      "Loss = 1.5427210330963135\n",
      "Training-Batch No.23750\n",
      "Loss = 1.82940673828125\n",
      "Training-Batch No.23760\n",
      "Loss = 1.9704145193099976\n",
      "Training-Batch No.23770\n",
      "Loss = 1.9169443845748901\n",
      "Training-Batch No.23780\n",
      "Loss = 2.0338897705078125\n",
      "Training-Batch No.23790\n",
      "Loss = 1.482593297958374\n",
      "Training-Batch No.23800\n",
      "Loss = 1.4622050523757935\n",
      "Training-Batch No.23810\n",
      "Loss = 2.8059637546539307\n",
      "Training-Batch No.23820\n",
      "Loss = 1.6773134469985962\n",
      "Training-Batch No.23830\n",
      "Loss = 1.5586726665496826\n",
      "Training-Batch No.23840\n",
      "Loss = 2.179119110107422\n",
      "Training-Batch No.23850\n",
      "Loss = 3.032824754714966\n",
      "Training-Batch No.23860\n",
      "Loss = 2.729358434677124\n",
      "Training-Batch No.23870\n",
      "Loss = 2.8947227001190186\n",
      "Training-Batch No.23880\n",
      "Loss = 1.339565396308899\n",
      "Training-Batch No.23890\n",
      "Loss = 2.3163540363311768\n",
      "Training-Batch No.23900\n",
      "Loss = 2.2116053104400635\n",
      "Training-Batch No.23910\n",
      "Loss = 2.1535520553588867\n",
      "Training-Batch No.23920\n",
      "Loss = 3.3983259201049805\n",
      "Training-Batch No.23930\n",
      "Loss = 2.053199291229248\n",
      "Training-Batch No.23940\n",
      "Loss = 1.6298770904541016\n",
      "Training-Batch No.23950\n",
      "Loss = 1.581181526184082\n",
      "Training-Batch No.23960\n",
      "Loss = 3.145127773284912\n",
      "Training-Batch No.23970\n",
      "Loss = 2.1816444396972656\n",
      "Training-Batch No.23980\n",
      "Loss = 2.1525607109069824\n",
      "Training-Batch No.23990\n",
      "Loss = 2.553791046142578\n",
      "Training-Batch No.24000\n",
      "Loss = 2.1393513679504395\n",
      "Training-Batch No.24010\n",
      "Loss = 4.524624824523926\n",
      "Training-Batch No.24020\n",
      "Loss = 1.518888235092163\n",
      "Training-Batch No.24030\n",
      "Loss = 1.3142685890197754\n",
      "Training-Batch No.24040\n",
      "Loss = 3.020481586456299\n",
      "Training-Batch No.24050\n",
      "Loss = 1.6927781105041504\n",
      "Training-Batch No.24060\n",
      "Loss = 1.8349740505218506\n",
      "Training-Batch No.24070\n",
      "Loss = 1.615082859992981\n",
      "Training-Batch No.24080\n",
      "Loss = 1.774522304534912\n",
      "Training-Batch No.24090\n",
      "Loss = 2.3690690994262695\n",
      "Training-Batch No.24100\n",
      "Loss = 1.261934757232666\n",
      "Training-Batch No.24110\n",
      "Loss = 1.9239662885665894\n",
      "Training-Batch No.24120\n",
      "Loss = 1.7840509414672852\n",
      "Training-Batch No.24130\n",
      "Loss = 1.539860486984253\n",
      "Training-Batch No.24140\n",
      "Loss = 1.6063473224639893\n",
      "Training-Batch No.24150\n",
      "Loss = 1.7284092903137207\n",
      "Training-Batch No.24160\n",
      "Loss = 2.529855728149414\n",
      "Training-Batch No.24170\n",
      "Loss = 1.6975202560424805\n",
      "Training-Batch No.24180\n",
      "Loss = 2.007269859313965\n",
      "Training-Batch No.24190\n",
      "Loss = 2.5778236389160156\n",
      "Epoch 23 Training Loss: 2.2064\n",
      "Start validation\n",
      "Epoch 23 Validation Loss: 3.1283\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.11887779362815026\n",
      "Average Top-3 accuracy 0.34236804564907275\n",
      "Average Top-5 accuracy 0.5168806466951973\n",
      "Average Top-7 accuracy 0.6543033761293391\n",
      "Average Top-9 accuracy 0.7641464574417499\n",
      "Average Top-11 accuracy 0.8373751783166904\n",
      "Average Top-13 accuracy 0.8773181169757489\n",
      "Average Top-15 accuracy 0.9144079885877318\n",
      "current acc 0.11887779362815026\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 24\n",
      "Training-Batch No.24200\n",
      "Loss = 2.204314947128296\n",
      "Training-Batch No.24210\n",
      "Loss = 2.539839744567871\n",
      "Training-Batch No.24220\n",
      "Loss = 2.828730583190918\n",
      "Training-Batch No.24230\n",
      "Loss = 2.0887227058410645\n",
      "Training-Batch No.24240\n",
      "Loss = 2.4400036334991455\n",
      "Training-Batch No.24250\n",
      "Loss = 2.2063939571380615\n",
      "Training-Batch No.24260\n",
      "Loss = 1.7996690273284912\n",
      "Training-Batch No.24270\n",
      "Loss = 2.889683961868286\n",
      "Training-Batch No.24280\n",
      "Loss = 2.4659194946289062\n",
      "Training-Batch No.24290\n",
      "Loss = 2.151029109954834\n",
      "Training-Batch No.24300\n",
      "Loss = 1.8923314809799194\n",
      "Training-Batch No.24310\n",
      "Loss = 1.7149853706359863\n",
      "Training-Batch No.24320\n",
      "Loss = 1.3887966871261597\n",
      "Training-Batch No.24330\n",
      "Loss = 2.5829601287841797\n",
      "Training-Batch No.24340\n",
      "Loss = 1.6534117460250854\n",
      "Training-Batch No.24350\n",
      "Loss = 1.568497896194458\n",
      "Training-Batch No.24360\n",
      "Loss = 3.7447588443756104\n",
      "Training-Batch No.24370\n",
      "Loss = 2.0846521854400635\n",
      "Training-Batch No.24380\n",
      "Loss = 2.459608554840088\n",
      "Training-Batch No.24390\n",
      "Loss = 3.6927051544189453\n",
      "Training-Batch No.24400\n",
      "Loss = 3.07295823097229\n",
      "Training-Batch No.24410\n",
      "Loss = 2.1080198287963867\n",
      "Training-Batch No.24420\n",
      "Loss = 2.1105804443359375\n",
      "Training-Batch No.24430\n",
      "Loss = 1.5942964553833008\n",
      "Training-Batch No.24440\n",
      "Loss = 1.7479140758514404\n",
      "Training-Batch No.24450\n",
      "Loss = 1.732755184173584\n",
      "Training-Batch No.24460\n",
      "Loss = 1.4563279151916504\n",
      "Training-Batch No.24470\n",
      "Loss = 2.4566962718963623\n",
      "Training-Batch No.24480\n",
      "Loss = 7.421780586242676\n",
      "Training-Batch No.24490\n",
      "Loss = 3.2213568687438965\n",
      "Training-Batch No.24500\n",
      "Loss = 1.938320517539978\n",
      "Training-Batch No.24510\n",
      "Loss = 2.2913055419921875\n",
      "Training-Batch No.24520\n",
      "Loss = 2.0037004947662354\n",
      "Training-Batch No.24530\n",
      "Loss = 1.7102035284042358\n",
      "Training-Batch No.24540\n",
      "Loss = 2.08516526222229\n",
      "Training-Batch No.24550\n",
      "Loss = 2.307553291320801\n",
      "Training-Batch No.24560\n",
      "Loss = 2.374685525894165\n",
      "Training-Batch No.24570\n",
      "Loss = 1.6995724439620972\n",
      "Training-Batch No.24580\n",
      "Loss = 3.8989105224609375\n",
      "Training-Batch No.24590\n",
      "Loss = 1.8574466705322266\n",
      "Training-Batch No.24600\n",
      "Loss = 2.1439766883850098\n",
      "Training-Batch No.24610\n",
      "Loss = 2.2748351097106934\n",
      "Training-Batch No.24620\n",
      "Loss = 2.1137890815734863\n",
      "Training-Batch No.24630\n",
      "Loss = 1.5332056283950806\n",
      "Training-Batch No.24640\n",
      "Loss = 1.0441761016845703\n",
      "Training-Batch No.24650\n",
      "Loss = 2.205026865005493\n",
      "Training-Batch No.24660\n",
      "Loss = 2.1620311737060547\n",
      "Training-Batch No.24670\n",
      "Loss = 2.168644428253174\n",
      "Training-Batch No.24680\n",
      "Loss = 1.5626437664031982\n",
      "Training-Batch No.24690\n",
      "Loss = 1.936408281326294\n",
      "Training-Batch No.24700\n",
      "Loss = 1.9667738676071167\n",
      "Training-Batch No.24710\n",
      "Loss = 2.2637085914611816\n",
      "Training-Batch No.24720\n",
      "Loss = 2.0834131240844727\n",
      "Training-Batch No.24730\n",
      "Loss = 2.1378438472747803\n",
      "Training-Batch No.24740\n",
      "Loss = 1.4133704900741577\n",
      "Training-Batch No.24750\n",
      "Loss = 4.9742112159729\n",
      "Training-Batch No.24760\n",
      "Loss = 3.8614144325256348\n",
      "Training-Batch No.24770\n",
      "Loss = 3.9120585918426514\n",
      "Training-Batch No.24780\n",
      "Loss = 1.3141992092132568\n",
      "Training-Batch No.24790\n",
      "Loss = 2.2595174312591553\n",
      "Training-Batch No.24800\n",
      "Loss = 1.6226403713226318\n",
      "Training-Batch No.24810\n",
      "Loss = 1.66399085521698\n",
      "Training-Batch No.24820\n",
      "Loss = 2.736198902130127\n",
      "Training-Batch No.24830\n",
      "Loss = 2.585000514984131\n",
      "Training-Batch No.24840\n",
      "Loss = 1.9977233409881592\n",
      "Training-Batch No.24850\n",
      "Loss = 2.1026225090026855\n",
      "Training-Batch No.24860\n",
      "Loss = 2.246453285217285\n",
      "Training-Batch No.24870\n",
      "Loss = 2.0361642837524414\n",
      "Training-Batch No.24880\n",
      "Loss = 1.9697593450546265\n",
      "Training-Batch No.24890\n",
      "Loss = 3.095996856689453\n",
      "Training-Batch No.24900\n",
      "Loss = 1.5637489557266235\n",
      "Training-Batch No.24910\n",
      "Loss = 1.507204294204712\n",
      "Training-Batch No.24920\n",
      "Loss = 2.3132004737854004\n",
      "Training-Batch No.24930\n",
      "Loss = 2.976447105407715\n",
      "Training-Batch No.24940\n",
      "Loss = 3.2767279148101807\n",
      "Training-Batch No.24950\n",
      "Loss = 1.8393611907958984\n",
      "Training-Batch No.24960\n",
      "Loss = 1.9216886758804321\n",
      "Training-Batch No.24970\n",
      "Loss = 1.7301276922225952\n",
      "Training-Batch No.24980\n",
      "Loss = 1.5362054109573364\n",
      "Training-Batch No.24990\n",
      "Loss = 1.5832798480987549\n",
      "Training-Batch No.25000\n",
      "Loss = 3.0734522342681885\n",
      "Training-Batch No.25010\n",
      "Loss = 2.2587642669677734\n",
      "Training-Batch No.25020\n",
      "Loss = 1.6799819469451904\n",
      "Training-Batch No.25030\n",
      "Loss = 1.581766963005066\n",
      "Training-Batch No.25040\n",
      "Loss = 1.565716028213501\n",
      "Training-Batch No.25050\n",
      "Loss = 1.1970008611679077\n",
      "Training-Batch No.25060\n",
      "Loss = 2.2625293731689453\n",
      "Training-Batch No.25070\n",
      "Loss = 3.7866318225860596\n",
      "Training-Batch No.25080\n",
      "Loss = 2.047121524810791\n",
      "Training-Batch No.25090\n",
      "Loss = 2.1075053215026855\n",
      "Training-Batch No.25100\n",
      "Loss = 2.5381762981414795\n",
      "Training-Batch No.25110\n",
      "Loss = 2.2141125202178955\n",
      "Training-Batch No.25120\n",
      "Loss = 5.092083930969238\n",
      "Training-Batch No.25130\n",
      "Loss = 1.9038782119750977\n",
      "Training-Batch No.25140\n",
      "Loss = 2.735504627227783\n",
      "Training-Batch No.25150\n",
      "Loss = 1.4017482995986938\n",
      "Training-Batch No.25160\n",
      "Loss = 2.29449462890625\n",
      "Training-Batch No.25170\n",
      "Loss = 1.107251524925232\n",
      "Training-Batch No.25180\n",
      "Loss = 1.7451074123382568\n",
      "Training-Batch No.25190\n",
      "Loss = 1.699500322341919\n",
      "Training-Batch No.25200\n",
      "Loss = 2.6001486778259277\n",
      "Training-Batch No.25210\n",
      "Loss = 1.9525237083435059\n",
      "Training-Batch No.25220\n",
      "Loss = 2.2762062549591064\n",
      "Training-Batch No.25230\n",
      "Loss = 1.8822708129882812\n",
      "Training-Batch No.25240\n",
      "Loss = 1.6356505155563354\n",
      "Epoch 24 Training Loss: 2.1888\n",
      "Start validation\n",
      "Epoch 24 Validation Loss: 3.0219\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.19400855920114124\n",
      "Average Top-3 accuracy 0.454113171659534\n",
      "Average Top-5 accuracy 0.609129814550642\n",
      "Average Top-7 accuracy 0.7047075606276747\n",
      "Average Top-9 accuracy 0.7546362339514978\n",
      "Average Top-11 accuracy 0.8155016642891108\n",
      "Average Top-13 accuracy 0.8601997146932953\n",
      "Average Top-15 accuracy 0.8944365192582026\n",
      "current acc 0.19400855920114124\n",
      "best acc 0.3176414645744175\n",
      "updated best accuracy 0.3176414645744175\n",
      "Epoch No. 25\n",
      "Training-Batch No.25250\n",
      "Loss = 1.5785609483718872\n",
      "Training-Batch No.25260\n",
      "Loss = 2.8268747329711914\n",
      "Training-Batch No.25270\n",
      "Loss = 1.6163852214813232\n",
      "Training-Batch No.25280\n",
      "Loss = 3.887493133544922\n",
      "Training-Batch No.25290\n",
      "Loss = 3.012772560119629\n",
      "Training-Batch No.25300\n",
      "Loss = 1.7193481922149658\n",
      "Training-Batch No.25310\n",
      "Loss = 1.612166404724121\n",
      "Training-Batch No.25320\n",
      "Loss = 1.2144112586975098\n",
      "Training-Batch No.25330\n",
      "Loss = 2.332167625427246\n",
      "Training-Batch No.25340\n",
      "Loss = 2.2313854694366455\n",
      "Training-Batch No.25350\n",
      "Loss = 2.21006441116333\n",
      "Training-Batch No.25360\n",
      "Loss = 1.820991039276123\n",
      "Training-Batch No.25370\n",
      "Loss = 1.9601308107376099\n",
      "Training-Batch No.25380\n",
      "Loss = 1.9801996946334839\n",
      "Training-Batch No.25390\n",
      "Loss = 2.4711265563964844\n",
      "Training-Batch No.25400\n",
      "Loss = 2.093750476837158\n",
      "Training-Batch No.25410\n",
      "Loss = 4.00191068649292\n",
      "Training-Batch No.25420\n",
      "Loss = 2.4792990684509277\n",
      "Training-Batch No.25430\n",
      "Loss = 1.3618199825286865\n",
      "Training-Batch No.25440\n",
      "Loss = 2.40877366065979\n",
      "Training-Batch No.25450\n",
      "Loss = 2.5891683101654053\n",
      "Training-Batch No.25460\n",
      "Loss = 2.914583444595337\n",
      "Training-Batch No.25470\n",
      "Loss = 2.5340664386749268\n",
      "Training-Batch No.25480\n",
      "Loss = 1.4927613735198975\n",
      "Training-Batch No.25490\n",
      "Loss = 1.9782607555389404\n",
      "Training-Batch No.25500\n",
      "Loss = 2.0800395011901855\n",
      "Training-Batch No.25510\n",
      "Loss = 6.366089820861816\n",
      "Training-Batch No.25520\n",
      "Loss = 3.4529917240142822\n",
      "Training-Batch No.25530\n",
      "Loss = 2.1015467643737793\n",
      "Training-Batch No.25540\n",
      "Loss = 1.757700800895691\n",
      "Training-Batch No.25550\n",
      "Loss = 2.057361125946045\n",
      "Training-Batch No.25560\n",
      "Loss = 1.818164348602295\n",
      "Training-Batch No.25570\n",
      "Loss = 1.4304323196411133\n",
      "Training-Batch No.25580\n",
      "Loss = 1.5465410947799683\n",
      "Training-Batch No.25590\n",
      "Loss = 2.3762049674987793\n",
      "Training-Batch No.25600\n",
      "Loss = 3.2016918659210205\n",
      "Training-Batch No.25610\n",
      "Loss = 1.5185893774032593\n",
      "Training-Batch No.25620\n",
      "Loss = 2.571042060852051\n",
      "Training-Batch No.25630\n",
      "Loss = 2.549163341522217\n",
      "Training-Batch No.25640\n",
      "Loss = 2.944441318511963\n",
      "Training-Batch No.25650\n",
      "Loss = 6.914535045623779\n",
      "Training-Batch No.25660\n",
      "Loss = 1.8481011390686035\n",
      "Training-Batch No.25670\n",
      "Loss = 1.732324481010437\n",
      "Training-Batch No.25680\n",
      "Loss = 2.5126137733459473\n",
      "Training-Batch No.25690\n",
      "Loss = 2.2643935680389404\n",
      "Training-Batch No.25700\n",
      "Loss = 2.2864480018615723\n",
      "Training-Batch No.25710\n",
      "Loss = 8.407510757446289\n",
      "Training-Batch No.25720\n",
      "Loss = 1.4681980609893799\n",
      "Training-Batch No.25730\n",
      "Loss = 4.249755382537842\n",
      "Training-Batch No.25740\n",
      "Loss = 1.6288381814956665\n",
      "Training-Batch No.25750\n",
      "Loss = 2.4343128204345703\n",
      "Training-Batch No.25760\n",
      "Loss = 1.4917868375778198\n",
      "Training-Batch No.25770\n",
      "Loss = 1.429660439491272\n",
      "Training-Batch No.25780\n",
      "Loss = 2.245368003845215\n",
      "Training-Batch No.25790\n",
      "Loss = 2.259937286376953\n",
      "Training-Batch No.25800\n",
      "Loss = 1.4229220151901245\n",
      "Training-Batch No.25810\n",
      "Loss = 2.400420665740967\n",
      "Training-Batch No.25820\n",
      "Loss = 2.2149055004119873\n",
      "Training-Batch No.25830\n",
      "Loss = 1.344664216041565\n",
      "Training-Batch No.25840\n",
      "Loss = 3.9588770866394043\n",
      "Training-Batch No.25850\n",
      "Loss = 1.8364951610565186\n",
      "Training-Batch No.25860\n",
      "Loss = 2.1025116443634033\n",
      "Training-Batch No.25870\n",
      "Loss = 1.7595784664154053\n",
      "Training-Batch No.25880\n",
      "Loss = 2.3997087478637695\n",
      "Training-Batch No.25890\n",
      "Loss = 1.8063790798187256\n",
      "Training-Batch No.25900\n",
      "Loss = 3.863205909729004\n",
      "Training-Batch No.25910\n",
      "Loss = 1.938718318939209\n",
      "Training-Batch No.25920\n",
      "Loss = 2.1298036575317383\n",
      "Training-Batch No.25930\n",
      "Loss = 2.2354445457458496\n",
      "Training-Batch No.25940\n",
      "Loss = 1.6092476844787598\n",
      "Training-Batch No.25950\n",
      "Loss = 1.7508316040039062\n",
      "Training-Batch No.25960\n",
      "Loss = 2.5832998752593994\n",
      "Training-Batch No.25970\n",
      "Loss = 6.075745582580566\n",
      "Training-Batch No.25980\n",
      "Loss = 1.5952637195587158\n",
      "Training-Batch No.25990\n",
      "Loss = 1.8738243579864502\n",
      "Training-Batch No.26000\n",
      "Loss = 1.7592790126800537\n",
      "Training-Batch No.26010\n",
      "Loss = 1.3554186820983887\n",
      "Training-Batch No.26020\n",
      "Loss = 1.6262869834899902\n",
      "Training-Batch No.26030\n",
      "Loss = 3.709402561187744\n",
      "Training-Batch No.26040\n",
      "Loss = 1.717228651046753\n",
      "Training-Batch No.26050\n",
      "Loss = 2.0331227779388428\n",
      "Training-Batch No.26060\n",
      "Loss = 2.530200719833374\n",
      "Training-Batch No.26070\n",
      "Loss = 1.9350734949111938\n",
      "Training-Batch No.26080\n",
      "Loss = 1.3387471437454224\n",
      "Training-Batch No.26090\n",
      "Loss = 2.209460496902466\n",
      "Training-Batch No.26100\n",
      "Loss = 5.049756050109863\n",
      "Training-Batch No.26110\n",
      "Loss = 1.7586660385131836\n",
      "Training-Batch No.26120\n",
      "Loss = 1.6065268516540527\n",
      "Training-Batch No.26130\n",
      "Loss = 1.6347239017486572\n",
      "Training-Batch No.26140\n",
      "Loss = 2.373300790786743\n",
      "Training-Batch No.26150\n",
      "Loss = 2.065143346786499\n",
      "Training-Batch No.26160\n",
      "Loss = 1.8897881507873535\n",
      "Training-Batch No.26170\n",
      "Loss = 2.445272445678711\n",
      "Training-Batch No.26180\n",
      "Loss = 2.313840389251709\n",
      "Training-Batch No.26190\n",
      "Loss = 1.2928569316864014\n",
      "Training-Batch No.26200\n",
      "Loss = 1.6948686838150024\n",
      "Training-Batch No.26210\n",
      "Loss = 2.347653865814209\n",
      "Training-Batch No.26220\n",
      "Loss = 1.7178900241851807\n",
      "Training-Batch No.26230\n",
      "Loss = 1.703264832496643\n",
      "Training-Batch No.26240\n",
      "Loss = 1.1842316389083862\n",
      "Training-Batch No.26250\n",
      "Loss = 2.1466379165649414\n",
      "Training-Batch No.26260\n",
      "Loss = 1.698089361190796\n",
      "Training-Batch No.26270\n",
      "Loss = 2.395092010498047\n",
      "Training-Batch No.26280\n",
      "Loss = 1.8585351705551147\n",
      "Training-Batch No.26290\n",
      "Loss = 1.1695953607559204\n",
      "Training-Batch No.26300\n",
      "Loss = 1.8301801681518555\n",
      "Epoch 25 Training Loss: 2.1868\n",
      "Start validation\n",
      "Epoch 25 Validation Loss: 2.4133\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.3518782691393248\n",
      "Average Top-3 accuracy 0.6761768901569187\n",
      "Average Top-5 accuracy 0.8292914883499762\n",
      "Average Top-7 accuracy 0.9115549215406562\n",
      "Average Top-9 accuracy 0.9348549690917737\n",
      "Average Top-11 accuracy 0.9495958155016643\n",
      "Average Top-13 accuracy 0.9543509272467903\n",
      "Average Top-15 accuracy 0.9629101283880172\n",
      "current acc 0.3518782691393248\n",
      "best acc 0.3176414645744175\n",
      "Saving the best model\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 26\n",
      "Training-Batch No.26310\n",
      "Loss = 8.118244171142578\n",
      "Training-Batch No.26320\n",
      "Loss = 2.4757516384124756\n",
      "Training-Batch No.26330\n",
      "Loss = 2.1035263538360596\n",
      "Training-Batch No.26340\n",
      "Loss = 1.3924379348754883\n",
      "Training-Batch No.26350\n",
      "Loss = 1.5227874517440796\n",
      "Training-Batch No.26360\n",
      "Loss = 2.111487865447998\n",
      "Training-Batch No.26370\n",
      "Loss = 2.2266507148742676\n",
      "Training-Batch No.26380\n",
      "Loss = 1.9759336709976196\n",
      "Training-Batch No.26390\n",
      "Loss = 2.0518805980682373\n",
      "Training-Batch No.26400\n",
      "Loss = 1.9224443435668945\n",
      "Training-Batch No.26410\n",
      "Loss = 1.8178179264068604\n",
      "Training-Batch No.26420\n",
      "Loss = 3.434443235397339\n",
      "Training-Batch No.26430\n",
      "Loss = 1.7259724140167236\n",
      "Training-Batch No.26440\n",
      "Loss = 2.093852996826172\n",
      "Training-Batch No.26450\n",
      "Loss = 1.8284051418304443\n",
      "Training-Batch No.26460\n",
      "Loss = 2.051301956176758\n",
      "Training-Batch No.26470\n",
      "Loss = 1.6829289197921753\n",
      "Training-Batch No.26480\n",
      "Loss = 2.526470422744751\n",
      "Training-Batch No.26490\n",
      "Loss = 1.7422202825546265\n",
      "Training-Batch No.26500\n",
      "Loss = 3.334944725036621\n",
      "Training-Batch No.26510\n",
      "Loss = 1.2629570960998535\n",
      "Training-Batch No.26520\n",
      "Loss = 2.146785020828247\n",
      "Training-Batch No.26530\n",
      "Loss = 1.9939777851104736\n",
      "Training-Batch No.26540\n",
      "Loss = 2.237034320831299\n",
      "Training-Batch No.26550\n",
      "Loss = 1.7152916193008423\n",
      "Training-Batch No.26560\n",
      "Loss = 1.922001600265503\n",
      "Training-Batch No.26570\n",
      "Loss = 2.4392495155334473\n",
      "Training-Batch No.26580\n",
      "Loss = 3.0394721031188965\n",
      "Training-Batch No.26590\n",
      "Loss = 2.1066877841949463\n",
      "Training-Batch No.26600\n",
      "Loss = 1.6823843717575073\n",
      "Training-Batch No.26610\n",
      "Loss = 2.465651035308838\n",
      "Training-Batch No.26620\n",
      "Loss = 2.3636813163757324\n",
      "Training-Batch No.26630\n",
      "Loss = 2.302703380584717\n",
      "Training-Batch No.26640\n",
      "Loss = 3.605067729949951\n",
      "Training-Batch No.26650\n",
      "Loss = 1.8801255226135254\n",
      "Training-Batch No.26660\n",
      "Loss = 1.7316102981567383\n",
      "Training-Batch No.26670\n",
      "Loss = 2.452084541320801\n",
      "Training-Batch No.26680\n",
      "Loss = 2.15574312210083\n",
      "Training-Batch No.26690\n",
      "Loss = 1.2178901433944702\n",
      "Training-Batch No.26700\n",
      "Loss = 3.179440975189209\n",
      "Training-Batch No.26710\n",
      "Loss = 2.782881259918213\n",
      "Training-Batch No.26720\n",
      "Loss = 1.6005165576934814\n",
      "Training-Batch No.26730\n",
      "Loss = 1.6677062511444092\n",
      "Training-Batch No.26740\n",
      "Loss = 3.6561107635498047\n",
      "Training-Batch No.26750\n",
      "Loss = 1.5574349164962769\n",
      "Training-Batch No.26760\n",
      "Loss = 1.7902252674102783\n",
      "Training-Batch No.26770\n",
      "Loss = 2.3817195892333984\n",
      "Training-Batch No.26780\n",
      "Loss = 2.2341973781585693\n",
      "Training-Batch No.26790\n",
      "Loss = 2.4519553184509277\n",
      "Training-Batch No.26800\n",
      "Loss = 1.3507357835769653\n",
      "Training-Batch No.26810\n",
      "Loss = 1.5182383060455322\n",
      "Training-Batch No.26820\n",
      "Loss = 2.350862741470337\n",
      "Training-Batch No.26830\n",
      "Loss = 1.9456177949905396\n",
      "Training-Batch No.26840\n",
      "Loss = 1.8262734413146973\n",
      "Training-Batch No.26850\n",
      "Loss = 2.3263731002807617\n",
      "Training-Batch No.26860\n",
      "Loss = 1.9023663997650146\n",
      "Training-Batch No.26870\n",
      "Loss = 2.6106834411621094\n",
      "Training-Batch No.26880\n",
      "Loss = 1.918173909187317\n",
      "Training-Batch No.26890\n",
      "Loss = 2.1162538528442383\n",
      "Training-Batch No.26900\n",
      "Loss = 2.526884078979492\n",
      "Training-Batch No.26910\n",
      "Loss = 2.6765124797821045\n",
      "Training-Batch No.26920\n",
      "Loss = 1.7060604095458984\n",
      "Training-Batch No.26930\n",
      "Loss = 1.8512243032455444\n",
      "Training-Batch No.26940\n",
      "Loss = 1.7462557554244995\n",
      "Training-Batch No.26950\n",
      "Loss = 1.8059718608856201\n",
      "Training-Batch No.26960\n",
      "Loss = 1.5367834568023682\n",
      "Training-Batch No.26970\n",
      "Loss = 3.127552032470703\n",
      "Training-Batch No.26980\n",
      "Loss = 1.903254747390747\n",
      "Training-Batch No.26990\n",
      "Loss = 1.2152196168899536\n",
      "Training-Batch No.27000\n",
      "Loss = 1.3149845600128174\n",
      "Training-Batch No.27010\n",
      "Loss = 1.9742262363433838\n",
      "Training-Batch No.27020\n",
      "Loss = 7.251739501953125\n",
      "Training-Batch No.27030\n",
      "Loss = 1.9131965637207031\n",
      "Training-Batch No.27040\n",
      "Loss = 2.3744986057281494\n",
      "Training-Batch No.27050\n",
      "Loss = 2.5080227851867676\n",
      "Training-Batch No.27060\n",
      "Loss = 3.5873124599456787\n",
      "Training-Batch No.27070\n",
      "Loss = 3.4397308826446533\n",
      "Training-Batch No.27080\n",
      "Loss = 2.304105281829834\n",
      "Training-Batch No.27090\n",
      "Loss = 1.679556131362915\n",
      "Training-Batch No.27100\n",
      "Loss = 2.3831372261047363\n",
      "Training-Batch No.27110\n",
      "Loss = 1.4671128988265991\n",
      "Training-Batch No.27120\n",
      "Loss = 1.6094729900360107\n",
      "Training-Batch No.27130\n",
      "Loss = 9.882302284240723\n",
      "Training-Batch No.27140\n",
      "Loss = 2.295250415802002\n",
      "Training-Batch No.27150\n",
      "Loss = 2.098776340484619\n",
      "Training-Batch No.27160\n",
      "Loss = 3.289412498474121\n",
      "Training-Batch No.27170\n",
      "Loss = 1.5578279495239258\n",
      "Training-Batch No.27180\n",
      "Loss = 1.5262858867645264\n",
      "Training-Batch No.27190\n",
      "Loss = 2.7610998153686523\n",
      "Training-Batch No.27200\n",
      "Loss = 1.4383490085601807\n",
      "Training-Batch No.27210\n",
      "Loss = 1.8622777462005615\n",
      "Training-Batch No.27220\n",
      "Loss = 2.1834285259246826\n",
      "Training-Batch No.27230\n",
      "Loss = 1.4535150527954102\n",
      "Training-Batch No.27240\n",
      "Loss = 1.5428991317749023\n",
      "Training-Batch No.27250\n",
      "Loss = 1.5460326671600342\n",
      "Training-Batch No.27260\n",
      "Loss = 2.141815185546875\n",
      "Training-Batch No.27270\n",
      "Loss = 1.7359728813171387\n",
      "Training-Batch No.27280\n",
      "Loss = 1.510034203529358\n",
      "Training-Batch No.27290\n",
      "Loss = 1.2731800079345703\n",
      "Training-Batch No.27300\n",
      "Loss = 1.1665503978729248\n",
      "Training-Batch No.27310\n",
      "Loss = 1.515163540840149\n",
      "Training-Batch No.27320\n",
      "Loss = 2.0609827041625977\n",
      "Training-Batch No.27330\n",
      "Loss = 1.383036732673645\n",
      "Training-Batch No.27340\n",
      "Loss = 2.0211408138275146\n",
      "Training-Batch No.27350\n",
      "Loss = 2.356170654296875\n",
      "Epoch 26 Training Loss: 2.2127\n",
      "Start validation\n",
      "Epoch 26 Validation Loss: 2.8371\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.24488825487398955\n",
      "Average Top-3 accuracy 0.5273418925344746\n",
      "Average Top-5 accuracy 0.6699952448882549\n",
      "Average Top-7 accuracy 0.7617689015691869\n",
      "Average Top-9 accuracy 0.792677127912506\n",
      "Average Top-11 accuracy 0.8226343319067998\n",
      "Average Top-13 accuracy 0.8563956252971945\n",
      "Average Top-15 accuracy 0.8873038516405135\n",
      "current acc 0.24488825487398955\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 27\n",
      "Training-Batch No.27360\n",
      "Loss = 2.1747355461120605\n",
      "Training-Batch No.27370\n",
      "Loss = 1.5851812362670898\n",
      "Training-Batch No.27380\n",
      "Loss = 1.6971416473388672\n",
      "Training-Batch No.27390\n",
      "Loss = 1.6202887296676636\n",
      "Training-Batch No.27400\n",
      "Loss = 1.8614628314971924\n",
      "Training-Batch No.27410\n",
      "Loss = 2.2028567790985107\n",
      "Training-Batch No.27420\n",
      "Loss = 1.7127349376678467\n",
      "Training-Batch No.27430\n",
      "Loss = 3.5071325302124023\n",
      "Training-Batch No.27440\n",
      "Loss = 1.4765210151672363\n",
      "Training-Batch No.27450\n",
      "Loss = 1.3634048700332642\n",
      "Training-Batch No.27460\n",
      "Loss = 5.32340669631958\n",
      "Training-Batch No.27470\n",
      "Loss = 1.5781872272491455\n",
      "Training-Batch No.27480\n",
      "Loss = 1.636365532875061\n",
      "Training-Batch No.27490\n",
      "Loss = 1.9736082553863525\n",
      "Training-Batch No.27500\n",
      "Loss = 1.501479148864746\n",
      "Training-Batch No.27510\n",
      "Loss = 2.3864424228668213\n",
      "Training-Batch No.27520\n",
      "Loss = 1.8365375995635986\n",
      "Training-Batch No.27530\n",
      "Loss = 2.6861014366149902\n",
      "Training-Batch No.27540\n",
      "Loss = 3.1635560989379883\n",
      "Training-Batch No.27550\n",
      "Loss = 2.194498300552368\n",
      "Training-Batch No.27560\n",
      "Loss = 1.834023118019104\n",
      "Training-Batch No.27570\n",
      "Loss = 1.795599341392517\n",
      "Training-Batch No.27580\n",
      "Loss = 1.5878119468688965\n",
      "Training-Batch No.27590\n",
      "Loss = 2.2725155353546143\n",
      "Training-Batch No.27600\n",
      "Loss = 4.091538906097412\n",
      "Training-Batch No.27610\n",
      "Loss = 1.906029462814331\n",
      "Training-Batch No.27620\n",
      "Loss = 1.6508303880691528\n",
      "Training-Batch No.27630\n",
      "Loss = 1.5751699209213257\n",
      "Training-Batch No.27640\n",
      "Loss = 1.5980961322784424\n",
      "Training-Batch No.27650\n",
      "Loss = 1.7371186017990112\n",
      "Training-Batch No.27660\n",
      "Loss = 1.9689953327178955\n",
      "Training-Batch No.27670\n",
      "Loss = 1.6461026668548584\n",
      "Training-Batch No.27680\n",
      "Loss = 1.7970163822174072\n",
      "Training-Batch No.27690\n",
      "Loss = 1.4262927770614624\n",
      "Training-Batch No.27700\n",
      "Loss = 1.9560025930404663\n",
      "Training-Batch No.27710\n",
      "Loss = 1.557288408279419\n",
      "Training-Batch No.27720\n",
      "Loss = 1.6821702718734741\n",
      "Training-Batch No.27730\n",
      "Loss = 2.7952513694763184\n",
      "Training-Batch No.27740\n",
      "Loss = 2.1670970916748047\n",
      "Training-Batch No.27750\n",
      "Loss = 1.9101027250289917\n",
      "Training-Batch No.27760\n",
      "Loss = 2.479796886444092\n",
      "Training-Batch No.27770\n",
      "Loss = 1.7173999547958374\n",
      "Training-Batch No.27780\n",
      "Loss = 1.6081029176712036\n",
      "Training-Batch No.27790\n",
      "Loss = 1.9890365600585938\n",
      "Training-Batch No.27800\n",
      "Loss = 3.5138790607452393\n",
      "Training-Batch No.27810\n",
      "Loss = 1.6466455459594727\n",
      "Training-Batch No.27820\n",
      "Loss = 2.500760078430176\n",
      "Training-Batch No.27830\n",
      "Loss = 1.3940120935440063\n",
      "Training-Batch No.27840\n",
      "Loss = 2.0634732246398926\n",
      "Training-Batch No.27850\n",
      "Loss = 1.1101915836334229\n",
      "Training-Batch No.27860\n",
      "Loss = 1.3352980613708496\n",
      "Training-Batch No.27870\n",
      "Loss = 1.6495444774627686\n",
      "Training-Batch No.27880\n",
      "Loss = 1.6220831871032715\n",
      "Training-Batch No.27890\n",
      "Loss = 1.4643282890319824\n",
      "Training-Batch No.27900\n",
      "Loss = 1.9517613649368286\n",
      "Training-Batch No.27910\n",
      "Loss = 3.056459426879883\n",
      "Training-Batch No.27920\n",
      "Loss = 3.6786460876464844\n",
      "Training-Batch No.27930\n",
      "Loss = 1.5568360090255737\n",
      "Training-Batch No.27940\n",
      "Loss = 2.3250975608825684\n",
      "Training-Batch No.27950\n",
      "Loss = 1.9271122217178345\n",
      "Training-Batch No.27960\n",
      "Loss = 2.0274157524108887\n",
      "Training-Batch No.27970\n",
      "Loss = 1.6282292604446411\n",
      "Training-Batch No.27980\n",
      "Loss = 1.5666987895965576\n",
      "Training-Batch No.27990\n",
      "Loss = 2.646772861480713\n",
      "Training-Batch No.28000\n",
      "Loss = 2.6478843688964844\n",
      "Training-Batch No.28010\n",
      "Loss = 1.5414001941680908\n",
      "Training-Batch No.28020\n",
      "Loss = 1.9940979480743408\n",
      "Training-Batch No.28030\n",
      "Loss = 2.106381893157959\n",
      "Training-Batch No.28040\n",
      "Loss = 2.5657787322998047\n",
      "Training-Batch No.28050\n",
      "Loss = 1.31596040725708\n",
      "Training-Batch No.28060\n",
      "Loss = 2.1633460521698\n",
      "Training-Batch No.28070\n",
      "Loss = 1.2229740619659424\n",
      "Training-Batch No.28080\n",
      "Loss = 2.884880781173706\n",
      "Training-Batch No.28090\n",
      "Loss = 2.5833449363708496\n",
      "Training-Batch No.28100\n",
      "Loss = 2.2492318153381348\n",
      "Training-Batch No.28110\n",
      "Loss = 1.9083912372589111\n",
      "Training-Batch No.28120\n",
      "Loss = 1.109017252922058\n",
      "Training-Batch No.28130\n",
      "Loss = 2.224125862121582\n",
      "Training-Batch No.28140\n",
      "Loss = 2.613354206085205\n",
      "Training-Batch No.28150\n",
      "Loss = 2.3709871768951416\n",
      "Training-Batch No.28160\n",
      "Loss = 1.651965856552124\n",
      "Training-Batch No.28170\n",
      "Loss = 1.5141562223434448\n",
      "Training-Batch No.28180\n",
      "Loss = 2.2536046504974365\n",
      "Training-Batch No.28190\n",
      "Loss = 1.4379137754440308\n",
      "Training-Batch No.28200\n",
      "Loss = 2.730175733566284\n",
      "Training-Batch No.28210\n",
      "Loss = 2.958587646484375\n",
      "Training-Batch No.28220\n",
      "Loss = 1.2814112901687622\n",
      "Training-Batch No.28230\n",
      "Loss = 2.2557172775268555\n",
      "Training-Batch No.28240\n",
      "Loss = 1.260480523109436\n",
      "Training-Batch No.28250\n",
      "Loss = 1.8424192667007446\n",
      "Training-Batch No.28260\n",
      "Loss = 1.7834781408309937\n",
      "Training-Batch No.28270\n",
      "Loss = 2.0254712104797363\n",
      "Training-Batch No.28280\n",
      "Loss = 1.8389207124710083\n",
      "Training-Batch No.28290\n",
      "Loss = 2.744719982147217\n",
      "Training-Batch No.28300\n",
      "Loss = 1.6737545728683472\n",
      "Training-Batch No.28310\n",
      "Loss = 1.7001986503601074\n",
      "Training-Batch No.28320\n",
      "Loss = 1.8920245170593262\n",
      "Training-Batch No.28330\n",
      "Loss = 1.8622214794158936\n",
      "Training-Batch No.28340\n",
      "Loss = 1.952305555343628\n",
      "Training-Batch No.28350\n",
      "Loss = 1.8615366220474243\n",
      "Training-Batch No.28360\n",
      "Loss = 1.296889066696167\n",
      "Training-Batch No.28370\n",
      "Loss = 2.6820249557495117\n",
      "Training-Batch No.28380\n",
      "Loss = 1.7364697456359863\n",
      "Training-Batch No.28390\n",
      "Loss = 1.7375538349151611\n",
      "Training-Batch No.28400\n",
      "Loss = 1.8299657106399536\n",
      "Epoch 27 Training Loss: 2.1621\n",
      "Start validation\n",
      "Epoch 27 Validation Loss: 2.5901\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.25962910128388017\n",
      "Average Top-3 accuracy 0.5877318116975749\n",
      "Average Top-5 accuracy 0.7636709462672373\n",
      "Average Top-7 accuracy 0.8611507370423205\n",
      "Average Top-9 accuracy 0.9067998097955302\n",
      "Average Top-11 accuracy 0.932001902044698\n",
      "Average Top-13 accuracy 0.9500713266761769\n",
      "Average Top-15 accuracy 0.9619591060389919\n",
      "current acc 0.25962910128388017\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 28\n",
      "Training-Batch No.28410\n",
      "Loss = 2.2262942790985107\n",
      "Training-Batch No.28420\n",
      "Loss = 1.818188190460205\n",
      "Training-Batch No.28430\n",
      "Loss = 1.4696742296218872\n",
      "Training-Batch No.28440\n",
      "Loss = 1.950490951538086\n",
      "Training-Batch No.28450\n",
      "Loss = 1.3796907663345337\n",
      "Training-Batch No.28460\n",
      "Loss = 2.135889768600464\n",
      "Training-Batch No.28470\n",
      "Loss = 6.294117450714111\n",
      "Training-Batch No.28480\n",
      "Loss = 1.4086625576019287\n",
      "Training-Batch No.28490\n",
      "Loss = 1.6009048223495483\n",
      "Training-Batch No.28500\n",
      "Loss = 2.9697136878967285\n",
      "Training-Batch No.28510\n",
      "Loss = 1.7944871187210083\n",
      "Training-Batch No.28520\n",
      "Loss = 1.2241287231445312\n",
      "Training-Batch No.28530\n",
      "Loss = 3.564249038696289\n",
      "Training-Batch No.28540\n",
      "Loss = 2.0693094730377197\n",
      "Training-Batch No.28550\n",
      "Loss = 2.0211222171783447\n",
      "Training-Batch No.28560\n",
      "Loss = 1.5227841138839722\n",
      "Training-Batch No.28570\n",
      "Loss = 1.484917402267456\n",
      "Training-Batch No.28580\n",
      "Loss = 1.928344488143921\n",
      "Training-Batch No.28590\n",
      "Loss = 4.037935256958008\n",
      "Training-Batch No.28600\n",
      "Loss = 1.9524190425872803\n",
      "Training-Batch No.28610\n",
      "Loss = 2.762416362762451\n",
      "Training-Batch No.28620\n",
      "Loss = 1.925207495689392\n",
      "Training-Batch No.28630\n",
      "Loss = 1.2253563404083252\n",
      "Training-Batch No.28640\n",
      "Loss = 1.717057466506958\n",
      "Training-Batch No.28650\n",
      "Loss = 1.5314701795578003\n",
      "Training-Batch No.28660\n",
      "Loss = 1.8395016193389893\n",
      "Training-Batch No.28670\n",
      "Loss = 1.6144156455993652\n",
      "Training-Batch No.28680\n",
      "Loss = 1.6273999214172363\n",
      "Training-Batch No.28690\n",
      "Loss = 1.5268187522888184\n",
      "Training-Batch No.28700\n",
      "Loss = 4.239233016967773\n",
      "Training-Batch No.28710\n",
      "Loss = 1.899683952331543\n",
      "Training-Batch No.28720\n",
      "Loss = 1.8940916061401367\n",
      "Training-Batch No.28730\n",
      "Loss = 1.6900633573532104\n",
      "Training-Batch No.28740\n",
      "Loss = 2.343574285507202\n",
      "Training-Batch No.28750\n",
      "Loss = 2.450251817703247\n",
      "Training-Batch No.28760\n",
      "Loss = 1.5248019695281982\n",
      "Training-Batch No.28770\n",
      "Loss = 1.8376353979110718\n",
      "Training-Batch No.28780\n",
      "Loss = 2.431406259536743\n",
      "Training-Batch No.28790\n",
      "Loss = 2.2160587310791016\n",
      "Training-Batch No.28800\n",
      "Loss = 4.067859172821045\n",
      "Training-Batch No.28810\n",
      "Loss = 2.5018930435180664\n",
      "Training-Batch No.28820\n",
      "Loss = 1.6681405305862427\n",
      "Training-Batch No.28830\n",
      "Loss = 1.7913724184036255\n",
      "Training-Batch No.28840\n",
      "Loss = 1.7145086526870728\n",
      "Training-Batch No.28850\n",
      "Loss = 1.3816888332366943\n",
      "Training-Batch No.28860\n",
      "Loss = 2.1232833862304688\n",
      "Training-Batch No.28870\n",
      "Loss = 1.6857370138168335\n",
      "Training-Batch No.28880\n",
      "Loss = 1.7971357107162476\n",
      "Training-Batch No.28890\n",
      "Loss = 1.2663915157318115\n",
      "Training-Batch No.28900\n",
      "Loss = 1.6748757362365723\n",
      "Training-Batch No.28910\n",
      "Loss = 1.8607652187347412\n",
      "Training-Batch No.28920\n",
      "Loss = 2.028911828994751\n",
      "Training-Batch No.28930\n",
      "Loss = 1.538635015487671\n",
      "Training-Batch No.28940\n",
      "Loss = 2.1346182823181152\n",
      "Training-Batch No.28950\n",
      "Loss = 1.2459614276885986\n",
      "Training-Batch No.28960\n",
      "Loss = 1.6872055530548096\n",
      "Training-Batch No.28970\n",
      "Loss = 1.632190227508545\n",
      "Training-Batch No.28980\n",
      "Loss = 1.5250911712646484\n",
      "Training-Batch No.28990\n",
      "Loss = 1.3209614753723145\n",
      "Training-Batch No.29000\n",
      "Loss = 1.4684462547302246\n",
      "Training-Batch No.29010\n",
      "Loss = 1.785306692123413\n",
      "Training-Batch No.29020\n",
      "Loss = 1.6934252977371216\n",
      "Training-Batch No.29030\n",
      "Loss = 1.8278182744979858\n",
      "Training-Batch No.29040\n",
      "Loss = 1.9307408332824707\n",
      "Training-Batch No.29050\n",
      "Loss = 1.4394006729125977\n",
      "Training-Batch No.29060\n",
      "Loss = 1.310382604598999\n",
      "Training-Batch No.29070\n",
      "Loss = 2.5709688663482666\n",
      "Training-Batch No.29080\n",
      "Loss = 1.8115456104278564\n",
      "Training-Batch No.29090\n",
      "Loss = 1.4524606466293335\n",
      "Training-Batch No.29100\n",
      "Loss = 2.1022253036499023\n",
      "Training-Batch No.29110\n",
      "Loss = 3.316168785095215\n",
      "Training-Batch No.29120\n",
      "Loss = 2.6842892169952393\n",
      "Training-Batch No.29130\n",
      "Loss = 2.362562656402588\n",
      "Training-Batch No.29140\n",
      "Loss = 1.3456958532333374\n",
      "Training-Batch No.29150\n",
      "Loss = 2.231595993041992\n",
      "Training-Batch No.29160\n",
      "Loss = 2.0330679416656494\n",
      "Training-Batch No.29170\n",
      "Loss = 2.137042284011841\n",
      "Training-Batch No.29180\n",
      "Loss = 3.3147058486938477\n",
      "Training-Batch No.29190\n",
      "Loss = 2.1293795108795166\n",
      "Training-Batch No.29200\n",
      "Loss = 1.5678308010101318\n",
      "Training-Batch No.29210\n",
      "Loss = 1.5347193479537964\n",
      "Training-Batch No.29220\n",
      "Loss = 2.703998327255249\n",
      "Training-Batch No.29230\n",
      "Loss = 1.989285945892334\n",
      "Training-Batch No.29240\n",
      "Loss = 2.075385332107544\n",
      "Training-Batch No.29250\n",
      "Loss = 2.4049243927001953\n",
      "Training-Batch No.29260\n",
      "Loss = 2.0758438110351562\n",
      "Training-Batch No.29270\n",
      "Loss = 4.2345685958862305\n",
      "Training-Batch No.29280\n",
      "Loss = 1.5119495391845703\n",
      "Training-Batch No.29290\n",
      "Loss = 1.223192811012268\n",
      "Training-Batch No.29300\n",
      "Loss = 2.9408950805664062\n",
      "Training-Batch No.29310\n",
      "Loss = 1.6777520179748535\n",
      "Training-Batch No.29320\n",
      "Loss = 1.675920844078064\n",
      "Training-Batch No.29330\n",
      "Loss = 1.5902765989303589\n",
      "Training-Batch No.29340\n",
      "Loss = 1.6735378503799438\n",
      "Training-Batch No.29350\n",
      "Loss = 2.2706422805786133\n",
      "Training-Batch No.29360\n",
      "Loss = 1.2843061685562134\n",
      "Training-Batch No.29370\n",
      "Loss = 1.8602906465530396\n",
      "Training-Batch No.29380\n",
      "Loss = 1.80606210231781\n",
      "Training-Batch No.29390\n",
      "Loss = 1.4744257926940918\n",
      "Training-Batch No.29400\n",
      "Loss = 1.5254011154174805\n",
      "Training-Batch No.29410\n",
      "Loss = 1.6816208362579346\n",
      "Training-Batch No.29420\n",
      "Loss = 2.4653987884521484\n",
      "Training-Batch No.29430\n",
      "Loss = 1.815760850906372\n",
      "Training-Batch No.29440\n",
      "Loss = 1.9531824588775635\n",
      "Training-Batch No.29450\n",
      "Loss = 2.418081760406494\n",
      "Epoch 28 Training Loss: 2.1545\n",
      "Start validation\n",
      "Epoch 28 Validation Loss: 4.2261\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.17498811222063718\n",
      "Average Top-3 accuracy 0.3656680932001902\n",
      "Average Top-5 accuracy 0.5092724679029957\n",
      "Average Top-7 accuracy 0.5891583452211127\n",
      "Average Top-9 accuracy 0.6733238231098431\n",
      "Average Top-11 accuracy 0.7146932952924394\n",
      "Average Top-13 accuracy 0.7551117451260104\n",
      "Average Top-15 accuracy 0.7850689491203043\n",
      "current acc 0.17498811222063718\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 29\n",
      "Training-Batch No.29460\n",
      "Loss = 2.1906983852386475\n",
      "Training-Batch No.29470\n",
      "Loss = 2.7299489974975586\n",
      "Training-Batch No.29480\n",
      "Loss = 1.871548056602478\n",
      "Training-Batch No.29490\n",
      "Loss = 1.9531188011169434\n",
      "Training-Batch No.29500\n",
      "Loss = 2.353489637374878\n",
      "Training-Batch No.29510\n",
      "Loss = 2.3775343894958496\n",
      "Training-Batch No.29520\n",
      "Loss = 1.7671641111373901\n",
      "Training-Batch No.29530\n",
      "Loss = 2.777677297592163\n",
      "Training-Batch No.29540\n",
      "Loss = 2.410231351852417\n",
      "Training-Batch No.29550\n",
      "Loss = 2.1696386337280273\n",
      "Training-Batch No.29560\n",
      "Loss = 1.6457512378692627\n",
      "Training-Batch No.29570\n",
      "Loss = 1.605970025062561\n",
      "Training-Batch No.29580\n",
      "Loss = 1.2676838636398315\n",
      "Training-Batch No.29590\n",
      "Loss = 2.5145716667175293\n",
      "Training-Batch No.29600\n",
      "Loss = 1.6313458681106567\n",
      "Training-Batch No.29610\n",
      "Loss = 1.5750236511230469\n",
      "Training-Batch No.29620\n",
      "Loss = 3.6361048221588135\n",
      "Training-Batch No.29630\n",
      "Loss = 2.121699810028076\n",
      "Training-Batch No.29640\n",
      "Loss = 2.4834911823272705\n",
      "Training-Batch No.29650\n",
      "Loss = 3.6647751331329346\n",
      "Training-Batch No.29660\n",
      "Loss = 2.6011624336242676\n",
      "Training-Batch No.29670\n",
      "Loss = 1.9923217296600342\n",
      "Training-Batch No.29680\n",
      "Loss = 2.20304799079895\n",
      "Training-Batch No.29690\n",
      "Loss = 1.6271917819976807\n",
      "Training-Batch No.29700\n",
      "Loss = 1.6413605213165283\n",
      "Training-Batch No.29710\n",
      "Loss = 1.8394571542739868\n",
      "Training-Batch No.29720\n",
      "Loss = 1.379514455795288\n",
      "Training-Batch No.29730\n",
      "Loss = 2.413379669189453\n",
      "Training-Batch No.29740\n",
      "Loss = 6.592045783996582\n",
      "Training-Batch No.29750\n",
      "Loss = 3.5018177032470703\n",
      "Training-Batch No.29760\n",
      "Loss = 1.9736127853393555\n",
      "Training-Batch No.29770\n",
      "Loss = 2.194876194000244\n",
      "Training-Batch No.29780\n",
      "Loss = 1.9224231243133545\n",
      "Training-Batch No.29790\n",
      "Loss = 1.6408395767211914\n",
      "Training-Batch No.29800\n",
      "Loss = 2.0948264598846436\n",
      "Training-Batch No.29810\n",
      "Loss = 1.9812536239624023\n",
      "Training-Batch No.29820\n",
      "Loss = 2.460134983062744\n",
      "Training-Batch No.29830\n",
      "Loss = 1.5861821174621582\n",
      "Training-Batch No.29840\n",
      "Loss = 4.389147758483887\n",
      "Training-Batch No.29850\n",
      "Loss = 1.8118858337402344\n",
      "Training-Batch No.29860\n",
      "Loss = 2.1987414360046387\n",
      "Training-Batch No.29870\n",
      "Loss = 2.5127596855163574\n",
      "Training-Batch No.29880\n",
      "Loss = 2.0182559490203857\n",
      "Training-Batch No.29890\n",
      "Loss = 1.4980380535125732\n",
      "Training-Batch No.29900\n",
      "Loss = 1.0521999597549438\n",
      "Training-Batch No.29910\n",
      "Loss = 2.210573673248291\n",
      "Training-Batch No.29920\n",
      "Loss = 2.206857204437256\n",
      "Training-Batch No.29930\n",
      "Loss = 2.030569553375244\n",
      "Training-Batch No.29940\n",
      "Loss = 1.592742681503296\n",
      "Training-Batch No.29950\n",
      "Loss = 1.8794331550598145\n",
      "Training-Batch No.29960\n",
      "Loss = 1.858554482460022\n",
      "Training-Batch No.29970\n",
      "Loss = 2.127046823501587\n",
      "Training-Batch No.29980\n",
      "Loss = 2.073613405227661\n",
      "Training-Batch No.29990\n",
      "Loss = 1.9252538681030273\n",
      "Training-Batch No.30000\n",
      "Loss = 1.3383203744888306\n",
      "Training-Batch No.30010\n",
      "Loss = 4.000125885009766\n",
      "Training-Batch No.30020\n",
      "Loss = 4.249213695526123\n",
      "Training-Batch No.30030\n",
      "Loss = 3.8920741081237793\n",
      "Training-Batch No.30040\n",
      "Loss = 1.2864985466003418\n",
      "Training-Batch No.30050\n",
      "Loss = 2.570868492126465\n",
      "Training-Batch No.30060\n",
      "Loss = 1.5149517059326172\n",
      "Training-Batch No.30070\n",
      "Loss = 1.784178614616394\n",
      "Training-Batch No.30080\n",
      "Loss = 2.576963424682617\n",
      "Training-Batch No.30090\n",
      "Loss = 2.541196823120117\n",
      "Training-Batch No.30100\n",
      "Loss = 1.9516782760620117\n",
      "Training-Batch No.30110\n",
      "Loss = 2.0348613262176514\n",
      "Training-Batch No.30120\n",
      "Loss = 2.234905242919922\n",
      "Training-Batch No.30130\n",
      "Loss = 2.0071306228637695\n",
      "Training-Batch No.30140\n",
      "Loss = 2.018306016921997\n",
      "Training-Batch No.30150\n",
      "Loss = 2.9955105781555176\n",
      "Training-Batch No.30160\n",
      "Loss = 1.453741431236267\n",
      "Training-Batch No.30170\n",
      "Loss = 1.662388801574707\n",
      "Training-Batch No.30180\n",
      "Loss = 2.651390552520752\n",
      "Training-Batch No.30190\n",
      "Loss = 3.1597189903259277\n",
      "Training-Batch No.30200\n",
      "Loss = 3.1979188919067383\n",
      "Training-Batch No.30210\n",
      "Loss = 2.1123194694519043\n",
      "Training-Batch No.30220\n",
      "Loss = 1.8153355121612549\n",
      "Training-Batch No.30230\n",
      "Loss = 1.6864333152770996\n",
      "Training-Batch No.30240\n",
      "Loss = 1.4530831575393677\n",
      "Training-Batch No.30250\n",
      "Loss = 1.6173949241638184\n",
      "Training-Batch No.30260\n",
      "Loss = 2.9477314949035645\n",
      "Training-Batch No.30270\n",
      "Loss = 2.2342529296875\n",
      "Training-Batch No.30280\n",
      "Loss = 2.020418643951416\n",
      "Training-Batch No.30290\n",
      "Loss = 1.5808860063552856\n",
      "Training-Batch No.30300\n",
      "Loss = 1.612889289855957\n",
      "Training-Batch No.30310\n",
      "Loss = 1.2915120124816895\n",
      "Training-Batch No.30320\n",
      "Loss = 2.1919119358062744\n",
      "Training-Batch No.30330\n",
      "Loss = 4.013303756713867\n",
      "Training-Batch No.30340\n",
      "Loss = 2.0925796031951904\n",
      "Training-Batch No.30350\n",
      "Loss = 2.026463031768799\n",
      "Training-Batch No.30360\n",
      "Loss = 2.5154154300689697\n",
      "Training-Batch No.30370\n",
      "Loss = 2.217679500579834\n",
      "Training-Batch No.30380\n",
      "Loss = 5.054626941680908\n",
      "Training-Batch No.30390\n",
      "Loss = 1.771275520324707\n",
      "Training-Batch No.30400\n",
      "Loss = 2.6854169368743896\n",
      "Training-Batch No.30410\n",
      "Loss = 1.3065743446350098\n",
      "Training-Batch No.30420\n",
      "Loss = 2.26973819732666\n",
      "Training-Batch No.30430\n",
      "Loss = 1.1184276342391968\n",
      "Training-Batch No.30440\n",
      "Loss = 2.1543757915496826\n",
      "Training-Batch No.30450\n",
      "Loss = 1.737205982208252\n",
      "Training-Batch No.30460\n",
      "Loss = 2.69016695022583\n",
      "Training-Batch No.30470\n",
      "Loss = 1.911428451538086\n",
      "Training-Batch No.30480\n",
      "Loss = 2.428896903991699\n",
      "Training-Batch No.30490\n",
      "Loss = 1.8694736957550049\n",
      "Training-Batch No.30500\n",
      "Loss = 1.5492794513702393\n",
      "Epoch 29 Training Loss: 2.1559\n",
      "Start validation\n",
      "Epoch 29 Validation Loss: 2.3960\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.3048026628625773\n",
      "Average Top-3 accuracy 0.6766524013314313\n",
      "Average Top-5 accuracy 0.8345221112696148\n",
      "Average Top-7 accuracy 0.9148834997622444\n",
      "Average Top-9 accuracy 0.9381835473133618\n",
      "Average Top-11 accuracy 0.9491203043271517\n",
      "Average Top-13 accuracy 0.9557774607703281\n",
      "Average Top-15 accuracy 0.9614835948644793\n",
      "current acc 0.3048026628625773\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 30\n",
      "Training-Batch No.30510\n",
      "Loss = 1.5710738897323608\n",
      "Training-Batch No.30520\n",
      "Loss = 2.7198009490966797\n",
      "Training-Batch No.30530\n",
      "Loss = 1.5252845287322998\n",
      "Training-Batch No.30540\n",
      "Loss = 4.062033653259277\n",
      "Training-Batch No.30550\n",
      "Loss = 2.7290711402893066\n",
      "Training-Batch No.30560\n",
      "Loss = 1.6023938655853271\n",
      "Training-Batch No.30570\n",
      "Loss = 1.5600759983062744\n",
      "Training-Batch No.30580\n",
      "Loss = 1.4019590616226196\n",
      "Training-Batch No.30590\n",
      "Loss = 2.1980557441711426\n",
      "Training-Batch No.30600\n",
      "Loss = 2.252098798751831\n",
      "Training-Batch No.30610\n",
      "Loss = 2.149029493331909\n",
      "Training-Batch No.30620\n",
      "Loss = 1.741457223892212\n",
      "Training-Batch No.30630\n",
      "Loss = 1.8918784856796265\n",
      "Training-Batch No.30640\n",
      "Loss = 1.947367787361145\n",
      "Training-Batch No.30650\n",
      "Loss = 2.4916539192199707\n",
      "Training-Batch No.30660\n",
      "Loss = 1.9986666440963745\n",
      "Training-Batch No.30670\n",
      "Loss = 3.904636859893799\n",
      "Training-Batch No.30680\n",
      "Loss = 2.394317150115967\n",
      "Training-Batch No.30690\n",
      "Loss = 1.3297539949417114\n",
      "Training-Batch No.30700\n",
      "Loss = 2.3494529724121094\n",
      "Training-Batch No.30710\n",
      "Loss = 2.4873366355895996\n",
      "Training-Batch No.30720\n",
      "Loss = 2.972153902053833\n",
      "Training-Batch No.30730\n",
      "Loss = 2.4170570373535156\n",
      "Training-Batch No.30740\n",
      "Loss = 1.4943890571594238\n",
      "Training-Batch No.30750\n",
      "Loss = 1.9979956150054932\n",
      "Training-Batch No.30760\n",
      "Loss = 1.9370591640472412\n",
      "Training-Batch No.30770\n",
      "Loss = 6.52004861831665\n",
      "Training-Batch No.30780\n",
      "Loss = 3.4971275329589844\n",
      "Training-Batch No.30790\n",
      "Loss = 2.098043203353882\n",
      "Training-Batch No.30800\n",
      "Loss = 1.6709706783294678\n",
      "Training-Batch No.30810\n",
      "Loss = 2.9640908241271973\n",
      "Training-Batch No.30820\n",
      "Loss = 1.864452600479126\n",
      "Training-Batch No.30830\n",
      "Loss = 1.4377474784851074\n",
      "Training-Batch No.30840\n",
      "Loss = 1.4495344161987305\n",
      "Training-Batch No.30850\n",
      "Loss = 2.2354211807250977\n",
      "Training-Batch No.30860\n",
      "Loss = 2.823064088821411\n",
      "Training-Batch No.30870\n",
      "Loss = 1.4264698028564453\n",
      "Training-Batch No.30880\n",
      "Loss = 2.2846004962921143\n",
      "Training-Batch No.30890\n",
      "Loss = 2.416409969329834\n",
      "Training-Batch No.30900\n",
      "Loss = 2.701014995574951\n",
      "Training-Batch No.30910\n",
      "Loss = 7.329061508178711\n",
      "Training-Batch No.30920\n",
      "Loss = 1.7377310991287231\n",
      "Training-Batch No.30930\n",
      "Loss = 1.6738958358764648\n",
      "Training-Batch No.30940\n",
      "Loss = 2.530343532562256\n",
      "Training-Batch No.30950\n",
      "Loss = 2.1936535835266113\n",
      "Training-Batch No.30960\n",
      "Loss = 2.19500732421875\n",
      "Training-Batch No.30970\n",
      "Loss = 8.212377548217773\n",
      "Training-Batch No.30980\n",
      "Loss = 1.3917378187179565\n",
      "Training-Batch No.30990\n",
      "Loss = 4.208953380584717\n",
      "Training-Batch No.31000\n",
      "Loss = 1.5392553806304932\n",
      "Training-Batch No.31010\n",
      "Loss = 2.3297958374023438\n",
      "Training-Batch No.31020\n",
      "Loss = 1.447088599205017\n",
      "Training-Batch No.31030\n",
      "Loss = 1.245983362197876\n",
      "Training-Batch No.31040\n",
      "Loss = 2.237867832183838\n",
      "Training-Batch No.31050\n",
      "Loss = 2.3351550102233887\n",
      "Training-Batch No.31060\n",
      "Loss = 1.3426389694213867\n",
      "Training-Batch No.31070\n",
      "Loss = 2.329604387283325\n",
      "Training-Batch No.31080\n",
      "Loss = 2.2219667434692383\n",
      "Training-Batch No.31090\n",
      "Loss = 1.3043609857559204\n",
      "Training-Batch No.31100\n",
      "Loss = 4.3075175285339355\n",
      "Training-Batch No.31110\n",
      "Loss = 1.7513519525527954\n",
      "Training-Batch No.31120\n",
      "Loss = 2.020113229751587\n",
      "Training-Batch No.31130\n",
      "Loss = 1.6266353130340576\n",
      "Training-Batch No.31140\n",
      "Loss = 2.5167038440704346\n",
      "Training-Batch No.31150\n",
      "Loss = 2.5196533203125\n",
      "Training-Batch No.31160\n",
      "Loss = 3.680323600769043\n",
      "Training-Batch No.31170\n",
      "Loss = 2.023247480392456\n",
      "Training-Batch No.31180\n",
      "Loss = 2.071558952331543\n",
      "Training-Batch No.31190\n",
      "Loss = 1.9026168584823608\n",
      "Training-Batch No.31200\n",
      "Loss = 1.613297462463379\n",
      "Training-Batch No.31210\n",
      "Loss = 1.687117099761963\n",
      "Training-Batch No.31220\n",
      "Loss = 2.4043312072753906\n",
      "Training-Batch No.31230\n",
      "Loss = 4.8547868728637695\n",
      "Training-Batch No.31240\n",
      "Loss = 1.435431957244873\n",
      "Training-Batch No.31250\n",
      "Loss = 1.9038476943969727\n",
      "Training-Batch No.31260\n",
      "Loss = 1.6664912700653076\n",
      "Training-Batch No.31270\n",
      "Loss = 1.378546953201294\n",
      "Training-Batch No.31280\n",
      "Loss = 1.7073191404342651\n",
      "Training-Batch No.31290\n",
      "Loss = 3.803971767425537\n",
      "Training-Batch No.31300\n",
      "Loss = 1.7774626016616821\n",
      "Training-Batch No.31310\n",
      "Loss = 1.9531131982803345\n",
      "Training-Batch No.31320\n",
      "Loss = 2.4213666915893555\n",
      "Training-Batch No.31330\n",
      "Loss = 1.5789825916290283\n",
      "Training-Batch No.31340\n",
      "Loss = 1.3085768222808838\n",
      "Training-Batch No.31350\n",
      "Loss = 1.996947169303894\n",
      "Training-Batch No.31360\n",
      "Loss = 5.037905216217041\n",
      "Training-Batch No.31370\n",
      "Loss = 1.70980703830719\n",
      "Training-Batch No.31380\n",
      "Loss = 1.4986752271652222\n",
      "Training-Batch No.31390\n",
      "Loss = 1.695868968963623\n",
      "Training-Batch No.31400\n",
      "Loss = 2.291661262512207\n",
      "Training-Batch No.31410\n",
      "Loss = 2.013441801071167\n",
      "Training-Batch No.31420\n",
      "Loss = 1.8666166067123413\n",
      "Training-Batch No.31430\n",
      "Loss = 2.3968281745910645\n",
      "Training-Batch No.31440\n",
      "Loss = 2.1475067138671875\n",
      "Training-Batch No.31450\n",
      "Loss = 1.1611605882644653\n",
      "Training-Batch No.31460\n",
      "Loss = 1.6379717588424683\n",
      "Training-Batch No.31470\n",
      "Loss = 2.5682239532470703\n",
      "Training-Batch No.31480\n",
      "Loss = 1.6395080089569092\n",
      "Training-Batch No.31490\n",
      "Loss = 1.6969521045684814\n",
      "Training-Batch No.31500\n",
      "Loss = 1.197698950767517\n",
      "Training-Batch No.31510\n",
      "Loss = 1.900762677192688\n",
      "Training-Batch No.31520\n",
      "Loss = 1.679482340812683\n",
      "Training-Batch No.31530\n",
      "Loss = 2.311382293701172\n",
      "Training-Batch No.31540\n",
      "Loss = 1.813591480255127\n",
      "Training-Batch No.31550\n",
      "Loss = 1.1082924604415894\n",
      "Training-Batch No.31560\n",
      "Loss = 1.8113552331924438\n",
      "Epoch 30 Training Loss: 2.1413\n",
      "Start validation\n",
      "Epoch 30 Validation Loss: 2.6941\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.25202092249167857\n",
      "Average Top-3 accuracy 0.5687113647170708\n",
      "Average Top-5 accuracy 0.7403708987161198\n",
      "Average Top-7 accuracy 0.828340466000951\n",
      "Average Top-9 accuracy 0.8735140275796481\n",
      "Average Top-11 accuracy 0.8930099857346647\n",
      "Average Top-13 accuracy 0.915359010936757\n",
      "Average Top-15 accuracy 0.9329529243937232\n",
      "current acc 0.25202092249167857\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 31\n",
      "Training-Batch No.31570\n",
      "Loss = 8.405302047729492\n",
      "Training-Batch No.31580\n",
      "Loss = 2.6785242557525635\n",
      "Training-Batch No.31590\n",
      "Loss = 2.192823886871338\n",
      "Training-Batch No.31600\n",
      "Loss = 1.554856777191162\n",
      "Training-Batch No.31610\n",
      "Loss = 1.719862461090088\n",
      "Training-Batch No.31620\n",
      "Loss = 2.140643835067749\n",
      "Training-Batch No.31630\n",
      "Loss = 2.0576372146606445\n",
      "Training-Batch No.31640\n",
      "Loss = 2.1643927097320557\n",
      "Training-Batch No.31650\n",
      "Loss = 2.009216785430908\n",
      "Training-Batch No.31660\n",
      "Loss = 1.9497959613800049\n",
      "Training-Batch No.31670\n",
      "Loss = 1.7825927734375\n",
      "Training-Batch No.31680\n",
      "Loss = 3.560861110687256\n",
      "Training-Batch No.31690\n",
      "Loss = 1.6050268411636353\n",
      "Training-Batch No.31700\n",
      "Loss = 2.027273416519165\n",
      "Training-Batch No.31710\n",
      "Loss = 1.802454948425293\n",
      "Training-Batch No.31720\n",
      "Loss = 2.059396982192993\n",
      "Training-Batch No.31730\n",
      "Loss = 1.6602126359939575\n",
      "Training-Batch No.31740\n",
      "Loss = 2.4465246200561523\n",
      "Training-Batch No.31750\n",
      "Loss = 1.667299509048462\n",
      "Training-Batch No.31760\n",
      "Loss = 3.1532108783721924\n",
      "Training-Batch No.31770\n",
      "Loss = 1.1697993278503418\n",
      "Training-Batch No.31780\n",
      "Loss = 2.1238529682159424\n",
      "Training-Batch No.31790\n",
      "Loss = 1.934883713722229\n",
      "Training-Batch No.31800\n",
      "Loss = 2.131509780883789\n",
      "Training-Batch No.31810\n",
      "Loss = 1.5966064929962158\n",
      "Training-Batch No.31820\n",
      "Loss = 1.8832811117172241\n",
      "Training-Batch No.31830\n",
      "Loss = 2.3868484497070312\n",
      "Training-Batch No.31840\n",
      "Loss = 3.12439227104187\n",
      "Training-Batch No.31850\n",
      "Loss = 2.08005952835083\n",
      "Training-Batch No.31860\n",
      "Loss = 1.6306548118591309\n",
      "Training-Batch No.31870\n",
      "Loss = 2.260862350463867\n",
      "Training-Batch No.31880\n",
      "Loss = 2.0534141063690186\n",
      "Training-Batch No.31890\n",
      "Loss = 2.3621182441711426\n",
      "Training-Batch No.31900\n",
      "Loss = 2.7044568061828613\n",
      "Training-Batch No.31910\n",
      "Loss = 1.6520740985870361\n",
      "Training-Batch No.31920\n",
      "Loss = 1.6751714944839478\n",
      "Training-Batch No.31930\n",
      "Loss = 2.2879867553710938\n",
      "Training-Batch No.31940\n",
      "Loss = 2.1191248893737793\n",
      "Training-Batch No.31950\n",
      "Loss = 1.051713466644287\n",
      "Training-Batch No.31960\n",
      "Loss = 3.193284034729004\n",
      "Training-Batch No.31970\n",
      "Loss = 1.7797794342041016\n",
      "Training-Batch No.31980\n",
      "Loss = 1.715510368347168\n",
      "Training-Batch No.31990\n",
      "Loss = 1.6695632934570312\n",
      "Training-Batch No.32000\n",
      "Loss = 3.3534717559814453\n",
      "Training-Batch No.32010\n",
      "Loss = 1.443328857421875\n",
      "Training-Batch No.32020\n",
      "Loss = 1.2898190021514893\n",
      "Training-Batch No.32030\n",
      "Loss = 2.3023359775543213\n",
      "Training-Batch No.32040\n",
      "Loss = 2.0874714851379395\n",
      "Training-Batch No.32050\n",
      "Loss = 2.410304069519043\n",
      "Training-Batch No.32060\n",
      "Loss = 1.286076307296753\n",
      "Training-Batch No.32070\n",
      "Loss = 1.4307281970977783\n",
      "Training-Batch No.32080\n",
      "Loss = 2.435014247894287\n",
      "Training-Batch No.32090\n",
      "Loss = 1.8444386720657349\n",
      "Training-Batch No.32100\n",
      "Loss = 1.7527854442596436\n",
      "Training-Batch No.32110\n",
      "Loss = 2.357504367828369\n",
      "Training-Batch No.32120\n",
      "Loss = 1.8542873859405518\n",
      "Training-Batch No.32130\n",
      "Loss = 2.4689249992370605\n",
      "Training-Batch No.32140\n",
      "Loss = 1.9884247779846191\n",
      "Training-Batch No.32150\n",
      "Loss = 1.9152085781097412\n",
      "Training-Batch No.32160\n",
      "Loss = 2.3239407539367676\n",
      "Training-Batch No.32170\n",
      "Loss = 2.653529167175293\n",
      "Training-Batch No.32180\n",
      "Loss = 1.6728832721710205\n",
      "Training-Batch No.32190\n",
      "Loss = 1.785127878189087\n",
      "Training-Batch No.32200\n",
      "Loss = 1.865691900253296\n",
      "Training-Batch No.32210\n",
      "Loss = 1.7998601198196411\n",
      "Training-Batch No.32220\n",
      "Loss = 1.5469415187835693\n",
      "Training-Batch No.32230\n",
      "Loss = 2.4150278568267822\n",
      "Training-Batch No.32240\n",
      "Loss = 1.7220546007156372\n",
      "Training-Batch No.32250\n",
      "Loss = 1.2338976860046387\n",
      "Training-Batch No.32260\n",
      "Loss = 1.3385896682739258\n",
      "Training-Batch No.32270\n",
      "Loss = 1.9446152448654175\n",
      "Training-Batch No.32280\n",
      "Loss = 7.876478672027588\n",
      "Training-Batch No.32290\n",
      "Loss = 1.901012659072876\n",
      "Training-Batch No.32300\n",
      "Loss = 2.281097650527954\n",
      "Training-Batch No.32310\n",
      "Loss = 2.517218828201294\n",
      "Training-Batch No.32320\n",
      "Loss = 3.6794610023498535\n",
      "Training-Batch No.32330\n",
      "Loss = 3.5650076866149902\n",
      "Training-Batch No.32340\n",
      "Loss = 2.1552038192749023\n",
      "Training-Batch No.32350\n",
      "Loss = 1.5579218864440918\n",
      "Training-Batch No.32360\n",
      "Loss = 2.29805588722229\n",
      "Training-Batch No.32370\n",
      "Loss = 1.6669557094573975\n",
      "Training-Batch No.32380\n",
      "Loss = 1.5952565670013428\n",
      "Training-Batch No.32390\n",
      "Loss = 9.869391441345215\n",
      "Training-Batch No.32400\n",
      "Loss = 2.291719436645508\n",
      "Training-Batch No.32410\n",
      "Loss = 1.9139407873153687\n",
      "Training-Batch No.32420\n",
      "Loss = 3.414440155029297\n",
      "Training-Batch No.32430\n",
      "Loss = 1.5541419982910156\n",
      "Training-Batch No.32440\n",
      "Loss = 1.4507560729980469\n",
      "Training-Batch No.32450\n",
      "Loss = 2.6807010173797607\n",
      "Training-Batch No.32460\n",
      "Loss = 1.4513030052185059\n",
      "Training-Batch No.32470\n",
      "Loss = 1.716286540031433\n",
      "Training-Batch No.32480\n",
      "Loss = 2.2515687942504883\n",
      "Training-Batch No.32490\n",
      "Loss = 1.313722848892212\n",
      "Training-Batch No.32500\n",
      "Loss = 1.2682679891586304\n",
      "Training-Batch No.32510\n",
      "Loss = 1.6156792640686035\n",
      "Training-Batch No.32520\n",
      "Loss = 2.030867338180542\n",
      "Training-Batch No.32530\n",
      "Loss = 1.5652687549591064\n",
      "Training-Batch No.32540\n",
      "Loss = 1.4675564765930176\n",
      "Training-Batch No.32550\n",
      "Loss = 1.1761763095855713\n",
      "Training-Batch No.32560\n",
      "Loss = 1.0607560873031616\n",
      "Training-Batch No.32570\n",
      "Loss = 1.429648518562317\n",
      "Training-Batch No.32580\n",
      "Loss = 2.209087610244751\n",
      "Training-Batch No.32590\n",
      "Loss = 1.4109141826629639\n",
      "Training-Batch No.32600\n",
      "Loss = 2.0590660572052\n",
      "Training-Batch No.32610\n",
      "Loss = 2.2677266597747803\n",
      "Epoch 31 Training Loss: 2.1270\n",
      "Start validation\n",
      "Epoch 31 Validation Loss: 3.8110\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.052781740370898715\n",
      "Average Top-3 accuracy 0.24108416547788872\n",
      "Average Top-5 accuracy 0.49310508796956726\n",
      "Average Top-7 accuracy 0.5929624346172135\n",
      "Average Top-9 accuracy 0.6975748930099858\n",
      "Average Top-11 accuracy 0.7560627674750356\n",
      "Average Top-13 accuracy 0.806942463147884\n",
      "Average Top-15 accuracy 0.849738468854018\n",
      "current acc 0.052781740370898715\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 32\n",
      "Training-Batch No.32620\n",
      "Loss = 2.3836851119995117\n",
      "Training-Batch No.32630\n",
      "Loss = 1.5362671613693237\n",
      "Training-Batch No.32640\n",
      "Loss = 1.5248291492462158\n",
      "Training-Batch No.32650\n",
      "Loss = 1.4963605403900146\n",
      "Training-Batch No.32660\n",
      "Loss = 1.8173919916152954\n",
      "Training-Batch No.32670\n",
      "Loss = 2.117598295211792\n",
      "Training-Batch No.32680\n",
      "Loss = 1.7167123556137085\n",
      "Training-Batch No.32690\n",
      "Loss = 3.4742331504821777\n",
      "Training-Batch No.32700\n",
      "Loss = 1.5648630857467651\n",
      "Training-Batch No.32710\n",
      "Loss = 1.3523545265197754\n",
      "Training-Batch No.32720\n",
      "Loss = 5.506038665771484\n",
      "Training-Batch No.32730\n",
      "Loss = 1.553552508354187\n",
      "Training-Batch No.32740\n",
      "Loss = 1.4959205389022827\n",
      "Training-Batch No.32750\n",
      "Loss = 1.8269124031066895\n",
      "Training-Batch No.32760\n",
      "Loss = 1.5474610328674316\n",
      "Training-Batch No.32770\n",
      "Loss = 2.400529146194458\n",
      "Training-Batch No.32780\n",
      "Loss = 1.7099287509918213\n",
      "Training-Batch No.32790\n",
      "Loss = 2.7308220863342285\n",
      "Training-Batch No.32800\n",
      "Loss = 3.5711002349853516\n",
      "Training-Batch No.32810\n",
      "Loss = 2.0808303356170654\n",
      "Training-Batch No.32820\n",
      "Loss = 1.909321665763855\n",
      "Training-Batch No.32830\n",
      "Loss = 1.7329201698303223\n",
      "Training-Batch No.32840\n",
      "Loss = 1.605802059173584\n",
      "Training-Batch No.32850\n",
      "Loss = 2.084261894226074\n",
      "Training-Batch No.32860\n",
      "Loss = 4.494613170623779\n",
      "Training-Batch No.32870\n",
      "Loss = 1.9275882244110107\n",
      "Training-Batch No.32880\n",
      "Loss = 1.6275367736816406\n",
      "Training-Batch No.32890\n",
      "Loss = 1.525805115699768\n",
      "Training-Batch No.32900\n",
      "Loss = 1.4570984840393066\n",
      "Training-Batch No.32910\n",
      "Loss = 1.6914297342300415\n",
      "Training-Batch No.32920\n",
      "Loss = 2.6545870304107666\n",
      "Training-Batch No.32930\n",
      "Loss = 1.7535181045532227\n",
      "Training-Batch No.32940\n",
      "Loss = 1.7658888101577759\n",
      "Training-Batch No.32950\n",
      "Loss = 1.4655537605285645\n",
      "Training-Batch No.32960\n",
      "Loss = 1.8711774349212646\n",
      "Training-Batch No.32970\n",
      "Loss = 1.4662269353866577\n",
      "Training-Batch No.32980\n",
      "Loss = 1.51437509059906\n",
      "Training-Batch No.32990\n",
      "Loss = 2.8813462257385254\n",
      "Training-Batch No.33000\n",
      "Loss = 1.9427461624145508\n",
      "Training-Batch No.33010\n",
      "Loss = 1.7951476573944092\n",
      "Training-Batch No.33020\n",
      "Loss = 2.644721508026123\n",
      "Training-Batch No.33030\n",
      "Loss = 1.6392865180969238\n",
      "Training-Batch No.33040\n",
      "Loss = 1.608233094215393\n",
      "Training-Batch No.33050\n",
      "Loss = 2.0098037719726562\n",
      "Training-Batch No.33060\n",
      "Loss = 3.4039132595062256\n",
      "Training-Batch No.33070\n",
      "Loss = 1.547067403793335\n",
      "Training-Batch No.33080\n",
      "Loss = 2.2629776000976562\n",
      "Training-Batch No.33090\n",
      "Loss = 1.4020483493804932\n",
      "Training-Batch No.33100\n",
      "Loss = 2.019680976867676\n",
      "Training-Batch No.33110\n",
      "Loss = 1.0893176794052124\n",
      "Training-Batch No.33120\n",
      "Loss = 1.2331708669662476\n",
      "Training-Batch No.33130\n",
      "Loss = 1.470166563987732\n",
      "Training-Batch No.33140\n",
      "Loss = 1.573056697845459\n",
      "Training-Batch No.33150\n",
      "Loss = 1.5306569337844849\n",
      "Training-Batch No.33160\n",
      "Loss = 1.9352716207504272\n",
      "Training-Batch No.33170\n",
      "Loss = 3.119680404663086\n",
      "Training-Batch No.33180\n",
      "Loss = 2.8526461124420166\n",
      "Training-Batch No.33190\n",
      "Loss = 1.521239995956421\n",
      "Training-Batch No.33200\n",
      "Loss = 2.1724023818969727\n",
      "Training-Batch No.33210\n",
      "Loss = 1.9668554067611694\n",
      "Training-Batch No.33220\n",
      "Loss = 2.0152127742767334\n",
      "Training-Batch No.33230\n",
      "Loss = 1.5353529453277588\n",
      "Training-Batch No.33240\n",
      "Loss = 1.5236353874206543\n",
      "Training-Batch No.33250\n",
      "Loss = 2.8384668827056885\n",
      "Training-Batch No.33260\n",
      "Loss = 2.484070062637329\n",
      "Training-Batch No.33270\n",
      "Loss = 1.5248205661773682\n",
      "Training-Batch No.33280\n",
      "Loss = 1.9105802774429321\n",
      "Training-Batch No.33290\n",
      "Loss = 2.055203914642334\n",
      "Training-Batch No.33300\n",
      "Loss = 2.3163321018218994\n",
      "Training-Batch No.33310\n",
      "Loss = 1.3380894660949707\n",
      "Training-Batch No.33320\n",
      "Loss = 2.1233632564544678\n",
      "Training-Batch No.33330\n",
      "Loss = 1.2353582382202148\n",
      "Training-Batch No.33340\n",
      "Loss = 2.3525919914245605\n",
      "Training-Batch No.33350\n",
      "Loss = 2.458587646484375\n",
      "Training-Batch No.33360\n",
      "Loss = 2.151808261871338\n",
      "Training-Batch No.33370\n",
      "Loss = 1.9442955255508423\n",
      "Training-Batch No.33380\n",
      "Loss = 1.0456637144088745\n",
      "Training-Batch No.33390\n",
      "Loss = 2.076686382293701\n",
      "Training-Batch No.33400\n",
      "Loss = 2.3928864002227783\n",
      "Training-Batch No.33410\n",
      "Loss = 2.273639440536499\n",
      "Training-Batch No.33420\n",
      "Loss = 1.529571533203125\n",
      "Training-Batch No.33430\n",
      "Loss = 1.4301328659057617\n",
      "Training-Batch No.33440\n",
      "Loss = 2.3662071228027344\n",
      "Training-Batch No.33450\n",
      "Loss = 1.4334182739257812\n",
      "Training-Batch No.33460\n",
      "Loss = 2.64876127243042\n",
      "Training-Batch No.33470\n",
      "Loss = 2.910431146621704\n",
      "Training-Batch No.33480\n",
      "Loss = 1.2944965362548828\n",
      "Training-Batch No.33490\n",
      "Loss = 2.198927402496338\n",
      "Training-Batch No.33500\n",
      "Loss = 1.2081780433654785\n",
      "Training-Batch No.33510\n",
      "Loss = 1.8256311416625977\n",
      "Training-Batch No.33520\n",
      "Loss = 1.7248053550720215\n",
      "Training-Batch No.33530\n",
      "Loss = 1.972556710243225\n",
      "Training-Batch No.33540\n",
      "Loss = 1.7762086391448975\n",
      "Training-Batch No.33550\n",
      "Loss = 2.7256250381469727\n",
      "Training-Batch No.33560\n",
      "Loss = 1.8784171342849731\n",
      "Training-Batch No.33570\n",
      "Loss = 1.6062190532684326\n",
      "Training-Batch No.33580\n",
      "Loss = 1.696519136428833\n",
      "Training-Batch No.33590\n",
      "Loss = 1.9608747959136963\n",
      "Training-Batch No.33600\n",
      "Loss = 2.00893497467041\n",
      "Training-Batch No.33610\n",
      "Loss = 1.6852967739105225\n",
      "Training-Batch No.33620\n",
      "Loss = 1.306481122970581\n",
      "Training-Batch No.33630\n",
      "Loss = 2.6424436569213867\n",
      "Training-Batch No.33640\n",
      "Loss = 1.6527607440948486\n",
      "Training-Batch No.33650\n",
      "Loss = 1.8013465404510498\n",
      "Training-Batch No.33660\n",
      "Loss = 1.713180422782898\n",
      "Epoch 32 Training Loss: 2.1462\n",
      "Start validation\n",
      "Epoch 32 Validation Loss: 2.8563\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.20684736091298145\n",
      "Average Top-3 accuracy 0.4526866381359962\n",
      "Average Top-5 accuracy 0.6333808844507846\n",
      "Average Top-7 accuracy 0.7403708987161198\n",
      "Average Top-9 accuracy 0.8273894436519258\n",
      "Average Top-11 accuracy 0.8720874940561103\n",
      "Average Top-13 accuracy 0.9053732762719924\n",
      "Average Top-15 accuracy 0.9310508796956728\n",
      "current acc 0.20684736091298145\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 33\n",
      "Training-Batch No.33670\n",
      "Loss = 2.0885567665100098\n",
      "Training-Batch No.33680\n",
      "Loss = 1.7551451921463013\n",
      "Training-Batch No.33690\n",
      "Loss = 1.2831801176071167\n",
      "Training-Batch No.33700\n",
      "Loss = 1.9457037448883057\n",
      "Training-Batch No.33710\n",
      "Loss = 1.4696259498596191\n",
      "Training-Batch No.33720\n",
      "Loss = 2.0990195274353027\n",
      "Training-Batch No.33730\n",
      "Loss = 6.02576208114624\n",
      "Training-Batch No.33740\n",
      "Loss = 1.3850038051605225\n",
      "Training-Batch No.33750\n",
      "Loss = 1.3167917728424072\n",
      "Training-Batch No.33760\n",
      "Loss = 3.1361589431762695\n",
      "Training-Batch No.33770\n",
      "Loss = 1.7419369220733643\n",
      "Training-Batch No.33780\n",
      "Loss = 1.2303563356399536\n",
      "Training-Batch No.33790\n",
      "Loss = 3.525679111480713\n",
      "Training-Batch No.33800\n",
      "Loss = 2.0730581283569336\n",
      "Training-Batch No.33810\n",
      "Loss = 2.005922555923462\n",
      "Training-Batch No.33820\n",
      "Loss = 1.4978139400482178\n",
      "Training-Batch No.33830\n",
      "Loss = 1.4423754215240479\n",
      "Training-Batch No.33840\n",
      "Loss = 1.9118785858154297\n",
      "Training-Batch No.33850\n",
      "Loss = 4.234394073486328\n",
      "Training-Batch No.33860\n",
      "Loss = 1.8144495487213135\n",
      "Training-Batch No.33870\n",
      "Loss = 2.706278085708618\n",
      "Training-Batch No.33880\n",
      "Loss = 1.813987135887146\n",
      "Training-Batch No.33890\n",
      "Loss = 1.233596920967102\n",
      "Training-Batch No.33900\n",
      "Loss = 1.7035605907440186\n",
      "Training-Batch No.33910\n",
      "Loss = 1.4728151559829712\n",
      "Training-Batch No.33920\n",
      "Loss = 1.8336265087127686\n",
      "Training-Batch No.33930\n",
      "Loss = 1.5201408863067627\n",
      "Training-Batch No.33940\n",
      "Loss = 1.5640758275985718\n",
      "Training-Batch No.33950\n",
      "Loss = 1.4993468523025513\n",
      "Training-Batch No.33960\n",
      "Loss = 3.64093017578125\n",
      "Training-Batch No.33970\n",
      "Loss = 1.7754836082458496\n",
      "Training-Batch No.33980\n",
      "Loss = 1.8269617557525635\n",
      "Training-Batch No.33990\n",
      "Loss = 1.6566494703292847\n",
      "Training-Batch No.34000\n",
      "Loss = 2.2767488956451416\n",
      "Training-Batch No.34010\n",
      "Loss = 2.4661638736724854\n",
      "Training-Batch No.34020\n",
      "Loss = 1.3455142974853516\n",
      "Training-Batch No.34030\n",
      "Loss = 1.7063877582550049\n",
      "Training-Batch No.34040\n",
      "Loss = 2.3545894622802734\n",
      "Training-Batch No.34050\n",
      "Loss = 2.21579647064209\n",
      "Training-Batch No.34060\n",
      "Loss = 4.170378684997559\n",
      "Training-Batch No.34070\n",
      "Loss = 2.3358469009399414\n",
      "Training-Batch No.34080\n",
      "Loss = 1.98799729347229\n",
      "Training-Batch No.34090\n",
      "Loss = 1.7372816801071167\n",
      "Training-Batch No.34100\n",
      "Loss = 1.6921250820159912\n",
      "Training-Batch No.34110\n",
      "Loss = 1.3953372240066528\n",
      "Training-Batch No.34120\n",
      "Loss = 2.0822231769561768\n",
      "Training-Batch No.34130\n",
      "Loss = 1.643054723739624\n",
      "Training-Batch No.34140\n",
      "Loss = 1.777401089668274\n",
      "Training-Batch No.34150\n",
      "Loss = 1.2187561988830566\n",
      "Training-Batch No.34160\n",
      "Loss = 1.678478479385376\n",
      "Training-Batch No.34170\n",
      "Loss = 1.8350169658660889\n",
      "Training-Batch No.34180\n",
      "Loss = 2.0066816806793213\n",
      "Training-Batch No.34190\n",
      "Loss = 1.4224679470062256\n",
      "Training-Batch No.34200\n",
      "Loss = 2.097501754760742\n",
      "Training-Batch No.34210\n",
      "Loss = 1.1575497388839722\n",
      "Training-Batch No.34220\n",
      "Loss = 1.626217007637024\n",
      "Training-Batch No.34230\n",
      "Loss = 1.6073261499404907\n",
      "Training-Batch No.34240\n",
      "Loss = 1.450703501701355\n",
      "Training-Batch No.34250\n",
      "Loss = 1.3998842239379883\n",
      "Training-Batch No.34260\n",
      "Loss = 1.4771301746368408\n",
      "Training-Batch No.34270\n",
      "Loss = 1.8139666318893433\n",
      "Training-Batch No.34280\n",
      "Loss = 1.970986008644104\n",
      "Training-Batch No.34290\n",
      "Loss = 1.7994815111160278\n",
      "Training-Batch No.34300\n",
      "Loss = 1.9752511978149414\n",
      "Training-Batch No.34310\n",
      "Loss = 1.3151195049285889\n",
      "Training-Batch No.34320\n",
      "Loss = 1.3167513608932495\n",
      "Training-Batch No.34330\n",
      "Loss = 2.764543294906616\n",
      "Training-Batch No.34340\n",
      "Loss = 1.6479498147964478\n",
      "Training-Batch No.34350\n",
      "Loss = 1.485661268234253\n",
      "Training-Batch No.34360\n",
      "Loss = 2.1159610748291016\n",
      "Training-Batch No.34370\n",
      "Loss = 3.359744071960449\n",
      "Training-Batch No.34380\n",
      "Loss = 2.8073854446411133\n",
      "Training-Batch No.34390\n",
      "Loss = 1.9394081830978394\n",
      "Training-Batch No.34400\n",
      "Loss = 1.2619154453277588\n",
      "Training-Batch No.34410\n",
      "Loss = 2.1921424865722656\n",
      "Training-Batch No.34420\n",
      "Loss = 1.9787343740463257\n",
      "Training-Batch No.34430\n",
      "Loss = 2.056504726409912\n",
      "Training-Batch No.34440\n",
      "Loss = 3.391268253326416\n",
      "Training-Batch No.34450\n",
      "Loss = 2.0451669692993164\n",
      "Training-Batch No.34460\n",
      "Loss = 1.4963183403015137\n",
      "Training-Batch No.34470\n",
      "Loss = 1.5601612329483032\n",
      "Training-Batch No.34480\n",
      "Loss = 2.992184638977051\n",
      "Training-Batch No.34490\n",
      "Loss = 1.9493510723114014\n",
      "Training-Batch No.34500\n",
      "Loss = 1.9807089567184448\n",
      "Training-Batch No.34510\n",
      "Loss = 2.398674488067627\n",
      "Training-Batch No.34520\n",
      "Loss = 2.2238917350769043\n",
      "Training-Batch No.34530\n",
      "Loss = 3.8723204135894775\n",
      "Training-Batch No.34540\n",
      "Loss = 1.4850616455078125\n",
      "Training-Batch No.34550\n",
      "Loss = 1.177025318145752\n",
      "Training-Batch No.34560\n",
      "Loss = 2.9290285110473633\n",
      "Training-Batch No.34570\n",
      "Loss = 1.6121115684509277\n",
      "Training-Batch No.34580\n",
      "Loss = 1.7096308469772339\n",
      "Training-Batch No.34590\n",
      "Loss = 1.528825283050537\n",
      "Training-Batch No.34600\n",
      "Loss = 1.5853271484375\n",
      "Training-Batch No.34610\n",
      "Loss = 2.7171590328216553\n",
      "Training-Batch No.34620\n",
      "Loss = 1.2599356174468994\n",
      "Training-Batch No.34630\n",
      "Loss = 1.7901521921157837\n",
      "Training-Batch No.34640\n",
      "Loss = 1.7499741315841675\n",
      "Training-Batch No.34650\n",
      "Loss = 1.3928011655807495\n",
      "Training-Batch No.34660\n",
      "Loss = 1.547394037246704\n",
      "Training-Batch No.34670\n",
      "Loss = 1.7216455936431885\n",
      "Training-Batch No.34680\n",
      "Loss = 2.4602203369140625\n",
      "Training-Batch No.34690\n",
      "Loss = 1.697914719581604\n",
      "Training-Batch No.34700\n",
      "Loss = 1.936008095741272\n",
      "Training-Batch No.34710\n",
      "Loss = 2.385204792022705\n",
      "Epoch 33 Training Loss: 2.1146\n",
      "Start validation\n",
      "Epoch 33 Validation Loss: 4.0354\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.11126961483594865\n",
      "Average Top-3 accuracy 0.3223965763195435\n",
      "Average Top-5 accuracy 0.49120304327151687\n",
      "Average Top-7 accuracy 0.5701378982406087\n",
      "Average Top-9 accuracy 0.6476462196861626\n",
      "Average Top-11 accuracy 0.7146932952924394\n",
      "Average Top-13 accuracy 0.7622444127436995\n",
      "Average Top-15 accuracy 0.7950546837850689\n",
      "current acc 0.11126961483594865\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 34\n",
      "Training-Batch No.34720\n",
      "Loss = 2.115593433380127\n",
      "Training-Batch No.34730\n",
      "Loss = 2.917715311050415\n",
      "Training-Batch No.34740\n",
      "Loss = 1.9590262174606323\n",
      "Training-Batch No.34750\n",
      "Loss = 1.958370327949524\n",
      "Training-Batch No.34760\n",
      "Loss = 2.306171178817749\n",
      "Training-Batch No.34770\n",
      "Loss = 2.494741678237915\n",
      "Training-Batch No.34780\n",
      "Loss = 1.7148674726486206\n",
      "Training-Batch No.34790\n",
      "Loss = 2.853440284729004\n",
      "Training-Batch No.34800\n",
      "Loss = 2.313450813293457\n",
      "Training-Batch No.34810\n",
      "Loss = 2.1092846393585205\n",
      "Training-Batch No.34820\n",
      "Loss = 1.6879916191101074\n",
      "Training-Batch No.34830\n",
      "Loss = 1.639850378036499\n",
      "Training-Batch No.34840\n",
      "Loss = 1.2330420017242432\n",
      "Training-Batch No.34850\n",
      "Loss = 2.4286746978759766\n",
      "Training-Batch No.34860\n",
      "Loss = 1.6092665195465088\n",
      "Training-Batch No.34870\n",
      "Loss = 1.5379924774169922\n",
      "Training-Batch No.34880\n",
      "Loss = 3.6944375038146973\n",
      "Training-Batch No.34890\n",
      "Loss = 2.200761318206787\n",
      "Training-Batch No.34900\n",
      "Loss = 2.5790531635284424\n",
      "Training-Batch No.34910\n",
      "Loss = 3.5509026050567627\n",
      "Training-Batch No.34920\n",
      "Loss = 2.6784310340881348\n",
      "Training-Batch No.34930\n",
      "Loss = 1.8785144090652466\n",
      "Training-Batch No.34940\n",
      "Loss = 2.0132787227630615\n",
      "Training-Batch No.34950\n",
      "Loss = 1.6097931861877441\n",
      "Training-Batch No.34960\n",
      "Loss = 1.5582854747772217\n",
      "Training-Batch No.34970\n",
      "Loss = 1.7354364395141602\n",
      "Training-Batch No.34980\n",
      "Loss = 1.3428771495819092\n",
      "Training-Batch No.34990\n",
      "Loss = 2.4452364444732666\n",
      "Training-Batch No.35000\n",
      "Loss = 6.827390670776367\n",
      "Training-Batch No.35010\n",
      "Loss = 4.165233612060547\n",
      "Training-Batch No.35020\n",
      "Loss = 1.9978463649749756\n",
      "Training-Batch No.35030\n",
      "Loss = 2.1128344535827637\n",
      "Training-Batch No.35040\n",
      "Loss = 1.954369306564331\n",
      "Training-Batch No.35050\n",
      "Loss = 1.5542031526565552\n",
      "Training-Batch No.35060\n",
      "Loss = 1.9624608755111694\n",
      "Training-Batch No.35070\n",
      "Loss = 2.426769256591797\n",
      "Training-Batch No.35080\n",
      "Loss = 2.402299642562866\n",
      "Training-Batch No.35090\n",
      "Loss = 1.5903266668319702\n",
      "Training-Batch No.35100\n",
      "Loss = 4.401307106018066\n",
      "Training-Batch No.35110\n",
      "Loss = 1.6879440546035767\n",
      "Training-Batch No.35120\n",
      "Loss = 2.4124844074249268\n",
      "Training-Batch No.35130\n",
      "Loss = 2.323233127593994\n",
      "Training-Batch No.35140\n",
      "Loss = 2.0270309448242188\n",
      "Training-Batch No.35150\n",
      "Loss = 1.46474289894104\n",
      "Training-Batch No.35160\n",
      "Loss = 1.0946201086044312\n",
      "Training-Batch No.35170\n",
      "Loss = 2.145951747894287\n",
      "Training-Batch No.35180\n",
      "Loss = 2.0571882724761963\n",
      "Training-Batch No.35190\n",
      "Loss = 1.988273024559021\n",
      "Training-Batch No.35200\n",
      "Loss = 1.542266845703125\n",
      "Training-Batch No.35210\n",
      "Loss = 1.8833576440811157\n",
      "Training-Batch No.35220\n",
      "Loss = 1.8187966346740723\n",
      "Training-Batch No.35230\n",
      "Loss = 2.1118574142456055\n",
      "Training-Batch No.35240\n",
      "Loss = 1.952747106552124\n",
      "Training-Batch No.35250\n",
      "Loss = 1.886914849281311\n",
      "Training-Batch No.35260\n",
      "Loss = 1.3140089511871338\n",
      "Training-Batch No.35270\n",
      "Loss = 3.9585742950439453\n",
      "Training-Batch No.35280\n",
      "Loss = 4.141470909118652\n",
      "Training-Batch No.35290\n",
      "Loss = 3.776851177215576\n",
      "Training-Batch No.35300\n",
      "Loss = 1.1587284803390503\n",
      "Training-Batch No.35310\n",
      "Loss = 2.1267051696777344\n",
      "Training-Batch No.35320\n",
      "Loss = 1.4775316715240479\n",
      "Training-Batch No.35330\n",
      "Loss = 1.6563187837600708\n",
      "Training-Batch No.35340\n",
      "Loss = 2.5708184242248535\n",
      "Training-Batch No.35350\n",
      "Loss = 2.468210220336914\n",
      "Training-Batch No.35360\n",
      "Loss = 1.9003900289535522\n",
      "Training-Batch No.35370\n",
      "Loss = 2.054043769836426\n",
      "Training-Batch No.35380\n",
      "Loss = 2.120199680328369\n",
      "Training-Batch No.35390\n",
      "Loss = 2.001396417617798\n",
      "Training-Batch No.35400\n",
      "Loss = 1.852401614189148\n",
      "Training-Batch No.35410\n",
      "Loss = 2.8581666946411133\n",
      "Training-Batch No.35420\n",
      "Loss = 1.4515103101730347\n",
      "Training-Batch No.35430\n",
      "Loss = 1.5343621969223022\n",
      "Training-Batch No.35440\n",
      "Loss = 2.2744503021240234\n",
      "Training-Batch No.35450\n",
      "Loss = 3.263684034347534\n",
      "Training-Batch No.35460\n",
      "Loss = 3.2307538986206055\n",
      "Training-Batch No.35470\n",
      "Loss = 1.5586025714874268\n",
      "Training-Batch No.35480\n",
      "Loss = 1.7147728204727173\n",
      "Training-Batch No.35490\n",
      "Loss = 1.5534840822219849\n",
      "Training-Batch No.35500\n",
      "Loss = 1.7915438413619995\n",
      "Training-Batch No.35510\n",
      "Loss = 1.548391342163086\n",
      "Training-Batch No.35520\n",
      "Loss = 3.297743558883667\n",
      "Training-Batch No.35530\n",
      "Loss = 2.0987327098846436\n",
      "Training-Batch No.35540\n",
      "Loss = 1.8098411560058594\n",
      "Training-Batch No.35550\n",
      "Loss = 1.5693024396896362\n",
      "Training-Batch No.35560\n",
      "Loss = 1.6387940645217896\n",
      "Training-Batch No.35570\n",
      "Loss = 1.2136211395263672\n",
      "Training-Batch No.35580\n",
      "Loss = 2.0104708671569824\n",
      "Training-Batch No.35590\n",
      "Loss = 2.8118896484375\n",
      "Training-Batch No.35600\n",
      "Loss = 1.9407165050506592\n",
      "Training-Batch No.35610\n",
      "Loss = 1.9967918395996094\n",
      "Training-Batch No.35620\n",
      "Loss = 2.4667296409606934\n",
      "Training-Batch No.35630\n",
      "Loss = 2.270979642868042\n",
      "Training-Batch No.35640\n",
      "Loss = 5.071897983551025\n",
      "Training-Batch No.35650\n",
      "Loss = 1.788534164428711\n",
      "Training-Batch No.35660\n",
      "Loss = 2.6077842712402344\n",
      "Training-Batch No.35670\n",
      "Loss = 1.217344045639038\n",
      "Training-Batch No.35680\n",
      "Loss = 2.1829075813293457\n",
      "Training-Batch No.35690\n",
      "Loss = 1.3082504272460938\n",
      "Training-Batch No.35700\n",
      "Loss = 1.7112221717834473\n",
      "Training-Batch No.35710\n",
      "Loss = 1.6043510437011719\n",
      "Training-Batch No.35720\n",
      "Loss = 2.613481044769287\n",
      "Training-Batch No.35730\n",
      "Loss = 1.8799118995666504\n",
      "Training-Batch No.35740\n",
      "Loss = 2.3185923099517822\n",
      "Training-Batch No.35750\n",
      "Loss = 1.6789097785949707\n",
      "Training-Batch No.35760\n",
      "Loss = 1.4769047498703003\n",
      "Epoch 34 Training Loss: 2.1135\n",
      "Start validation\n",
      "Epoch 34 Validation Loss: 2.3591\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.3509272467902996\n",
      "Average Top-3 accuracy 0.6994769377080361\n",
      "Average Top-5 accuracy 0.8326200665715644\n",
      "Average Top-7 accuracy 0.9191631003328579\n",
      "Average Top-9 accuracy 0.9377080361388492\n",
      "Average Top-11 accuracy 0.9481692819781264\n",
      "Average Top-13 accuracy 0.9591060389919163\n",
      "Average Top-15 accuracy 0.9686162624821684\n",
      "current acc 0.3509272467902996\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 35\n",
      "Training-Batch No.35770\n",
      "Loss = 1.4903556108474731\n",
      "Training-Batch No.35780\n",
      "Loss = 2.6796212196350098\n",
      "Training-Batch No.35790\n",
      "Loss = 1.483229637145996\n",
      "Training-Batch No.35800\n",
      "Loss = 3.4222593307495117\n",
      "Training-Batch No.35810\n",
      "Loss = 3.0341827869415283\n",
      "Training-Batch No.35820\n",
      "Loss = 1.4846546649932861\n",
      "Training-Batch No.35830\n",
      "Loss = 1.4721087217330933\n",
      "Training-Batch No.35840\n",
      "Loss = 1.2077417373657227\n",
      "Training-Batch No.35850\n",
      "Loss = 2.084245443344116\n",
      "Training-Batch No.35860\n",
      "Loss = 2.0905981063842773\n",
      "Training-Batch No.35870\n",
      "Loss = 2.257378339767456\n",
      "Training-Batch No.35880\n",
      "Loss = 1.7180192470550537\n",
      "Training-Batch No.35890\n",
      "Loss = 1.916704773902893\n",
      "Training-Batch No.35900\n",
      "Loss = 1.8673250675201416\n",
      "Training-Batch No.35910\n",
      "Loss = 2.4524900913238525\n",
      "Training-Batch No.35920\n",
      "Loss = 2.061924457550049\n",
      "Training-Batch No.35930\n",
      "Loss = 3.513345241546631\n",
      "Training-Batch No.35940\n",
      "Loss = 2.312838315963745\n",
      "Training-Batch No.35950\n",
      "Loss = 1.3271633386611938\n",
      "Training-Batch No.35960\n",
      "Loss = 2.2709548473358154\n",
      "Training-Batch No.35970\n",
      "Loss = 2.6143908500671387\n",
      "Training-Batch No.35980\n",
      "Loss = 3.2153992652893066\n",
      "Training-Batch No.35990\n",
      "Loss = 2.5078141689300537\n",
      "Training-Batch No.36000\n",
      "Loss = 1.4776924848556519\n",
      "Training-Batch No.36010\n",
      "Loss = 1.8616077899932861\n",
      "Training-Batch No.36020\n",
      "Loss = 1.904065728187561\n",
      "Training-Batch No.36030\n",
      "Loss = 6.59458589553833\n",
      "Training-Batch No.36040\n",
      "Loss = 3.6404526233673096\n",
      "Training-Batch No.36050\n",
      "Loss = 1.9956969022750854\n",
      "Training-Batch No.36060\n",
      "Loss = 1.6342872381210327\n",
      "Training-Batch No.36070\n",
      "Loss = 1.9645001888275146\n",
      "Training-Batch No.36080\n",
      "Loss = 1.7741882801055908\n",
      "Training-Batch No.36090\n",
      "Loss = 1.3767037391662598\n",
      "Training-Batch No.36100\n",
      "Loss = 1.5084903240203857\n",
      "Training-Batch No.36110\n",
      "Loss = 2.171513080596924\n",
      "Training-Batch No.36120\n",
      "Loss = 2.9619784355163574\n",
      "Training-Batch No.36130\n",
      "Loss = 1.423370599746704\n",
      "Training-Batch No.36140\n",
      "Loss = 2.2555668354034424\n",
      "Training-Batch No.36150\n",
      "Loss = 2.5289058685302734\n",
      "Training-Batch No.36160\n",
      "Loss = 2.604912757873535\n",
      "Training-Batch No.36170\n",
      "Loss = 7.931883811950684\n",
      "Training-Batch No.36180\n",
      "Loss = 1.762468695640564\n",
      "Training-Batch No.36190\n",
      "Loss = 1.6104925870895386\n",
      "Training-Batch No.36200\n",
      "Loss = 2.3494088649749756\n",
      "Training-Batch No.36210\n",
      "Loss = 2.140090227127075\n",
      "Training-Batch No.36220\n",
      "Loss = 2.096963405609131\n",
      "Training-Batch No.36230\n",
      "Loss = 8.125648498535156\n",
      "Training-Batch No.36240\n",
      "Loss = 1.385392427444458\n",
      "Training-Batch No.36250\n",
      "Loss = 4.618995189666748\n",
      "Training-Batch No.36260\n",
      "Loss = 1.5019195079803467\n",
      "Training-Batch No.36270\n",
      "Loss = 2.2861225605010986\n",
      "Training-Batch No.36280\n",
      "Loss = 1.3844773769378662\n",
      "Training-Batch No.36290\n",
      "Loss = 1.292795181274414\n",
      "Training-Batch No.36300\n",
      "Loss = 2.261009454727173\n",
      "Training-Batch No.36310\n",
      "Loss = 2.155764579772949\n",
      "Training-Batch No.36320\n",
      "Loss = 1.2555996179580688\n",
      "Training-Batch No.36330\n",
      "Loss = 2.343723773956299\n",
      "Training-Batch No.36340\n",
      "Loss = 2.0543317794799805\n",
      "Training-Batch No.36350\n",
      "Loss = 1.272780179977417\n",
      "Training-Batch No.36360\n",
      "Loss = 4.24237060546875\n",
      "Training-Batch No.36370\n",
      "Loss = 1.6819016933441162\n",
      "Training-Batch No.36380\n",
      "Loss = 1.9696619510650635\n",
      "Training-Batch No.36390\n",
      "Loss = 1.6376500129699707\n",
      "Training-Batch No.36400\n",
      "Loss = 2.246762275695801\n",
      "Training-Batch No.36410\n",
      "Loss = 1.7338552474975586\n",
      "Training-Batch No.36420\n",
      "Loss = 3.749182939529419\n",
      "Training-Batch No.36430\n",
      "Loss = 1.78434419631958\n",
      "Training-Batch No.36440\n",
      "Loss = 1.7587535381317139\n",
      "Training-Batch No.36450\n",
      "Loss = 1.8126258850097656\n",
      "Training-Batch No.36460\n",
      "Loss = 1.7453508377075195\n",
      "Training-Batch No.36470\n",
      "Loss = 1.5988054275512695\n",
      "Training-Batch No.36480\n",
      "Loss = 2.415794849395752\n",
      "Training-Batch No.36490\n",
      "Loss = 3.99161434173584\n",
      "Training-Batch No.36500\n",
      "Loss = 1.5895168781280518\n",
      "Training-Batch No.36510\n",
      "Loss = 1.825945496559143\n",
      "Training-Batch No.36520\n",
      "Loss = 1.5555026531219482\n",
      "Training-Batch No.36530\n",
      "Loss = 1.209685206413269\n",
      "Training-Batch No.36540\n",
      "Loss = 1.6523261070251465\n",
      "Training-Batch No.36550\n",
      "Loss = 3.6264076232910156\n",
      "Training-Batch No.36560\n",
      "Loss = 1.6969525814056396\n",
      "Training-Batch No.36570\n",
      "Loss = 2.045138359069824\n",
      "Training-Batch No.36580\n",
      "Loss = 2.3907275199890137\n",
      "Training-Batch No.36590\n",
      "Loss = 1.4446779489517212\n",
      "Training-Batch No.36600\n",
      "Loss = 1.2171885967254639\n",
      "Training-Batch No.36610\n",
      "Loss = 1.9756064414978027\n",
      "Training-Batch No.36620\n",
      "Loss = 4.848779201507568\n",
      "Training-Batch No.36630\n",
      "Loss = 1.8782739639282227\n",
      "Training-Batch No.36640\n",
      "Loss = 1.5392687320709229\n",
      "Training-Batch No.36650\n",
      "Loss = 1.6566572189331055\n",
      "Training-Batch No.36660\n",
      "Loss = 2.2027106285095215\n",
      "Training-Batch No.36670\n",
      "Loss = 1.8749045133590698\n",
      "Training-Batch No.36680\n",
      "Loss = 1.9689030647277832\n",
      "Training-Batch No.36690\n",
      "Loss = 2.572707176208496\n",
      "Training-Batch No.36700\n",
      "Loss = 2.0325024127960205\n",
      "Training-Batch No.36710\n",
      "Loss = 1.1452391147613525\n",
      "Training-Batch No.36720\n",
      "Loss = 1.7082475423812866\n",
      "Training-Batch No.36730\n",
      "Loss = 2.2974441051483154\n",
      "Training-Batch No.36740\n",
      "Loss = 1.6709339618682861\n",
      "Training-Batch No.36750\n",
      "Loss = 1.606495976448059\n",
      "Training-Batch No.36760\n",
      "Loss = 1.0499370098114014\n",
      "Training-Batch No.36770\n",
      "Loss = 1.6992602348327637\n",
      "Training-Batch No.36780\n",
      "Loss = 1.7094208002090454\n",
      "Training-Batch No.36790\n",
      "Loss = 2.295405149459839\n",
      "Training-Batch No.36800\n",
      "Loss = 1.7590758800506592\n",
      "Training-Batch No.36810\n",
      "Loss = 1.1195876598358154\n",
      "Training-Batch No.36820\n",
      "Loss = 1.7604557275772095\n",
      "Epoch 35 Training Loss: 2.0975\n",
      "Start validation\n",
      "Epoch 35 Validation Loss: 2.4690\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.29006181645268664\n",
      "Average Top-3 accuracy 0.6443176414645744\n",
      "Average Top-5 accuracy 0.7907750832144556\n",
      "Average Top-7 accuracy 0.8744650499286734\n",
      "Average Top-9 accuracy 0.9144079885877318\n",
      "Average Top-11 accuracy 0.9324774132192106\n",
      "Average Top-13 accuracy 0.9500713266761769\n",
      "Average Top-15 accuracy 0.9576795054683785\n",
      "current acc 0.29006181645268664\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 36\n",
      "Training-Batch No.36830\n",
      "Loss = 8.107147216796875\n",
      "Training-Batch No.36840\n",
      "Loss = 2.3678414821624756\n",
      "Training-Batch No.36850\n",
      "Loss = 2.1709160804748535\n",
      "Training-Batch No.36860\n",
      "Loss = 1.3834152221679688\n",
      "Training-Batch No.36870\n",
      "Loss = 1.4950275421142578\n",
      "Training-Batch No.36880\n",
      "Loss = 1.8642070293426514\n",
      "Training-Batch No.36890\n",
      "Loss = 2.277346134185791\n",
      "Training-Batch No.36900\n",
      "Loss = 2.1018574237823486\n",
      "Training-Batch No.36910\n",
      "Loss = 1.970637321472168\n",
      "Training-Batch No.36920\n",
      "Loss = 1.94700288772583\n",
      "Training-Batch No.36930\n",
      "Loss = 1.8351892232894897\n",
      "Training-Batch No.36940\n",
      "Loss = 3.2408924102783203\n",
      "Training-Batch No.36950\n",
      "Loss = 1.5573711395263672\n",
      "Training-Batch No.36960\n",
      "Loss = 1.9725099802017212\n",
      "Training-Batch No.36970\n",
      "Loss = 1.7527998685836792\n",
      "Training-Batch No.36980\n",
      "Loss = 1.9389187097549438\n",
      "Training-Batch No.36990\n",
      "Loss = 1.5948703289031982\n",
      "Training-Batch No.37000\n",
      "Loss = 2.4051616191864014\n",
      "Training-Batch No.37010\n",
      "Loss = 1.4979511499404907\n",
      "Training-Batch No.37020\n",
      "Loss = 3.1055572032928467\n",
      "Training-Batch No.37030\n",
      "Loss = 1.2262896299362183\n",
      "Training-Batch No.37040\n",
      "Loss = 2.074511766433716\n",
      "Training-Batch No.37050\n",
      "Loss = 1.8987358808517456\n",
      "Training-Batch No.37060\n",
      "Loss = 2.0725414752960205\n",
      "Training-Batch No.37070\n",
      "Loss = 1.5020549297332764\n",
      "Training-Batch No.37080\n",
      "Loss = 1.7596874237060547\n",
      "Training-Batch No.37090\n",
      "Loss = 2.3476080894470215\n",
      "Training-Batch No.37100\n",
      "Loss = 3.0827879905700684\n",
      "Training-Batch No.37110\n",
      "Loss = 2.182936668395996\n",
      "Training-Batch No.37120\n",
      "Loss = 1.5294336080551147\n",
      "Training-Batch No.37130\n",
      "Loss = 2.4831061363220215\n",
      "Training-Batch No.37140\n",
      "Loss = 2.053192615509033\n",
      "Training-Batch No.37150\n",
      "Loss = 2.3836312294006348\n",
      "Training-Batch No.37160\n",
      "Loss = 2.6724693775177\n",
      "Training-Batch No.37170\n",
      "Loss = 1.6004201173782349\n",
      "Training-Batch No.37180\n",
      "Loss = 1.8407716751098633\n",
      "Training-Batch No.37190\n",
      "Loss = 2.2437314987182617\n",
      "Training-Batch No.37200\n",
      "Loss = 2.151127815246582\n",
      "Training-Batch No.37210\n",
      "Loss = 1.0451765060424805\n",
      "Training-Batch No.37220\n",
      "Loss = 3.544355869293213\n",
      "Training-Batch No.37230\n",
      "Loss = 1.7879395484924316\n",
      "Training-Batch No.37240\n",
      "Loss = 1.7922993898391724\n",
      "Training-Batch No.37250\n",
      "Loss = 1.8821710348129272\n",
      "Training-Batch No.37260\n",
      "Loss = 3.3008384704589844\n",
      "Training-Batch No.37270\n",
      "Loss = 1.4513156414031982\n",
      "Training-Batch No.37280\n",
      "Loss = 1.1989868879318237\n",
      "Training-Batch No.37290\n",
      "Loss = 2.3981006145477295\n",
      "Training-Batch No.37300\n",
      "Loss = 2.148251533508301\n",
      "Training-Batch No.37310\n",
      "Loss = 2.2931628227233887\n",
      "Training-Batch No.37320\n",
      "Loss = 1.2626893520355225\n",
      "Training-Batch No.37330\n",
      "Loss = 1.3405139446258545\n",
      "Training-Batch No.37340\n",
      "Loss = 2.508563280105591\n",
      "Training-Batch No.37350\n",
      "Loss = 1.8603923320770264\n",
      "Training-Batch No.37360\n",
      "Loss = 1.685738205909729\n",
      "Training-Batch No.37370\n",
      "Loss = 2.166182518005371\n",
      "Training-Batch No.37380\n",
      "Loss = 1.8821722269058228\n",
      "Training-Batch No.37390\n",
      "Loss = 2.4935054779052734\n",
      "Training-Batch No.37400\n",
      "Loss = 1.7759990692138672\n",
      "Training-Batch No.37410\n",
      "Loss = 2.134901523590088\n",
      "Training-Batch No.37420\n",
      "Loss = 2.2490835189819336\n",
      "Training-Batch No.37430\n",
      "Loss = 3.074697256088257\n",
      "Training-Batch No.37440\n",
      "Loss = 1.686033010482788\n",
      "Training-Batch No.37450\n",
      "Loss = 1.7495763301849365\n",
      "Training-Batch No.37460\n",
      "Loss = 1.7548828125\n",
      "Training-Batch No.37470\n",
      "Loss = 1.821305513381958\n",
      "Training-Batch No.37480\n",
      "Loss = 1.3630672693252563\n",
      "Training-Batch No.37490\n",
      "Loss = 2.159518241882324\n",
      "Training-Batch No.37500\n",
      "Loss = 1.6141303777694702\n",
      "Training-Batch No.37510\n",
      "Loss = 1.2433487176895142\n",
      "Training-Batch No.37520\n",
      "Loss = 1.40134596824646\n",
      "Training-Batch No.37530\n",
      "Loss = 1.8928302526474\n",
      "Training-Batch No.37540\n",
      "Loss = 8.369805335998535\n",
      "Training-Batch No.37550\n",
      "Loss = 1.9276468753814697\n",
      "Training-Batch No.37560\n",
      "Loss = 2.2004711627960205\n",
      "Training-Batch No.37570\n",
      "Loss = 2.4559154510498047\n",
      "Training-Batch No.37580\n",
      "Loss = 3.737724781036377\n",
      "Training-Batch No.37590\n",
      "Loss = 3.6830406188964844\n",
      "Training-Batch No.37600\n",
      "Loss = 2.1803531646728516\n",
      "Training-Batch No.37610\n",
      "Loss = 1.3936270475387573\n",
      "Training-Batch No.37620\n",
      "Loss = 2.296765089035034\n",
      "Training-Batch No.37630\n",
      "Loss = 1.528764009475708\n",
      "Training-Batch No.37640\n",
      "Loss = 1.4427450895309448\n",
      "Training-Batch No.37650\n",
      "Loss = 8.781818389892578\n",
      "Training-Batch No.37660\n",
      "Loss = 2.1810309886932373\n",
      "Training-Batch No.37670\n",
      "Loss = 1.9603644609451294\n",
      "Training-Batch No.37680\n",
      "Loss = 3.306550979614258\n",
      "Training-Batch No.37690\n",
      "Loss = 1.5552200078964233\n",
      "Training-Batch No.37700\n",
      "Loss = 1.3734900951385498\n",
      "Training-Batch No.37710\n",
      "Loss = 2.580601930618286\n",
      "Training-Batch No.37720\n",
      "Loss = 1.3199795484542847\n",
      "Training-Batch No.37730\n",
      "Loss = 1.7156914472579956\n",
      "Training-Batch No.37740\n",
      "Loss = 2.246367931365967\n",
      "Training-Batch No.37750\n",
      "Loss = 1.2597981691360474\n",
      "Training-Batch No.37760\n",
      "Loss = 1.2237834930419922\n",
      "Training-Batch No.37770\n",
      "Loss = 1.4752066135406494\n",
      "Training-Batch No.37780\n",
      "Loss = 1.9661827087402344\n",
      "Training-Batch No.37790\n",
      "Loss = 1.559317708015442\n",
      "Training-Batch No.37800\n",
      "Loss = 1.551159381866455\n",
      "Training-Batch No.37810\n",
      "Loss = 1.16970694065094\n",
      "Training-Batch No.37820\n",
      "Loss = 0.9780493974685669\n",
      "Training-Batch No.37830\n",
      "Loss = 1.3807734251022339\n",
      "Training-Batch No.37840\n",
      "Loss = 1.9984025955200195\n",
      "Training-Batch No.37850\n",
      "Loss = 1.4421789646148682\n",
      "Training-Batch No.37860\n",
      "Loss = 1.9807093143463135\n",
      "Training-Batch No.37870\n",
      "Loss = 2.255119562149048\n",
      "Epoch 36 Training Loss: 2.0959\n",
      "Start validation\n",
      "Epoch 36 Validation Loss: 2.5322\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.2249167855444603\n",
      "Average Top-3 accuracy 0.5920114122681883\n",
      "Average Top-5 accuracy 0.7779362815026153\n",
      "Average Top-7 accuracy 0.8730385164051355\n",
      "Average Top-9 accuracy 0.9139324774132193\n",
      "Average Top-11 accuracy 0.9324774132192106\n",
      "Average Top-13 accuracy 0.9415121255349501\n",
      "Average Top-15 accuracy 0.9529243937232525\n",
      "current acc 0.2249167855444603\n",
      "best acc 0.3518782691393248\n",
      "updated best accuracy 0.3518782691393248\n",
      "Epoch No. 37\n",
      "Training-Batch No.37880\n",
      "Loss = 2.383894920349121\n",
      "Training-Batch No.37890\n",
      "Loss = 1.5607523918151855\n",
      "Training-Batch No.37900\n",
      "Loss = 1.507643222808838\n",
      "Training-Batch No.37910\n",
      "Loss = 1.4577255249023438\n",
      "Training-Batch No.37920\n",
      "Loss = 1.9152772426605225\n",
      "Training-Batch No.37930\n",
      "Loss = 2.1107938289642334\n",
      "Training-Batch No.37940\n",
      "Loss = 1.7626049518585205\n",
      "Training-Batch No.37950\n",
      "Loss = 3.1784327030181885\n",
      "Training-Batch No.37960\n",
      "Loss = 1.685517430305481\n",
      "Training-Batch No.37970\n",
      "Loss = 1.3769234418869019\n",
      "Training-Batch No.37980\n",
      "Loss = 5.029464244842529\n",
      "Training-Batch No.37990\n",
      "Loss = 1.4689669609069824\n",
      "Training-Batch No.38000\n",
      "Loss = 1.4991285800933838\n",
      "Training-Batch No.38010\n",
      "Loss = 1.8270494937896729\n",
      "Training-Batch No.38020\n",
      "Loss = 1.5245890617370605\n",
      "Training-Batch No.38030\n",
      "Loss = 2.2273073196411133\n",
      "Training-Batch No.38040\n",
      "Loss = 1.7412536144256592\n",
      "Training-Batch No.38050\n",
      "Loss = 2.684391498565674\n",
      "Training-Batch No.38060\n",
      "Loss = 3.207939863204956\n",
      "Training-Batch No.38070\n",
      "Loss = 2.109367609024048\n",
      "Training-Batch No.38080\n",
      "Loss = 2.0125770568847656\n",
      "Training-Batch No.38090\n",
      "Loss = 1.6962714195251465\n",
      "Training-Batch No.38100\n",
      "Loss = 1.5223451852798462\n",
      "Training-Batch No.38110\n",
      "Loss = 2.017744779586792\n",
      "Training-Batch No.38120\n",
      "Loss = 4.780644416809082\n",
      "Training-Batch No.38130\n",
      "Loss = 1.745604395866394\n",
      "Training-Batch No.38140\n",
      "Loss = 1.5544168949127197\n",
      "Training-Batch No.38150\n",
      "Loss = 1.521621823310852\n",
      "Training-Batch No.38160\n",
      "Loss = 1.425235629081726\n",
      "Training-Batch No.38170\n",
      "Loss = 1.6894832849502563\n",
      "Training-Batch No.38180\n",
      "Loss = 1.9027937650680542\n",
      "Training-Batch No.38190\n",
      "Loss = 1.7310638427734375\n",
      "Training-Batch No.38200\n",
      "Loss = 1.6983692646026611\n",
      "Training-Batch No.38210\n",
      "Loss = 1.4105103015899658\n",
      "Training-Batch No.38220\n",
      "Loss = 1.9550293684005737\n",
      "Training-Batch No.38230\n",
      "Loss = 1.4951326847076416\n",
      "Training-Batch No.38240\n",
      "Loss = 1.4747248888015747\n",
      "Training-Batch No.38250\n",
      "Loss = 2.7919294834136963\n",
      "Training-Batch No.38260\n",
      "Loss = 1.9695897102355957\n",
      "Training-Batch No.38270\n",
      "Loss = 1.731245756149292\n",
      "Training-Batch No.38280\n",
      "Loss = 2.2138078212738037\n",
      "Training-Batch No.38290\n",
      "Loss = 1.7865372896194458\n",
      "Training-Batch No.38300\n",
      "Loss = 1.7958630323410034\n",
      "Training-Batch No.38310\n",
      "Loss = 1.936873435974121\n",
      "Training-Batch No.38320\n",
      "Loss = 3.4298110008239746\n",
      "Training-Batch No.38330\n",
      "Loss = 1.517512559890747\n",
      "Training-Batch No.38340\n",
      "Loss = 2.2756242752075195\n",
      "Training-Batch No.38350\n",
      "Loss = 1.3344061374664307\n",
      "Training-Batch No.38360\n",
      "Loss = 1.8906028270721436\n",
      "Training-Batch No.38370\n",
      "Loss = 1.1027368307113647\n",
      "Training-Batch No.38380\n",
      "Loss = 1.0968897342681885\n",
      "Training-Batch No.38390\n",
      "Loss = 1.5319241285324097\n",
      "Training-Batch No.38400\n",
      "Loss = 1.4866557121276855\n",
      "Training-Batch No.38410\n",
      "Loss = 1.5327675342559814\n",
      "Training-Batch No.38420\n",
      "Loss = 1.8873435258865356\n",
      "Training-Batch No.38430\n",
      "Loss = 3.0383219718933105\n",
      "Training-Batch No.38440\n",
      "Loss = 2.6641173362731934\n",
      "Training-Batch No.38450\n",
      "Loss = 1.525285005569458\n",
      "Training-Batch No.38460\n",
      "Loss = 2.1318020820617676\n",
      "Training-Batch No.38470\n",
      "Loss = 1.9556690454483032\n",
      "Training-Batch No.38480\n",
      "Loss = 1.9740768671035767\n",
      "Training-Batch No.38490\n",
      "Loss = 1.6132051944732666\n",
      "Training-Batch No.38500\n",
      "Loss = 1.463091254234314\n",
      "Training-Batch No.38510\n",
      "Loss = 2.638676404953003\n",
      "Training-Batch No.38520\n",
      "Loss = 2.5397863388061523\n",
      "Training-Batch No.38530\n",
      "Loss = 1.4223510026931763\n",
      "Training-Batch No.38540\n",
      "Loss = 1.8675320148468018\n",
      "Training-Batch No.38550\n",
      "Loss = 1.8715308904647827\n",
      "Training-Batch No.38560\n",
      "Loss = 2.3313684463500977\n",
      "Training-Batch No.38570\n",
      "Loss = 1.4093232154846191\n",
      "Training-Batch No.38580\n",
      "Loss = 1.9434973001480103\n",
      "Training-Batch No.38590\n",
      "Loss = 1.2845486402511597\n",
      "Training-Batch No.38600\n",
      "Loss = 2.5187230110168457\n",
      "Training-Batch No.38610\n",
      "Loss = 2.0381155014038086\n",
      "Training-Batch No.38620\n",
      "Loss = 2.078676700592041\n",
      "Training-Batch No.38630\n",
      "Loss = 1.9931769371032715\n",
      "Training-Batch No.38640\n",
      "Loss = 0.9597287178039551\n",
      "Training-Batch No.38650\n",
      "Loss = 2.142564296722412\n",
      "Training-Batch No.38660\n",
      "Loss = 2.4420337677001953\n",
      "Training-Batch No.38670\n",
      "Loss = 2.133694648742676\n",
      "Training-Batch No.38680\n",
      "Loss = 1.7337779998779297\n",
      "Training-Batch No.38690\n",
      "Loss = 1.5780977010726929\n",
      "Training-Batch No.38700\n",
      "Loss = 2.06754469871521\n",
      "Training-Batch No.38710\n",
      "Loss = 1.3846291303634644\n",
      "Training-Batch No.38720\n",
      "Loss = 2.5288636684417725\n",
      "Training-Batch No.38730\n",
      "Loss = 2.821183204650879\n",
      "Training-Batch No.38740\n",
      "Loss = 1.2630882263183594\n",
      "Training-Batch No.38750\n",
      "Loss = 1.938713788986206\n",
      "Training-Batch No.38760\n",
      "Loss = 1.297284483909607\n",
      "Training-Batch No.38770\n",
      "Loss = 1.76566481590271\n",
      "Training-Batch No.38780\n",
      "Loss = 1.6908462047576904\n",
      "Training-Batch No.38790\n",
      "Loss = 1.9811527729034424\n",
      "Training-Batch No.38800\n",
      "Loss = 1.626423954963684\n",
      "Training-Batch No.38810\n",
      "Loss = 2.868756055831909\n",
      "Training-Batch No.38820\n",
      "Loss = 1.7029727697372437\n",
      "Training-Batch No.38830\n",
      "Loss = 1.593480110168457\n",
      "Training-Batch No.38840\n",
      "Loss = 1.6720943450927734\n",
      "Training-Batch No.38850\n",
      "Loss = 1.9054315090179443\n",
      "Training-Batch No.38860\n",
      "Loss = 1.775970458984375\n",
      "Training-Batch No.38870\n",
      "Loss = 1.822576880455017\n",
      "Training-Batch No.38880\n",
      "Loss = 1.4224753379821777\n",
      "Training-Batch No.38890\n",
      "Loss = 3.158470630645752\n",
      "Training-Batch No.38900\n",
      "Loss = 1.6505358219146729\n",
      "Training-Batch No.38910\n",
      "Loss = 1.60398268699646\n",
      "Training-Batch No.38920\n",
      "Loss = 1.7932053804397583\n",
      "Epoch 37 Training Loss: 2.0756\n",
      "Start validation\n",
      "Epoch 37 Validation Loss: 2.2662\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.357584403233476\n",
      "Average Top-3 accuracy 0.7146932952924394\n",
      "Average Top-5 accuracy 0.8506894912030433\n",
      "Average Top-7 accuracy 0.9220161673799334\n",
      "Average Top-9 accuracy 0.9405611031859249\n",
      "Average Top-11 accuracy 0.9495958155016643\n",
      "Average Top-13 accuracy 0.9605325725154541\n",
      "Average Top-15 accuracy 0.9638611507370424\n",
      "current acc 0.357584403233476\n",
      "best acc 0.3518782691393248\n",
      "Saving the best model\n",
      "updated best accuracy 0.357584403233476\n",
      "Epoch No. 38\n",
      "Training-Batch No.38930\n",
      "Loss = 2.014967441558838\n",
      "Training-Batch No.38940\n",
      "Loss = 1.7758257389068604\n",
      "Training-Batch No.38950\n",
      "Loss = 1.3595900535583496\n",
      "Training-Batch No.38960\n",
      "Loss = 1.9290359020233154\n",
      "Training-Batch No.38970\n",
      "Loss = 1.277540683746338\n",
      "Training-Batch No.38980\n",
      "Loss = 1.978411316871643\n",
      "Training-Batch No.38990\n",
      "Loss = 6.610111236572266\n",
      "Training-Batch No.39000\n",
      "Loss = 1.332039475440979\n",
      "Training-Batch No.39010\n",
      "Loss = 1.2974931001663208\n",
      "Training-Batch No.39020\n",
      "Loss = 3.0929007530212402\n",
      "Training-Batch No.39030\n",
      "Loss = 1.718590497970581\n",
      "Training-Batch No.39040\n",
      "Loss = 1.2505065202713013\n",
      "Training-Batch No.39050\n",
      "Loss = 3.3212382793426514\n",
      "Training-Batch No.39060\n",
      "Loss = 1.9605188369750977\n",
      "Training-Batch No.39070\n",
      "Loss = 1.881589651107788\n",
      "Training-Batch No.39080\n",
      "Loss = 1.4723984003067017\n",
      "Training-Batch No.39090\n",
      "Loss = 1.3953869342803955\n",
      "Training-Batch No.39100\n",
      "Loss = 1.885560154914856\n",
      "Training-Batch No.39110\n",
      "Loss = 4.47137451171875\n",
      "Training-Batch No.39120\n",
      "Loss = 1.8371422290802002\n",
      "Training-Batch No.39130\n",
      "Loss = 2.6589910984039307\n",
      "Training-Batch No.39140\n",
      "Loss = 1.7837059497833252\n",
      "Training-Batch No.39150\n",
      "Loss = 1.0952236652374268\n",
      "Training-Batch No.39160\n",
      "Loss = 1.6464779376983643\n",
      "Training-Batch No.39170\n",
      "Loss = 1.4034523963928223\n",
      "Training-Batch No.39180\n",
      "Loss = 1.8752387762069702\n",
      "Training-Batch No.39190\n",
      "Loss = 1.435544729232788\n",
      "Training-Batch No.39200\n",
      "Loss = 1.4582575559616089\n",
      "Training-Batch No.39210\n",
      "Loss = 1.5088698863983154\n",
      "Training-Batch No.39220\n",
      "Loss = 3.686833620071411\n",
      "Training-Batch No.39230\n",
      "Loss = 1.806272029876709\n",
      "Training-Batch No.39240\n",
      "Loss = 1.8190654516220093\n",
      "Training-Batch No.39250\n",
      "Loss = 1.6129313707351685\n",
      "Training-Batch No.39260\n",
      "Loss = 2.2093067169189453\n",
      "Training-Batch No.39270\n",
      "Loss = 2.6456661224365234\n",
      "Training-Batch No.39280\n",
      "Loss = 1.4536844491958618\n",
      "Training-Batch No.39290\n",
      "Loss = 1.6146137714385986\n",
      "Training-Batch No.39300\n",
      "Loss = 2.348902702331543\n",
      "Training-Batch No.39310\n",
      "Loss = 2.1989502906799316\n",
      "Training-Batch No.39320\n",
      "Loss = 4.065487861633301\n",
      "Training-Batch No.39330\n",
      "Loss = 2.2911252975463867\n",
      "Training-Batch No.39340\n",
      "Loss = 1.7145010232925415\n",
      "Training-Batch No.39350\n",
      "Loss = 1.693260908126831\n",
      "Training-Batch No.39360\n",
      "Loss = 1.6429210901260376\n",
      "Training-Batch No.39370\n",
      "Loss = 1.388793706893921\n",
      "Training-Batch No.39380\n",
      "Loss = 2.071756601333618\n",
      "Training-Batch No.39390\n",
      "Loss = 1.5923805236816406\n",
      "Training-Batch No.39400\n",
      "Loss = 1.7575035095214844\n",
      "Training-Batch No.39410\n",
      "Loss = 1.2047606706619263\n",
      "Training-Batch No.39420\n",
      "Loss = 1.7495096921920776\n",
      "Training-Batch No.39430\n",
      "Loss = 1.8001103401184082\n",
      "Training-Batch No.39440\n",
      "Loss = 1.9882549047470093\n",
      "Training-Batch No.39450\n",
      "Loss = 1.3444223403930664\n",
      "Training-Batch No.39460\n",
      "Loss = 2.0521278381347656\n",
      "Training-Batch No.39470\n",
      "Loss = 1.1393258571624756\n",
      "Training-Batch No.39480\n",
      "Loss = 1.5702029466629028\n",
      "Training-Batch No.39490\n",
      "Loss = 1.574704885482788\n",
      "Training-Batch No.39500\n",
      "Loss = 1.4999067783355713\n",
      "Training-Batch No.39510\n",
      "Loss = 1.3039343357086182\n",
      "Training-Batch No.39520\n",
      "Loss = 1.470952033996582\n",
      "Training-Batch No.39530\n",
      "Loss = 1.756354570388794\n",
      "Training-Batch No.39540\n",
      "Loss = 1.9861582517623901\n",
      "Training-Batch No.39550\n",
      "Loss = 1.7795310020446777\n",
      "Training-Batch No.39560\n",
      "Loss = 1.9476286172866821\n",
      "Training-Batch No.39570\n",
      "Loss = 1.2540507316589355\n",
      "Training-Batch No.39580\n",
      "Loss = 1.3550604581832886\n",
      "Training-Batch No.39590\n",
      "Loss = 2.5077905654907227\n",
      "Training-Batch No.39600\n",
      "Loss = 1.534593939781189\n",
      "Training-Batch No.39610\n",
      "Loss = 1.5005894899368286\n",
      "Training-Batch No.39620\n",
      "Loss = 1.9876371622085571\n",
      "Training-Batch No.39630\n",
      "Loss = 2.9580094814300537\n",
      "Training-Batch No.39640\n",
      "Loss = 2.8586926460266113\n",
      "Training-Batch No.39650\n",
      "Loss = 1.999234676361084\n",
      "Training-Batch No.39660\n",
      "Loss = 1.2766666412353516\n",
      "Training-Batch No.39670\n",
      "Loss = 2.138617515563965\n",
      "Training-Batch No.39680\n",
      "Loss = 2.0834598541259766\n",
      "Training-Batch No.39690\n",
      "Loss = 2.0077223777770996\n",
      "Training-Batch No.39700\n",
      "Loss = 3.2782363891601562\n",
      "Training-Batch No.39710\n",
      "Loss = 2.20737886428833\n",
      "Training-Batch No.39720\n",
      "Loss = 1.5015614032745361\n",
      "Training-Batch No.39730\n",
      "Loss = 1.52969229221344\n",
      "Training-Batch No.39740\n",
      "Loss = 2.8826584815979004\n",
      "Training-Batch No.39750\n",
      "Loss = 1.856834888458252\n",
      "Training-Batch No.39760\n",
      "Loss = 1.8463172912597656\n",
      "Training-Batch No.39770\n",
      "Loss = 2.4081501960754395\n",
      "Training-Batch No.39780\n",
      "Loss = 2.1489973068237305\n",
      "Training-Batch No.39790\n",
      "Loss = 4.447443008422852\n",
      "Training-Batch No.39800\n",
      "Loss = 2.1784005165100098\n",
      "Training-Batch No.39810\n",
      "Loss = 1.486580491065979\n",
      "Training-Batch No.39820\n",
      "Loss = 3.266111373901367\n",
      "Training-Batch No.39830\n",
      "Loss = 1.5424824953079224\n",
      "Training-Batch No.39840\n",
      "Loss = 1.601195216178894\n",
      "Training-Batch No.39850\n",
      "Loss = 1.4530507326126099\n",
      "Training-Batch No.39860\n",
      "Loss = 1.9331282377243042\n",
      "Training-Batch No.39870\n",
      "Loss = 2.5788917541503906\n",
      "Training-Batch No.39880\n",
      "Loss = 1.2118409872055054\n",
      "Training-Batch No.39890\n",
      "Loss = 1.7825392484664917\n",
      "Training-Batch No.39900\n",
      "Loss = 1.7501904964447021\n",
      "Training-Batch No.39910\n",
      "Loss = 1.3860819339752197\n",
      "Training-Batch No.39920\n",
      "Loss = 1.4086737632751465\n",
      "Training-Batch No.39930\n",
      "Loss = 1.5884406566619873\n",
      "Training-Batch No.39940\n",
      "Loss = 2.3373959064483643\n",
      "Training-Batch No.39950\n",
      "Loss = 1.665295124053955\n",
      "Training-Batch No.39960\n",
      "Loss = 1.9743103981018066\n",
      "Training-Batch No.39970\n",
      "Loss = 2.377300500869751\n",
      "Epoch 38 Training Loss: 2.0952\n",
      "Start validation\n",
      "Epoch 38 Validation Loss: 2.2243\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.3780313837375178\n",
      "Average Top-3 accuracy 0.7275320970042796\n",
      "Average Top-5 accuracy 0.8601997146932953\n",
      "Average Top-7 accuracy 0.9286733238231099\n",
      "Average Top-9 accuracy 0.9434141702330004\n",
      "Average Top-11 accuracy 0.9500713266761769\n",
      "Average Top-13 accuracy 0.9600570613409415\n",
      "Average Top-15 accuracy 0.9643366619115549\n",
      "current acc 0.3780313837375178\n",
      "best acc 0.357584403233476\n",
      "Saving the best model\n",
      "updated best accuracy 0.3780313837375178\n",
      "Epoch No. 39\n",
      "Training-Batch No.39980\n",
      "Loss = 2.09248685836792\n",
      "Training-Batch No.39990\n",
      "Loss = 3.361482620239258\n",
      "Training-Batch No.40000\n",
      "Loss = 1.754045009613037\n",
      "Training-Batch No.40010\n",
      "Loss = 1.9734817743301392\n",
      "Training-Batch No.40020\n",
      "Loss = 2.349456310272217\n",
      "Training-Batch No.40030\n",
      "Loss = 2.1297249794006348\n",
      "Training-Batch No.40040\n",
      "Loss = 1.6426481008529663\n",
      "Training-Batch No.40050\n",
      "Loss = 2.640272378921509\n",
      "Training-Batch No.40060\n",
      "Loss = 2.4460208415985107\n",
      "Training-Batch No.40070\n",
      "Loss = 2.071049213409424\n",
      "Training-Batch No.40080\n",
      "Loss = 1.7228026390075684\n",
      "Training-Batch No.40090\n",
      "Loss = 1.5622090101242065\n",
      "Training-Batch No.40100\n",
      "Loss = 1.207696557044983\n",
      "Training-Batch No.40110\n",
      "Loss = 2.3076562881469727\n",
      "Training-Batch No.40120\n",
      "Loss = 1.4369118213653564\n",
      "Training-Batch No.40130\n",
      "Loss = 1.4388998746871948\n",
      "Training-Batch No.40140\n",
      "Loss = 3.498059034347534\n",
      "Training-Batch No.40150\n",
      "Loss = 2.258875608444214\n",
      "Training-Batch No.40160\n",
      "Loss = 2.5772945880889893\n",
      "Training-Batch No.40170\n",
      "Loss = 3.6274778842926025\n",
      "Training-Batch No.40180\n",
      "Loss = 2.5924477577209473\n",
      "Training-Batch No.40190\n",
      "Loss = 1.8997355699539185\n",
      "Training-Batch No.40200\n",
      "Loss = 2.0692946910858154\n",
      "Training-Batch No.40210\n",
      "Loss = 1.5914558172225952\n",
      "Training-Batch No.40220\n",
      "Loss = 1.5621799230575562\n",
      "Training-Batch No.40230\n",
      "Loss = 1.6081513166427612\n",
      "Training-Batch No.40240\n",
      "Loss = 1.1610016822814941\n",
      "Training-Batch No.40250\n",
      "Loss = 2.272331476211548\n",
      "Training-Batch No.40260\n",
      "Loss = 5.7887983322143555\n",
      "Training-Batch No.40270\n",
      "Loss = 3.753577947616577\n",
      "Training-Batch No.40280\n",
      "Loss = 2.087874412536621\n",
      "Training-Batch No.40290\n",
      "Loss = 2.0856518745422363\n",
      "Training-Batch No.40300\n",
      "Loss = 1.898972988128662\n",
      "Training-Batch No.40310\n",
      "Loss = 1.6154158115386963\n",
      "Training-Batch No.40320\n",
      "Loss = 2.1783201694488525\n",
      "Training-Batch No.40330\n",
      "Loss = 2.077747106552124\n",
      "Training-Batch No.40340\n",
      "Loss = 2.5004537105560303\n",
      "Training-Batch No.40350\n",
      "Loss = 1.7592558860778809\n",
      "Training-Batch No.40360\n",
      "Loss = 4.6647162437438965\n",
      "Training-Batch No.40370\n",
      "Loss = 1.6055688858032227\n",
      "Training-Batch No.40380\n",
      "Loss = 2.165191173553467\n",
      "Training-Batch No.40390\n",
      "Loss = 2.1515331268310547\n",
      "Training-Batch No.40400\n",
      "Loss = 2.0184812545776367\n",
      "Training-Batch No.40410\n",
      "Loss = 1.4295965433120728\n",
      "Training-Batch No.40420\n",
      "Loss = 0.8947038650512695\n",
      "Training-Batch No.40430\n",
      "Loss = 2.0467753410339355\n",
      "Training-Batch No.40440\n",
      "Loss = 1.9533735513687134\n",
      "Training-Batch No.40450\n",
      "Loss = 1.9117629528045654\n",
      "Training-Batch No.40460\n",
      "Loss = 1.4106647968292236\n",
      "Training-Batch No.40470\n",
      "Loss = 1.873254656791687\n",
      "Training-Batch No.40480\n",
      "Loss = 1.7870603799819946\n",
      "Training-Batch No.40490\n",
      "Loss = 1.9422928094863892\n",
      "Training-Batch No.40500\n",
      "Loss = 1.8767311573028564\n",
      "Training-Batch No.40510\n",
      "Loss = 1.787566065788269\n",
      "Training-Batch No.40520\n",
      "Loss = 1.3349881172180176\n",
      "Training-Batch No.40530\n",
      "Loss = 3.633141040802002\n",
      "Training-Batch No.40540\n",
      "Loss = 3.6646761894226074\n",
      "Training-Batch No.40550\n",
      "Loss = 3.8264503479003906\n",
      "Training-Batch No.40560\n",
      "Loss = 1.1496104001998901\n",
      "Training-Batch No.40570\n",
      "Loss = 2.1186022758483887\n",
      "Training-Batch No.40580\n",
      "Loss = 1.4730043411254883\n",
      "Training-Batch No.40590\n",
      "Loss = 1.6363396644592285\n",
      "Training-Batch No.40600\n",
      "Loss = 2.611133575439453\n",
      "Training-Batch No.40610\n",
      "Loss = 2.43385910987854\n",
      "Training-Batch No.40620\n",
      "Loss = 1.8815276622772217\n",
      "Training-Batch No.40630\n",
      "Loss = 2.128462553024292\n",
      "Training-Batch No.40640\n",
      "Loss = 2.057166576385498\n",
      "Training-Batch No.40650\n",
      "Loss = 1.9635703563690186\n",
      "Training-Batch No.40660\n",
      "Loss = 1.8012268543243408\n",
      "Training-Batch No.40670\n",
      "Loss = 2.742483377456665\n",
      "Training-Batch No.40680\n",
      "Loss = 1.683097004890442\n",
      "Training-Batch No.40690\n",
      "Loss = 1.5301830768585205\n",
      "Training-Batch No.40700\n",
      "Loss = 2.2043943405151367\n",
      "Training-Batch No.40710\n",
      "Loss = 3.614468812942505\n",
      "Training-Batch No.40720\n",
      "Loss = 2.96466064453125\n",
      "Training-Batch No.40730\n",
      "Loss = 1.500889778137207\n",
      "Training-Batch No.40740\n",
      "Loss = 1.5739669799804688\n",
      "Training-Batch No.40750\n",
      "Loss = 1.5868446826934814\n",
      "Training-Batch No.40760\n",
      "Loss = 1.3573263883590698\n",
      "Training-Batch No.40770\n",
      "Loss = 1.415071964263916\n",
      "Training-Batch No.40780\n",
      "Loss = 3.083578109741211\n",
      "Training-Batch No.40790\n",
      "Loss = 2.020871877670288\n",
      "Training-Batch No.40800\n",
      "Loss = 1.7667734622955322\n",
      "Training-Batch No.40810\n",
      "Loss = 1.4580755233764648\n",
      "Training-Batch No.40820\n",
      "Loss = 1.6164541244506836\n",
      "Training-Batch No.40830\n",
      "Loss = 1.16483473777771\n",
      "Training-Batch No.40840\n",
      "Loss = 1.9724266529083252\n",
      "Training-Batch No.40850\n",
      "Loss = 2.982060670852661\n",
      "Training-Batch No.40860\n",
      "Loss = 1.779369592666626\n",
      "Training-Batch No.40870\n",
      "Loss = 1.829585075378418\n",
      "Training-Batch No.40880\n",
      "Loss = 2.4589240550994873\n",
      "Training-Batch No.40890\n",
      "Loss = 2.134451389312744\n",
      "Training-Batch No.40900\n",
      "Loss = 4.677404403686523\n",
      "Training-Batch No.40910\n",
      "Loss = 1.6909503936767578\n",
      "Training-Batch No.40920\n",
      "Loss = 2.5109875202178955\n",
      "Training-Batch No.40930\n",
      "Loss = 1.1544328927993774\n",
      "Training-Batch No.40940\n",
      "Loss = 2.1013729572296143\n",
      "Training-Batch No.40950\n",
      "Loss = 1.1433141231536865\n",
      "Training-Batch No.40960\n",
      "Loss = 1.9748735427856445\n",
      "Training-Batch No.40970\n",
      "Loss = 2.001002073287964\n",
      "Training-Batch No.40980\n",
      "Loss = 2.521879196166992\n",
      "Training-Batch No.40990\n",
      "Loss = 1.8386428356170654\n",
      "Training-Batch No.41000\n",
      "Loss = 2.374009370803833\n",
      "Training-Batch No.41010\n",
      "Loss = 1.6725236177444458\n",
      "Training-Batch No.41020\n",
      "Loss = 1.4884600639343262\n",
      "Epoch 39 Training Loss: 2.0565\n",
      "Start validation\n",
      "Epoch 39 Validation Loss: 2.2252\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.3770803613884926\n",
      "Average Top-3 accuracy 0.7242035187826914\n",
      "Average Top-5 accuracy 0.8621017593913457\n",
      "Average Top-7 accuracy 0.9315263908701854\n",
      "Average Top-9 accuracy 0.9462672372800761\n",
      "Average Top-11 accuracy 0.9510223490252021\n",
      "Average Top-13 accuracy 0.9605325725154541\n",
      "Average Top-15 accuracy 0.9643366619115549\n",
      "current acc 0.3770803613884926\n",
      "best acc 0.3780313837375178\n",
      "updated best accuracy 0.3780313837375178\n",
      "Epoch No. 40\n",
      "Training-Batch No.41030\n",
      "Loss = 1.5447953939437866\n",
      "Training-Batch No.41040\n",
      "Loss = 2.5752997398376465\n",
      "Training-Batch No.41050\n",
      "Loss = 1.4351814985275269\n",
      "Training-Batch No.41060\n",
      "Loss = 3.476377010345459\n",
      "Training-Batch No.41070\n",
      "Loss = 3.1061272621154785\n",
      "Training-Batch No.41080\n",
      "Loss = 1.4833455085754395\n",
      "Training-Batch No.41090\n",
      "Loss = 1.424931287765503\n",
      "Training-Batch No.41100\n",
      "Loss = 1.2181236743927002\n",
      "Training-Batch No.41110\n",
      "Loss = 2.034395694732666\n",
      "Training-Batch No.41120\n",
      "Loss = 2.0595381259918213\n",
      "Training-Batch No.41130\n",
      "Loss = 2.1253740787506104\n",
      "Training-Batch No.41140\n",
      "Loss = 1.6383413076400757\n",
      "Training-Batch No.41150\n",
      "Loss = 1.8840293884277344\n",
      "Training-Batch No.41160\n",
      "Loss = 1.9293113946914673\n",
      "Training-Batch No.41170\n",
      "Loss = 2.4269461631774902\n",
      "Training-Batch No.41180\n",
      "Loss = 1.8925974369049072\n",
      "Training-Batch No.41190\n",
      "Loss = 3.070834159851074\n",
      "Training-Batch No.41200\n",
      "Loss = 2.856598377227783\n",
      "Training-Batch No.41210\n",
      "Loss = 1.2319986820220947\n",
      "Training-Batch No.41220\n",
      "Loss = 2.2776310443878174\n",
      "Training-Batch No.41230\n",
      "Loss = 2.4750399589538574\n",
      "Training-Batch No.41240\n",
      "Loss = 3.123854875564575\n",
      "Training-Batch No.41250\n",
      "Loss = 2.43032169342041\n",
      "Training-Batch No.41260\n",
      "Loss = 1.3538436889648438\n",
      "Training-Batch No.41270\n",
      "Loss = 1.7331037521362305\n",
      "Training-Batch No.41280\n",
      "Loss = 1.8918874263763428\n",
      "Training-Batch No.41290\n",
      "Loss = 6.550741672515869\n",
      "Training-Batch No.41300\n",
      "Loss = 3.5991225242614746\n",
      "Training-Batch No.41310\n",
      "Loss = 1.8350735902786255\n",
      "Training-Batch No.41320\n",
      "Loss = 1.7044187784194946\n",
      "Training-Batch No.41330\n",
      "Loss = 1.903078317642212\n",
      "Training-Batch No.41340\n",
      "Loss = 1.7074580192565918\n",
      "Training-Batch No.41350\n",
      "Loss = 1.4426512718200684\n",
      "Training-Batch No.41360\n",
      "Loss = 1.4575684070587158\n",
      "Training-Batch No.41370\n",
      "Loss = 2.140868663787842\n",
      "Training-Batch No.41380\n",
      "Loss = 2.7820396423339844\n",
      "Training-Batch No.41390\n",
      "Loss = 1.3817027807235718\n",
      "Training-Batch No.41400\n",
      "Loss = 2.2701096534729004\n",
      "Training-Batch No.41410\n",
      "Loss = 2.4598517417907715\n",
      "Training-Batch No.41420\n",
      "Loss = 2.495074987411499\n",
      "Training-Batch No.41430\n",
      "Loss = 8.60372543334961\n",
      "Training-Batch No.41440\n",
      "Loss = 1.5878033638000488\n",
      "Training-Batch No.41450\n",
      "Loss = 1.555429458618164\n",
      "Training-Batch No.41460\n",
      "Loss = 2.3872475624084473\n",
      "Training-Batch No.41470\n",
      "Loss = 1.9982213973999023\n",
      "Training-Batch No.41480\n",
      "Loss = 2.026872158050537\n",
      "Training-Batch No.41490\n",
      "Loss = 7.1143035888671875\n",
      "Training-Batch No.41500\n",
      "Loss = 1.3232687711715698\n",
      "Training-Batch No.41510\n",
      "Loss = 4.836994171142578\n",
      "Training-Batch No.41520\n",
      "Loss = 1.4770267009735107\n",
      "Training-Batch No.41530\n",
      "Loss = 2.1429834365844727\n",
      "Training-Batch No.41540\n",
      "Loss = 1.3850104808807373\n",
      "Training-Batch No.41550\n",
      "Loss = 1.1157822608947754\n",
      "Training-Batch No.41560\n",
      "Loss = 2.0827646255493164\n",
      "Training-Batch No.41570\n",
      "Loss = 2.0593035221099854\n",
      "Training-Batch No.41580\n",
      "Loss = 1.284328579902649\n",
      "Training-Batch No.41590\n",
      "Loss = 2.1951284408569336\n",
      "Training-Batch No.41600\n",
      "Loss = 1.9687284231185913\n",
      "Training-Batch No.41610\n",
      "Loss = 1.2177255153656006\n",
      "Training-Batch No.41620\n",
      "Loss = 4.488847732543945\n",
      "Training-Batch No.41630\n",
      "Loss = 1.6771796941757202\n",
      "Training-Batch No.41640\n",
      "Loss = 1.8786838054656982\n",
      "Training-Batch No.41650\n",
      "Loss = 1.503085970878601\n",
      "Training-Batch No.41660\n",
      "Loss = 2.3391366004943848\n",
      "Training-Batch No.41670\n",
      "Loss = 1.7869383096694946\n",
      "Training-Batch No.41680\n",
      "Loss = 3.8347573280334473\n",
      "Training-Batch No.41690\n",
      "Loss = 1.6571729183197021\n",
      "Training-Batch No.41700\n",
      "Loss = 1.8594892024993896\n",
      "Training-Batch No.41710\n",
      "Loss = 1.9210529327392578\n",
      "Training-Batch No.41720\n",
      "Loss = 1.5323431491851807\n",
      "Training-Batch No.41730\n",
      "Loss = 1.6649055480957031\n",
      "Training-Batch No.41740\n",
      "Loss = 2.3645224571228027\n",
      "Training-Batch No.41750\n",
      "Loss = 3.0999765396118164\n",
      "Training-Batch No.41760\n",
      "Loss = 1.2377827167510986\n",
      "Training-Batch No.41770\n",
      "Loss = 1.7552094459533691\n",
      "Training-Batch No.41780\n",
      "Loss = 1.551688313484192\n",
      "Training-Batch No.41790\n",
      "Loss = 1.2630211114883423\n",
      "Training-Batch No.41800\n",
      "Loss = 1.6659194231033325\n",
      "Training-Batch No.41810\n",
      "Loss = 3.4745073318481445\n",
      "Training-Batch No.41820\n",
      "Loss = 1.7666122913360596\n",
      "Training-Batch No.41830\n",
      "Loss = 1.9498263597488403\n",
      "Training-Batch No.41840\n",
      "Loss = 2.3632116317749023\n",
      "Training-Batch No.41850\n",
      "Loss = 1.5591704845428467\n",
      "Training-Batch No.41860\n",
      "Loss = 1.1397467851638794\n",
      "Training-Batch No.41870\n",
      "Loss = 1.8905525207519531\n",
      "Training-Batch No.41880\n",
      "Loss = 4.957444667816162\n",
      "Training-Batch No.41890\n",
      "Loss = 1.9992588758468628\n",
      "Training-Batch No.41900\n",
      "Loss = 1.5183813571929932\n",
      "Training-Batch No.41910\n",
      "Loss = 1.5327033996582031\n",
      "Training-Batch No.41920\n",
      "Loss = 2.204307794570923\n",
      "Training-Batch No.41930\n",
      "Loss = 1.9417355060577393\n",
      "Training-Batch No.41940\n",
      "Loss = 2.074435234069824\n",
      "Training-Batch No.41950\n",
      "Loss = 2.3443894386291504\n",
      "Training-Batch No.41960\n",
      "Loss = 1.98592209815979\n",
      "Training-Batch No.41970\n",
      "Loss = 1.1200727224349976\n",
      "Training-Batch No.41980\n",
      "Loss = 1.7074847221374512\n",
      "Training-Batch No.41990\n",
      "Loss = 2.4269652366638184\n",
      "Training-Batch No.42000\n",
      "Loss = 1.438117504119873\n",
      "Training-Batch No.42010\n",
      "Loss = 1.6499398946762085\n",
      "Training-Batch No.42020\n",
      "Loss = 1.2281821966171265\n",
      "Training-Batch No.42030\n",
      "Loss = 1.6938486099243164\n",
      "Training-Batch No.42040\n",
      "Loss = 1.703735589981079\n",
      "Training-Batch No.42050\n",
      "Loss = 2.271798610687256\n",
      "Training-Batch No.42060\n",
      "Loss = 1.7723534107208252\n",
      "Training-Batch No.42070\n",
      "Loss = 1.139974594116211\n",
      "Training-Batch No.42080\n",
      "Loss = 1.5705714225769043\n",
      "Epoch 40 Training Loss: 2.0469\n",
      "Start validation\n",
      "Epoch 40 Validation Loss: 2.2147\n",
      "total training examples are 2103\n",
      "Training_size 1--No. of skipped batches 0\n",
      "Average Top-1 accuracy 0.38611507370423204\n",
      "Average Top-3 accuracy 0.7265810746552543\n",
      "Average Top-5 accuracy 0.8659058487874465\n",
      "Average Top-7 accuracy 0.9281978126485972\n",
      "Average Top-9 accuracy 0.9472182596291013\n",
      "Average Top-11 accuracy 0.9524488825487399\n",
      "Average Top-13 accuracy 0.9614835948644793\n",
      "Average Top-15 accuracy 0.9648121730860675\n",
      "current acc 0.38611507370423204\n",
      "best acc 0.3780313837375178\n",
      "Saving the best model\n",
      "updated best accuracy 0.38611507370423204\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "acc_loss = 0\n",
    "itr = []\n",
    "\n",
    "for idx, n in enumerate(train_size):\n",
    "    print('```````````````````````````````````````````````````````')\n",
    "    print('Training size is {}'.format(n))\n",
    "\n",
    "    # Optimization parameters:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(fusion_model.parameters(), lr=lr, weight_decay=decay)\n",
    "    LR_sch = torch.optim.lr_scheduler.MultiStepLR(opt, [4, 8, 12], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "    count = 0\n",
    "    running_loss = []\n",
    "    running_top1_acc = []\n",
    "    running_top3_acc = []\n",
    "    running_top5_acc = []\n",
    "    running_top7_acc = []\n",
    "    running_top9_acc = []\n",
    "    running_top11_acc = []\n",
    "    running_top13_acc = []\n",
    "    running_top15_acc = []\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch No. ' + str(epoch + 1))\n",
    "        skipped_batches = 0\n",
    "        epoch_train_loss = 0  # To track the training loss for the epoch\n",
    "        for tr_count, data in enumerate(train_loader):\n",
    "\n",
    "            img, gps, pnts, label = data\n",
    "            img = img.to(device)\n",
    "            gps = gps.to(device)\n",
    "            pnts = pnts.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            fusion_model.train()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            batch_size, channels, height, width = img.shape\n",
    "            img = img.view(batch_size, channels, height * width)\n",
    "            out = fusion_model(img, gps, pnts)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            batch_loss = loss.item()\n",
    "            acc_loss += batch_loss\n",
    "            epoch_train_loss += batch_loss  # Accumulate batch loss for the epoch\n",
    "            count += 1\n",
    "            if count % 10 == 0:\n",
    "                print('Training-Batch No.' + str(count))\n",
    "                running_loss.append(batch_loss)\n",
    "                itr.append(count)\n",
    "                print('Loss = ' + str(running_loss[-1]))\n",
    "\n",
    "        epoch_train_loss /= len(train_loader)  # Calculate average training loss for the epoch\n",
    "        print(f'Epoch {epoch + 1} Training Loss: {epoch_train_loss:.4f}')\n",
    "\n",
    "        print('Start validation')\n",
    "        ave_top1_acc = 0\n",
    "        ave_top3_acc = 0\n",
    "        ave_top5_acc = 0\n",
    "        ave_top7_acc = 0\n",
    "        ave_top9_acc = 0\n",
    "        ave_top11_acc = 0\n",
    "        ave_top13_acc = 0\n",
    "        ave_top15_acc = 0\n",
    "        val_loss = 0  # To track the validation loss\n",
    "        ind_ten = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "        top1_pred_out = []\n",
    "        top3_pred_out = []\n",
    "        top5_pred_out = []\n",
    "        top7_pred_out = []\n",
    "        top9_pred_out = []\n",
    "        top11_pred_out = []\n",
    "        top13_pred_out = []\n",
    "        top15_pred_out = []\n",
    "        gt_beam = []\n",
    "        total_count = 0\n",
    "\n",
    "        for val_count, data in enumerate(val_loader):\n",
    "\n",
    "            img, gps, pnts, labels = data\n",
    "            # batch_size, channels, height, width = img.shape\n",
    "            # img = img.view(batch_size, channels, height * width)\n",
    "            img = img.to(device)\n",
    "            gps = gps.to(device)\n",
    "            pnts = pnts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "            fusion_model.eval()\n",
    "            batch_size, channels, height, width = img.shape\n",
    "            img = img.view(batch_size, channels, height * width)\n",
    "            out = fusion_model(img, gps, pnts)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            _, top_1_pred = torch.max(out, dim=1)\n",
    "\n",
    "            val_batch_loss = criterion(out, labels).item()  # Calculate validation loss for the batch\n",
    "            val_loss += val_batch_loss  # Accumulate batch loss for the epoch\n",
    "\n",
    "            gt_beam.append(labels.detach().cpu().numpy()[0])\n",
    "\n",
    "            top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0])\n",
    "            sorted_out = torch.argsort(out, dim=1, descending=True)\n",
    "\n",
    "            top_3_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "            top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "            top_5_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "            top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "            top_7_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "            top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "            top_9_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "            top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "            top_11_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "            top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "            top_13_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "            top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "            top_15_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "            top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0])\n",
    "\n",
    "            reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "            tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "            tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "            tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "            tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "            tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "            tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "            tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "            batch_top1_acc = torch.sum(top_1_pred == labels, dtype=torch.float32)\n",
    "            batch_top3_acc = torch.sum(top_3_pred == tiled_3_labels, dtype=torch.float32)\n",
    "            batch_top5_acc = torch.sum(top_5_pred == tiled_5_labels, dtype=torch.float32)\n",
    "            batch_top7_acc = torch.sum(top_7_pred == tiled_7_labels, dtype=torch.float32)\n",
    "            batch_top9_acc = torch.sum(top_9_pred == tiled_9_labels, dtype=torch.float32)\n",
    "            batch_top11_acc = torch.sum(top_11_pred == tiled_11_labels, dtype=torch.float32)\n",
    "            batch_top13_acc = torch.sum(top_13_pred == tiled_13_labels, dtype=torch.float32)\n",
    "            batch_top15_acc = torch.sum(top_15_pred == tiled_15_labels, dtype=torch.float32)\n",
    "\n",
    "            ave_top1_acc += batch_top1_acc.item()\n",
    "            ave_top3_acc += batch_top3_acc.item()\n",
    "            ave_top5_acc += batch_top5_acc.item()\n",
    "            ave_top7_acc += batch_top7_acc.item()\n",
    "            ave_top9_acc += batch_top9_acc.item()\n",
    "            ave_top11_acc += batch_top11_acc.item()\n",
    "            ave_top13_acc += batch_top13_acc.item()\n",
    "            ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "        val_loss /= len(val_loader)  # Calculate average validation loss for the epoch\n",
    "        print(f'Epoch {epoch + 1} Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        print(\"total training examples are\", total_count)\n",
    "        running_top1_acc.append(ave_top1_acc / total_count)\n",
    "        running_top3_acc.append(ave_top3_acc / total_count)\n",
    "        running_top5_acc.append(ave_top5_acc / total_count)\n",
    "        running_top7_acc.append(ave_top7_acc / total_count)\n",
    "        running_top9_acc.append(ave_top9_acc / total_count)\n",
    "        running_top11_acc.append(ave_top11_acc / total_count)\n",
    "        running_top13_acc.append(ave_top13_acc / total_count)\n",
    "        running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "        print('Training_size {}--No. of skipped batches {}'.format(n, skipped_batches))\n",
    "        print('Average Top-1 accuracy {}'.format(running_top1_acc[-1]))\n",
    "        print('Average Top-3 accuracy {}'.format(running_top3_acc[-1]))\n",
    "        print('Average Top-5 accuracy {}'.format(running_top5_acc[-1]))\n",
    "        print('Average Top-7 accuracy {}'.format(running_top7_acc[-1]))\n",
    "        print('Average Top-9 accuracy {}'.format(running_top9_acc[-1]))\n",
    "        print('Average Top-11 accuracy {}'.format(running_top11_acc[-1]))\n",
    "        print('Average Top-13 accuracy {}'.format(running_top13_acc[-1]))\n",
    "        print('Average Top-15 accuracy {}'.format(running_top15_acc[-1]))\n",
    "\n",
    "        cur_accuracy = running_top1_acc[-1]\n",
    "\n",
    "        print(\"current acc\", cur_accuracy)\n",
    "        print(\"best acc\", best_accuracy)\n",
    "        if cur_accuracy > best_accuracy:\n",
    "            print(\"Saving the best model\")\n",
    "            net_name = checkpoint_directory + '//' + '/fusionmodel_64_beam'\n",
    "            torch.save(fusion_model.state_dict(), net_name)\n",
    "            best_accuracy = cur_accuracy\n",
    "        print(\"updated best accuracy\", best_accuracy)\n",
    "\n",
    "\n",
    "    print(\"Saving the predicted value in a csv file\")\n",
    "    file_to_save = f'{save_directory}//topk_pred_beam_val_after_{epoch + 1}th_epoch.csv'\n",
    "    indx = np.arange(1, len(top1_pred_out) + 1, 1)\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['index'] = indx\n",
    "    df1['link_status'] = gt_beam\n",
    "    df1['top1_pred'] = top1_pred_out\n",
    "    df1['top3_pred'] = top3_pred_out\n",
    "    df1['top5_pred'] = top5_pred_out\n",
    "    df1['top7_pred'] = top7_pred_out\n",
    "    df1['top9_pred'] = top9_pred_out\n",
    "    df1['top11_pred'] = top11_pred_out\n",
    "    df1['top13_pred'] = top13_pred_out\n",
    "    df1['top15_pred'] = top15_pred_out\n",
    "    df1.to_csv(file_to_save, index=False)\n",
    "\n",
    "    LR_sch.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "image_test_dir = 'scenario31_64_img_beam_test.csv'\n",
    "pos_test_dir = 'scenario31_64_pos_beam_test.csv'\n",
    "lidar_test_dir = 'scenario31_64_lidar_beam_test.csv'\n",
    "\n",
    "# Load the test data\n",
    "ds_test = DataFeed(path_to_img=image_test_dir,\n",
    "                        path_to_gps=pos_test_dir,\n",
    "                        path_to_lidar=lidar_test_dir,\n",
    "                        transform=rgb_proc_pipe)\n",
    "\n",
    "test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(image_test_dir)\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "link_status_data = test_data['original_unit1_pwr_best-beam'].tolist()\n",
    "org = test_data['original_index'].tolist()\n",
    "pwr_60ghz = test_data['original_unit1_pwr'].tolist()\n",
    "\n",
    "# # Fusion model stays the same\n",
    "# model = FusionModel(rgb_encoder, lidar_encoder, num_features, num_heads).to(device)\n",
    "\n",
    "# checkpoint_path = f'{checkpoint_directory}/fusionmodel_64_beam'\n",
    "# model.load_state_dict(torch.load(checkpoint_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n",
      "total test examples are 702\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.36894586894586895\n",
      "Average Top-3 accuracy 0.7350427350427351\n",
      "Average Top-5 accuracy 0.8603988603988604\n",
      "Average Top-7 accuracy 0.915954415954416\n",
      "Average Top-9 accuracy 0.9401709401709402\n",
      "Average Top-11 accuracy 0.9529914529914529\n",
      "Average Top-13 accuracy 0.9643874643874644\n",
      "Average Top-15 accuracy 0.9672364672364673\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "print('Start Testing')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "running_top1_acc = []\n",
    "running_top3_acc = []\n",
    "running_top5_acc = []\n",
    "running_top7_acc = []\n",
    "running_top9_acc = []\n",
    "running_top11_acc = []\n",
    "running_top13_acc = []\n",
    "running_top15_acc = []\n",
    "total_count = 0\n",
    "\n",
    "gt_beam = []\n",
    "\n",
    "for val_count, data in enumerate(test_loader):\n",
    "\n",
    "    img, gps, pnts, labels = data\n",
    "    # batch_size, channels, height, width = img.shape\n",
    "    # img = img.view(batch_size, channels, height * width)\n",
    "    img = img.to(device)\n",
    "    gps = gps.to(device)\n",
    "    pnts = pnts.to(device)\n",
    "    labels = labels.to(device)\n",
    "    total_count += labels.size(0)\n",
    "\n",
    "    fusion_model.eval()\n",
    "    batch_size, channels, height, width = img.shape\n",
    "    img = img.view(batch_size, channels, height * width)\n",
    "    out = fusion_model(img, gps, pnts)\n",
    "    \n",
    "    \n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total test examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval_Test.csv'\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "# link_status_data = test_data['original_unit1_pwr3_best-beam'].tolist()\n",
    "# org = test_data['original_index'].tolist()\n",
    "# pwr_60ghz = test_data['original_unit1_pwr3'].tolist()\n",
    "\n",
    "indx = test_data.index + 1\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = org\n",
    "df2['link_status'] = link_status_data  # Add the link_status column\n",
    "df2['original_unit1_pwr1'] = pwr_60ghz # Add the original_unit1_pwr_60ghz column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
