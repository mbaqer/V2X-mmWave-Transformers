{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f0868-196a-4215-9be3-8137263ec221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSense Scenario 31 64 Beams: GPS and Image Model Fusion Transformers!\n",
    "\n",
    "# Average Top-1 accuracy 0.45014245014245013\n",
    "# Average Top-3 accuracy 0.8190883190883191\n",
    "# Average Top-5 accuracy 0.9131054131054132\n",
    "# Average Top-7 accuracy 0.9387464387464387\n",
    "# Average Top-9 accuracy 0.9515669515669516\n",
    "# Average Top-11 accuracy 0.9601139601139601\n",
    "# Average Top-13 accuracy 0.9629629629629629\n",
    "# Average Top-15 accuracy 0.9672364672364673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd5875-6e40-45de-a1e8-60c49cfb7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e45418-5b1f-4c57-858e-1f32d5d520ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch as t\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transf\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage import io\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8924f11c-c048-4e07-8676-83fbe880e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper-parameters\n",
    "batch_size = 16 # 32 out of memory problem!\n",
    "val_batch_size = 1\n",
    "lr = 0.001\n",
    "decay = 1e-4\n",
    "num_epochs = 20\n",
    "train_size = [1]\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = 2\n",
    "node = 512\n",
    "output_size = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce65823-1f36-42b9-9091-a881ae141cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image transform\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "image_proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "\n",
    "\n",
    "#POS transform\n",
    "pos_proc_pipe = transf.Compose(\n",
    "    [\n",
    "        transf.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f14f46-4153-40d2-957a-a1ea8c711db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07-25-2024\n",
      "22_35\n"
     ]
    }
   ],
   "source": [
    "# year month day\n",
    "dayTime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "# Minutes and seconds\n",
    "hourTime = datetime.datetime.now().strftime('%H_%M')\n",
    "print(dayTime + '\\n' + hourTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b6ceef-4199-4939-91ac-40cca3164afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baqer\\Desktop\\V2X_CNN_All\\Scenario31_64-Beams\\Main_Folder//saved_folder//07-25-2024_22_35\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd() + '//' + 'saved_folder' + '//' + dayTime + '_' + hourTime\n",
    "print(pwd)\n",
    "# Determine whether the folder already exists\n",
    "isExists = os.path.exists(pwd)\n",
    "if not isExists:\n",
    "    os.makedirs(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac7b204-1037-4e19-aede-29fc74a77f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = pwd + '//' + 'saved_analysis_files'\n",
    "checkpoint_directory = pwd + '//' + 'checkpoint'\n",
    "\n",
    "isExists = os.path.exists(save_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "isExists = os.path.exists(checkpoint_directory)\n",
    "if not isExists:\n",
    "    os.makedirs(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0df101b-47ed-4462-bc61-1720d774bd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 12% | 95% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 12% | 96% |\n"
     ]
    }
   ],
   "source": [
    "# !pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fe4f11-fc5e-462c-8666-89ae4c8d803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e92a756-d926-4603-aee7-929994ae9f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca44b03-fe8a-461c-86a8-b4a638499782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b43a26-048b-4116-8e9f-240336096429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a3bcb6-f829-4143-acbc-c67cbd5ca2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size is 1\n",
      "Epoch No. 1\n",
      "Training-Batch No.100\n",
      "Loss = 3.1596076488494873\n",
      "Training-Batch No.200\n",
      "Loss = 2.6108641624450684\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 2.5504680305818574\n",
      "Average Top-1 accuracy 0.24251069900142652\n",
      "Average Top-3 accuracy 0.6015216357584403\n",
      "Average Top-5 accuracy 0.7779362815026153\n",
      "Average Top-7 accuracy 0.8720874940561103\n",
      "Average Top-9 accuracy 0.9110794103661436\n",
      "Average Top-11 accuracy 0.9310508796956728\n",
      "Average Top-13 accuracy 0.9410366143604375\n",
      "Average Top-15 accuracy 0.9529243937232525\n",
      "Current accuracy: 0.24251069900142652\n",
      "Best accuracy: 0\n",
      "Saving the best model\n",
      "Updated best accuracy: 0.24251069900142652\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 2\n",
      "Training-Batch No.300\n",
      "Loss = 1.9324923753738403\n",
      "Training-Batch No.400\n",
      "Loss = 1.8710750341415405\n",
      "Training-Batch No.500\n",
      "Loss = 2.350407123565674\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 2.1818884924404887\n",
      "Average Top-1 accuracy 0.37375178316690444\n",
      "Average Top-3 accuracy 0.7332382310984308\n",
      "Average Top-5 accuracy 0.8839752734189253\n",
      "Average Top-7 accuracy 0.9220161673799334\n",
      "Average Top-9 accuracy 0.9358059914407989\n",
      "Average Top-11 accuracy 0.9491203043271517\n",
      "Average Top-13 accuracy 0.9591060389919163\n",
      "Average Top-15 accuracy 0.9624346172135045\n",
      "Current accuracy: 0.37375178316690444\n",
      "Best accuracy: 0.24251069900142652\n",
      "Saving the best model\n",
      "Updated best accuracy: 0.37375178316690444\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 3\n",
      "Training-Batch No.600\n",
      "Loss = 2.9917585849761963\n",
      "Training-Batch No.700\n",
      "Loss = 1.799994707107544\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.8541607521881787\n",
      "Average Top-1 accuracy 0.4117926771279125\n",
      "Average Top-3 accuracy 0.7931526390870185\n",
      "Average Top-5 accuracy 0.9077508321445554\n",
      "Average Top-7 accuracy 0.9424631478839752\n",
      "Average Top-9 accuracy 0.9519733713742273\n",
      "Average Top-11 accuracy 0.9600570613409415\n",
      "Average Top-13 accuracy 0.9648121730860675\n",
      "Average Top-15 accuracy 0.9671897289586305\n",
      "Current accuracy: 0.4117926771279125\n",
      "Best accuracy: 0.37375178316690444\n",
      "Saving the best model\n",
      "Updated best accuracy: 0.4117926771279125\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 4\n",
      "Training-Batch No.800\n",
      "Loss = 1.5010838508605957\n",
      "Training-Batch No.900\n",
      "Loss = 0.9532253742218018\n",
      "Training-Batch No.1000\n",
      "Loss = 1.156143069267273\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.8171946017706657\n",
      "Average Top-1 accuracy 0.43033761293390393\n",
      "Average Top-3 accuracy 0.8088445078459344\n",
      "Average Top-5 accuracy 0.908226343319068\n",
      "Average Top-7 accuracy 0.9429386590584878\n",
      "Average Top-9 accuracy 0.9519733713742273\n",
      "Average Top-11 accuracy 0.9605325725154541\n",
      "Average Top-13 accuracy 0.9657631954350927\n",
      "Average Top-15 accuracy 0.9671897289586305\n",
      "Current accuracy: 0.43033761293390393\n",
      "Best accuracy: 0.4117926771279125\n",
      "Saving the best model\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 5\n",
      "Training-Batch No.1100\n",
      "Loss = 2.0660808086395264\n",
      "Training-Batch No.1200\n",
      "Loss = 2.437161922454834\n",
      "Training-Batch No.1300\n",
      "Loss = 1.4542206525802612\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.8775817033282516\n",
      "Average Top-1 accuracy 0.42938659058487877\n",
      "Average Top-3 accuracy 0.8107465525439848\n",
      "Average Top-5 accuracy 0.9025202092249168\n",
      "Average Top-7 accuracy 0.9396100808368997\n",
      "Average Top-9 accuracy 0.9529243937232525\n",
      "Average Top-11 accuracy 0.9586305278174037\n",
      "Average Top-13 accuracy 0.9633856395625298\n",
      "Average Top-15 accuracy 0.9657631954350927\n",
      "Current accuracy: 0.42938659058487877\n",
      "Best accuracy: 0.43033761293390393\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 6\n",
      "Training-Batch No.1400\n",
      "Loss = 1.3729496002197266\n",
      "Training-Batch No.1500\n",
      "Loss = 1.6997804641723633\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.8818505705377202\n",
      "Average Top-1 accuracy 0.42082738944365194\n",
      "Average Top-3 accuracy 0.8064669519733714\n",
      "Average Top-5 accuracy 0.908226343319068\n",
      "Average Top-7 accuracy 0.9438896814075131\n",
      "Average Top-9 accuracy 0.9529243937232525\n",
      "Average Top-11 accuracy 0.9595815501664289\n",
      "Average Top-13 accuracy 0.9638611507370424\n",
      "Average Top-15 accuracy 0.9667142177841179\n",
      "Current accuracy: 0.42082738944365194\n",
      "Best accuracy: 0.43033761293390393\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 7\n",
      "Training-Batch No.1600\n",
      "Loss = 1.7174782752990723\n",
      "Training-Batch No.1700\n",
      "Loss = 1.228816032409668\n",
      "Training-Batch No.1800\n",
      "Loss = 1.0451298952102661\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.9427691041166428\n",
      "Average Top-1 accuracy 0.41512125534950073\n",
      "Average Top-3 accuracy 0.7941036614360437\n",
      "Average Top-5 accuracy 0.9039467427484545\n",
      "Average Top-7 accuracy 0.9329529243937232\n",
      "Average Top-9 accuracy 0.9453162149310509\n",
      "Average Top-11 accuracy 0.9543509272467903\n",
      "Average Top-13 accuracy 0.9595815501664289\n",
      "Average Top-15 accuracy 0.9643366619115549\n",
      "Current accuracy: 0.41512125534950073\n",
      "Best accuracy: 0.43033761293390393\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 8\n",
      "Training-Batch No.1900\n",
      "Loss = 1.6090515851974487\n",
      "Training-Batch No.2000\n",
      "Loss = 1.7458597421646118\n",
      "Training-Batch No.2100\n",
      "Loss = 0.9313575029373169\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.9566579987669184\n",
      "Average Top-1 accuracy 0.4141702330004755\n",
      "Average Top-3 accuracy 0.7969567284831194\n",
      "Average Top-5 accuracy 0.8920589633856396\n",
      "Average Top-7 accuracy 0.9253447456015217\n",
      "Average Top-9 accuracy 0.9329529243937232\n",
      "Average Top-11 accuracy 0.9505468378506895\n",
      "Average Top-13 accuracy 0.9553019495958155\n",
      "Average Top-15 accuracy 0.9619591060389919\n",
      "Current accuracy: 0.4141702330004755\n",
      "Best accuracy: 0.43033761293390393\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 9\n",
      "Training-Batch No.2200\n",
      "Loss = 1.50807785987854\n",
      "Training-Batch No.2300\n",
      "Loss = 2.017409324645996\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.955009949118558\n",
      "Average Top-1 accuracy 0.39514978601997147\n",
      "Average Top-3 accuracy 0.7788873038516405\n",
      "Average Top-5 accuracy 0.8906324298621018\n",
      "Average Top-7 accuracy 0.9258202567760342\n",
      "Average Top-9 accuracy 0.9362815026153115\n",
      "Average Top-11 accuracy 0.9443651925820257\n",
      "Average Top-13 accuracy 0.9510223490252021\n",
      "Average Top-15 accuracy 0.9553019495958155\n",
      "Current accuracy: 0.39514978601997147\n",
      "Best accuracy: 0.43033761293390393\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 10\n",
      "Training-Batch No.2400\n",
      "Loss = 1.5191192626953125\n",
      "Training-Batch No.2500\n",
      "Loss = 1.5989173650741577\n",
      "Training-Batch No.2600\n",
      "Loss = 2.017789840698242\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.9603335309082388\n",
      "Average Top-1 accuracy 0.4098906324298621\n",
      "Average Top-3 accuracy 0.7983832620066571\n",
      "Average Top-5 accuracy 0.8991916310033286\n",
      "Average Top-7 accuracy 0.9281978126485972\n",
      "Average Top-9 accuracy 0.9405611031859249\n",
      "Average Top-11 accuracy 0.9514978601997147\n",
      "Average Top-13 accuracy 0.9567284831193533\n",
      "Average Top-15 accuracy 0.9591060389919163\n",
      "Current accuracy: 0.4098906324298621\n",
      "Best accuracy: 0.43033761293390393\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 11\n",
      "Training-Batch No.2700\n",
      "Loss = 1.628676176071167\n",
      "Training-Batch No.2800\n",
      "Loss = 1.0788995027542114\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.8882923351177396\n",
      "Average Top-1 accuracy 0.4217784117926771\n",
      "Average Top-3 accuracy 0.8202567760342369\n",
      "Average Top-5 accuracy 0.9163100332857822\n",
      "Average Top-7 accuracy 0.9424631478839752\n",
      "Average Top-9 accuracy 0.9524488825487399\n",
      "Average Top-11 accuracy 0.9572039942938659\n",
      "Average Top-13 accuracy 0.9600570613409415\n",
      "Average Top-15 accuracy 0.9629101283880172\n",
      "Current accuracy: 0.4217784117926771\n",
      "Best accuracy: 0.43033761293390393\n",
      "Updated best accuracy: 0.43033761293390393\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 12\n",
      "Training-Batch No.2900\n",
      "Loss = 1.231452465057373\n",
      "Training-Batch No.3000\n",
      "Loss = 1.5869944095611572\n",
      "Training-Batch No.3100\n",
      "Loss = 1.3475589752197266\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.8661677097548304\n",
      "Average Top-1 accuracy 0.4346172135045174\n",
      "Average Top-3 accuracy 0.8159771754636234\n",
      "Average Top-5 accuracy 0.9006181645268664\n",
      "Average Top-7 accuracy 0.9353304802662863\n",
      "Average Top-9 accuracy 0.9472182596291013\n",
      "Average Top-11 accuracy 0.953399904897765\n",
      "Average Top-13 accuracy 0.9567284831193533\n",
      "Average Top-15 accuracy 0.9605325725154541\n",
      "Current accuracy: 0.4346172135045174\n",
      "Best accuracy: 0.43033761293390393\n",
      "Saving the best model\n",
      "Updated best accuracy: 0.4346172135045174\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 13\n",
      "Training-Batch No.3200\n",
      "Loss = 1.8289577960968018\n",
      "Training-Batch No.3300\n",
      "Loss = 2.075195550918579\n",
      "Training-Batch No.3400\n",
      "Loss = 0.9709765911102295\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.983010541689301\n",
      "Average Top-1 accuracy 0.41844983357108895\n",
      "Average Top-3 accuracy 0.7812648597242036\n",
      "Average Top-5 accuracy 0.8801711840228246\n",
      "Average Top-7 accuracy 0.920114122681883\n",
      "Average Top-9 accuracy 0.9381835473133618\n",
      "Average Top-11 accuracy 0.9491203043271517\n",
      "Average Top-13 accuracy 0.9557774607703281\n",
      "Average Top-15 accuracy 0.9610080836899667\n",
      "Current accuracy: 0.41844983357108895\n",
      "Best accuracy: 0.4346172135045174\n",
      "Updated best accuracy: 0.4346172135045174\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 14\n",
      "Training-Batch No.3500\n",
      "Loss = 1.1933786869049072\n",
      "Training-Batch No.3600\n",
      "Loss = 1.3067271709442139\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.9729260552766534\n",
      "Average Top-1 accuracy 0.4027579648121731\n",
      "Average Top-3 accuracy 0.7945791726105563\n",
      "Average Top-5 accuracy 0.8887303851640513\n",
      "Average Top-7 accuracy 0.922491678554446\n",
      "Average Top-9 accuracy 0.9362815026153115\n",
      "Average Top-11 accuracy 0.9453162149310509\n",
      "Average Top-13 accuracy 0.9524488825487399\n",
      "Average Top-15 accuracy 0.9591060389919163\n",
      "Current accuracy: 0.4027579648121731\n",
      "Best accuracy: 0.4346172135045174\n",
      "Updated best accuracy: 0.4346172135045174\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 15\n",
      "Training-Batch No.3700\n",
      "Loss = 1.3051989078521729\n",
      "Training-Batch No.3800\n",
      "Loss = 1.1659960746765137\n",
      "Training-Batch No.3900\n",
      "Loss = 0.8981045484542847\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 2.1257293484498327\n",
      "Average Top-1 accuracy 0.41512125534950073\n",
      "Average Top-3 accuracy 0.7750832144555397\n",
      "Average Top-5 accuracy 0.8649548264384213\n",
      "Average Top-7 accuracy 0.9025202092249168\n",
      "Average Top-9 accuracy 0.9239182120779839\n",
      "Average Top-11 accuracy 0.9348549690917737\n",
      "Average Top-13 accuracy 0.9419876367094627\n",
      "Average Top-15 accuracy 0.9495958155016643\n",
      "Current accuracy: 0.41512125534950073\n",
      "Best accuracy: 0.4346172135045174\n",
      "Updated best accuracy: 0.4346172135045174\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 16\n",
      "Training-Batch No.4000\n",
      "Loss = 1.557319164276123\n",
      "Training-Batch No.4100\n",
      "Loss = 1.3125996589660645\n",
      "Training-Batch No.4200\n",
      "Loss = 0.9791558980941772\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.8140092667384198\n",
      "Average Top-1 accuracy 0.47503566333808844\n",
      "Average Top-3 accuracy 0.8454588682834047\n",
      "Average Top-5 accuracy 0.9205896338563956\n",
      "Average Top-7 accuracy 0.9457917261055635\n",
      "Average Top-9 accuracy 0.9548264384213029\n",
      "Average Top-11 accuracy 0.9581550166428912\n",
      "Average Top-13 accuracy 0.9633856395625298\n",
      "Average Top-15 accuracy 0.9681407513076558\n",
      "Current accuracy: 0.47503566333808844\n",
      "Best accuracy: 0.4346172135045174\n",
      "Saving the best model\n",
      "Updated best accuracy: 0.47503566333808844\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 17\n",
      "Training-Batch No.4300\n",
      "Loss = 1.115990161895752\n",
      "Training-Batch No.4400\n",
      "Loss = 0.5745900273323059\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 1.90466506554145\n",
      "Average Top-1 accuracy 0.46504992867332384\n",
      "Average Top-3 accuracy 0.8326200665715644\n",
      "Average Top-5 accuracy 0.9163100332857822\n",
      "Average Top-7 accuracy 0.9391345696623871\n",
      "Average Top-9 accuracy 0.9543509272467903\n",
      "Average Top-11 accuracy 0.9572039942938659\n",
      "Average Top-13 accuracy 0.9614835948644793\n",
      "Average Top-15 accuracy 0.9676652401331431\n",
      "Current accuracy: 0.46504992867332384\n",
      "Best accuracy: 0.47503566333808844\n",
      "Updated best accuracy: 0.47503566333808844\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 18\n",
      "Training-Batch No.4500\n",
      "Loss = 1.1655699014663696\n",
      "Training-Batch No.4600\n",
      "Loss = 1.4922353029251099\n",
      "Training-Batch No.4700\n",
      "Loss = 0.7819492220878601\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 2.0154302788730023\n",
      "Average Top-1 accuracy 0.4484070375653828\n",
      "Average Top-3 accuracy 0.8254873989538755\n",
      "Average Top-5 accuracy 0.908226343319068\n",
      "Average Top-7 accuracy 0.9372325249643366\n",
      "Average Top-9 accuracy 0.9491203043271517\n",
      "Average Top-11 accuracy 0.9557774607703281\n",
      "Average Top-13 accuracy 0.9605325725154541\n",
      "Average Top-15 accuracy 0.9638611507370424\n",
      "Current accuracy: 0.4484070375653828\n",
      "Best accuracy: 0.47503566333808844\n",
      "Updated best accuracy: 0.47503566333808844\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 19\n",
      "Training-Batch No.4800\n",
      "Loss = 1.1062126159667969\n",
      "Training-Batch No.4900\n",
      "Loss = 1.3647915124893188\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 2.161724222189144\n",
      "Average Top-1 accuracy 0.43937232524964337\n",
      "Average Top-3 accuracy 0.8126485972420352\n",
      "Average Top-5 accuracy 0.901093675701379\n",
      "Average Top-7 accuracy 0.9300998573466477\n",
      "Average Top-9 accuracy 0.9453162149310509\n",
      "Average Top-11 accuracy 0.9543509272467903\n",
      "Average Top-13 accuracy 0.9600570613409415\n",
      "Average Top-15 accuracy 0.9624346172135045\n",
      "Current accuracy: 0.43937232524964337\n",
      "Best accuracy: 0.47503566333808844\n",
      "Updated best accuracy: 0.47503566333808844\n",
      "Saving the predicted values in a CSV file\n",
      "Epoch No. 20\n",
      "Training-Batch No.5000\n",
      "Loss = 1.7022095918655396\n",
      "Training-Batch No.5100\n",
      "Loss = 0.7921979427337646\n",
      "Training-Batch No.5200\n",
      "Loss = 0.7779116630554199\n",
      "Start Validation\n",
      "Total validation examples: 2103\n",
      "Validation Loss: 2.358191128004874\n",
      "Average Top-1 accuracy 0.4322396576319544\n",
      "Average Top-3 accuracy 0.8040893961008083\n",
      "Average Top-5 accuracy 0.8887303851640513\n",
      "Average Top-7 accuracy 0.9215406562054208\n",
      "Average Top-9 accuracy 0.9391345696623871\n",
      "Average Top-11 accuracy 0.9481692819781264\n",
      "Average Top-13 accuracy 0.9572039942938659\n",
      "Average Top-15 accuracy 0.9619591060389919\n",
      "Current accuracy: 0.4322396576319544\n",
      "Best accuracy: 0.47503566333808844\n",
      "Updated best accuracy: 0.47503566333808844\n",
      "Saving the predicted values in a CSV file\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as transf\n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from skimage import io\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.nclass = num_classes\n",
    "        self.nf = num_features\n",
    "\n",
    "        # Using MaxViT from timm\n",
    "        self.maxvit = timm.create_model('maxvit_tiny_tf_224.in1k', pretrained=True)\n",
    "        self.maxvit_fc = nn.Linear(self.maxvit.num_features, 128)\n",
    "\n",
    "        # Using a simple MLP for the beam prediction\n",
    "        self.positional_fc = nn.Sequential(\n",
    "            nn.Linear(self.nf, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, self.nclass)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Process image data through MaxViT\n",
    "        x = self.maxvit.forward_features(inputs[0])\n",
    "        x = x.mean(dim=[2, 3])  # Global average pooling\n",
    "        x = self.maxvit_fc(x)\n",
    "\n",
    "        # Process positional data through the simple MLP\n",
    "        y = self.positional_fc(inputs[1].reshape([-1, self.nf]))\n",
    "\n",
    "        # Concatenate and pass through fully connected layers\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "def create_samples(root, shuffle=False, nat_sort=False):\n",
    "    f = pd.read_csv(root)\n",
    "    data_samples = []\n",
    "    for idx, row in f.iterrows():\n",
    "        data = list(row.values[1:])\n",
    "        data_samples.append(data)\n",
    "    return data_samples\n",
    "\n",
    "class DataFeed(Dataset):\n",
    "    def __init__(self, image_dir, pos_dir, nat_sort=False, image_transform=None, pos_transform=None, init_shuflle=True):\n",
    "        self.root1 = image_dir\n",
    "        self.root2 = pos_dir\n",
    "        self.samples1 = create_samples(self.root1, shuffle=init_shuflle, nat_sort=nat_sort)\n",
    "        self.samples2 = create_samples(self.root2, shuffle=init_shuflle, nat_sort=nat_sort)\n",
    "        self.transform1 = image_transform\n",
    "        self.transform2 = pos_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample1 = self.samples1[idx]\n",
    "        img = io.imread(sample1[0])\n",
    "        if self.transform1:\n",
    "            img = self.transform1(img)\n",
    "        label = sample1[1]\n",
    "\n",
    "        sample2 = self.samples2[idx]\n",
    "        pos_val = sample2[:1]\n",
    "        pos_val = ast.literal_eval(pos_val[0])\n",
    "        pos_val = np.asarray(pos_val)\n",
    "\n",
    "        return ([img, pos_val], label)\n",
    "\n",
    "### Image Transform\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "image_proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "\n",
    "### POS Transform\n",
    "pos_proc_pipe = transf.Compose(\n",
    "    [\n",
    "        transf.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_train_dir = './scenario31_64_img_beam_train.csv'\n",
    "image_val_dir = './scenario31_64_img_beam_val.csv'\n",
    "pos_train_dir = './scenario31_64_pos_beam_train.csv'\n",
    "pos_val_dir = './scenario31_64_pos_beam_val.csv'\n",
    "\n",
    "train_loader = DataLoader(DataFeed(image_train_dir, pos_train_dir, image_transform=image_proc_pipe),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)\n",
    "val_loader = DataLoader(DataFeed(image_val_dir, pos_val_dir, image_transform=image_proc_pipe),\n",
    "                        batch_size=val_batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FusionModel(num_features=input_size, num_classes=output_size).to(device)\n",
    "\n",
    "### Training and Validation Logic\n",
    "val_acc = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optimizer.Adam(model.parameters(), lr=lr, weight_decay=decay)\n",
    "LR_sch = optimizer.lr_scheduler.MultiStepLR(opt, [15, 25, 40], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "top_1 = np.zeros((1, len(train_size)))\n",
    "top_3 = np.zeros((1, len(train_size)))\n",
    "top_5 = np.zeros((1, len(train_size)))\n",
    "top_7 = np.zeros((1, len(train_size)))\n",
    "top_9 = np.zeros((1, len(train_size)))\n",
    "top_11 = np.zeros((1, len(train_size)))\n",
    "top_13 = np.zeros((1, len(train_size)))\n",
    "top_15 = np.zeros((1, len(train_size)))\n",
    "acc_loss = 0\n",
    "itr = []\n",
    "\n",
    "for idx, n in enumerate(train_size):\n",
    "    print('Training size is {}'.format(n))\n",
    "    net = model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optimizer.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "    LR_sch = optimizer.lr_scheduler.MultiStepLR(opt, [15, 25, 40], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "    count = 0\n",
    "    running_loss = []\n",
    "    running_top1_acc = []\n",
    "    running_top3_acc = []\n",
    "    running_top5_acc = []\n",
    "    running_top7_acc = []\n",
    "    running_top9_acc = []\n",
    "    running_top11_acc = []\n",
    "    running_top13_acc = []\n",
    "    running_top15_acc = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch No. ' + str(epoch + 1))\n",
    "        skipped_batches = 0\n",
    "        net.train()\n",
    "        for tr_count, (pos_data, beam_val) in enumerate(train_loader):\n",
    "            data = [pos_data[0].to(device), pos_data[1].type(torch.Tensor).to(device)]\n",
    "            label = beam_val.type(torch.LongTensor).to(device)\n",
    "            opt.zero_grad()\n",
    "            out = net(data)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            batch_loss = loss.item()\n",
    "            acc_loss += batch_loss\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print('Training-Batch No.' + str(count))\n",
    "                running_loss.append(batch_loss)\n",
    "                itr.append(count)\n",
    "                print('Loss = ' + str(running_loss[-1]))\n",
    "\n",
    "        print('Start Validation')\n",
    "        ave_top1_acc = 0\n",
    "        ave_top3_acc = 0\n",
    "        ave_top5_acc = 0\n",
    "        ave_top7_acc = 0\n",
    "        ave_top9_acc = 0\n",
    "        ave_top11_acc = 0\n",
    "        ave_top13_acc = 0\n",
    "        ave_top15_acc = 0\n",
    "\n",
    "        ind_ten = torch.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device=device)\n",
    "\n",
    "        top1_pred_out = []\n",
    "        top3_pred_out = []\n",
    "        top5_pred_out = []\n",
    "        top7_pred_out = []\n",
    "        top9_pred_out = []\n",
    "        top11_pred_out = []\n",
    "        top13_pred_out = []\n",
    "        top15_pred_out = []\n",
    "\n",
    "        total_count = 0\n",
    "        gt_beam = []\n",
    "\n",
    "        # To track validation loss\n",
    "        val_loss = 0\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_count, (pos_data, beam_val) in enumerate(val_loader):\n",
    "                data = [pos_data[0].to(device), pos_data[1].type(torch.Tensor).to(device)]\n",
    "                labels = beam_val.type(torch.LongTensor).to(device)\n",
    "                gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "                total_count += labels.size(0)\n",
    "                out = net(data)\n",
    "                loss = criterion(out, labels)  # Calculate validation loss\n",
    "                val_loss += loss.item()  # Accumulate validation loss\n",
    "                _, top_1_pred = torch.max(out, dim=1)\n",
    "                top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "                sorted_out = torch.argsort(out, dim=1, descending=True)\n",
    "\n",
    "                top_3_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "                top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_5_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "                top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_7_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "                top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_9_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "                top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "\n",
    "                top_11_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "                top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_13_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "                top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                top_15_pred = torch.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "                top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "                reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "                tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "                tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "                tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "                tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "                tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "                tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "                tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "                batch_top1_acc = torch.sum(top_1_pred == labels, dtype=torch.float32)\n",
    "                batch_top3_acc = torch.sum(top_3_pred == tiled_3_labels, dtype=torch.float32)\n",
    "                batch_top5_acc = torch.sum(top_5_pred == tiled_5_labels, dtype=torch.float32)\n",
    "                batch_top7_acc = torch.sum(top_7_pred == tiled_7_labels, dtype=torch.float32)\n",
    "                batch_top9_acc = torch.sum(top_9_pred == tiled_9_labels, dtype=torch.float32)\n",
    "                batch_top11_acc = torch.sum(top_11_pred == tiled_11_labels, dtype=torch.float32)\n",
    "                batch_top13_acc = torch.sum(top_13_pred == tiled_13_labels, dtype=torch.float32)\n",
    "                batch_top15_acc = torch.sum(top_15_pred == tiled_15_labels, dtype=torch.float32)\n",
    "\n",
    "                ave_top1_acc += batch_top1_acc.item()\n",
    "                ave_top3_acc += batch_top3_acc.item()\n",
    "                ave_top5_acc += batch_top5_acc.item()\n",
    "                ave_top7_acc += batch_top7_acc.item()\n",
    "                ave_top9_acc += batch_top9_acc.item()\n",
    "                ave_top11_acc += batch_top11_acc.item()\n",
    "                ave_top13_acc += batch_top13_acc.item()\n",
    "                ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "        print(\"Total validation examples:\", total_count)\n",
    "        running_top1_acc.append(ave_top1_acc / total_count)\n",
    "        running_top3_acc.append(ave_top3_acc / total_count)\n",
    "        running_top5_acc.append(ave_top5_acc / total_count)\n",
    "        running_top7_acc.append(ave_top7_acc / total_count)\n",
    "        running_top9_acc.append(ave_top9_acc / total_count)\n",
    "        running_top11_acc.append(ave_top11_acc / total_count)\n",
    "        running_top13_acc.append(ave_top13_acc / total_count)\n",
    "        running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "        print('Validation Loss: {}'.format(val_loss / len(val_loader)))  # Print the validation loss\n",
    "        print('Average Top-1 accuracy {}'.format(running_top1_acc[-1]))\n",
    "        print('Average Top-3 accuracy {}'.format(running_top3_acc[-1]))\n",
    "        print('Average Top-5 accuracy {}'.format(running_top5_acc[-1]))\n",
    "        print('Average Top-7 accuracy {}'.format(running_top7_acc[-1]))\n",
    "        print('Average Top-9 accuracy {}'.format(running_top9_acc[-1]))\n",
    "        print('Average Top-11 accuracy {}'.format(running_top11_acc[-1]))\n",
    "        print('Average Top-13 accuracy {}'.format(running_top13_acc[-1]))\n",
    "        print('Average Top-15 accuracy {}'.format(running_top15_acc[-1]))\n",
    "\n",
    "        cur_accuracy = running_top1_acc[-1]\n",
    "\n",
    "        print(\"Current accuracy:\", cur_accuracy)\n",
    "        print(\"Best accuracy:\", best_accuracy)\n",
    "        if cur_accuracy > best_accuracy:\n",
    "            print(\"Saving the best model\")\n",
    "            net_name = checkpoint_directory + '/fused_transfomer_beam_pred'\n",
    "            torch.save(net.state_dict(), net_name)\n",
    "            best_accuracy = cur_accuracy\n",
    "        print(\"Updated best accuracy:\", best_accuracy)\n",
    "\n",
    "        print(\"Saving the predicted values in a CSV file\")\n",
    "        file_to_save = f'{save_directory}/topk_pred_beam_val_after_{epoch+1}th_epoch.csv'\n",
    "        indx = np.arange(1, len(top1_pred_out) + 1, 1)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['index'] = indx\n",
    "        df1['link_status'] = gt_beam\n",
    "        df1['top1_pred'] = top1_pred_out\n",
    "        df1['top3_pred'] = top3_pred_out\n",
    "        df1['top5_pred'] = top5_pred_out\n",
    "        df1['top7_pred'] = top7_pred_out\n",
    "        df1['top9_pred'] = top9_pred_out\n",
    "        df1['top11_pred'] = top11_pred_out\n",
    "        df1['top13_pred'] = top13_pred_out\n",
    "        df1['top15_pred'] = top15_pred_out\n",
    "        df1.to_csv(file_to_save, index=False)\n",
    "\n",
    "        LR_sch.step()\n",
    "\n",
    "    top_1[0, idx] = running_top1_acc[-1]\n",
    "    top_3[0, idx] = running_top3_acc[-1]\n",
    "    top_5[0, idx] = running_top5_acc[-1]\n",
    "    top_7[0, idx] = running_top7_acc[-1]\n",
    "    top_9[0, idx] = running_top9_acc[-1]\n",
    "    top_11[0, idx] = running_top11_acc[-1]\n",
    "    top_13[0, idx] = running_top13_acc[-1]\n",
    "    top_15[0, idx] = running_top15_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b957b51b-6244-4f68-bdd2-a85fd611cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n",
      "total examples are 2103\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.4322396576319544\n",
      "Average Top-3 accuracy 0.8040893961008083\n",
      "Average Top-5 accuracy 0.8887303851640513\n",
      "Average Top-7 accuracy 0.9215406562054208\n",
      "Average Top-9 accuracy 0.9391345696623871\n",
      "Average Top-11 accuracy 0.9481692819781264\n",
      "Average Top-13 accuracy 0.9572039942938659\n",
      "Average Top-15 accuracy 0.9619591060389919\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "print('Start Validation')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "running_top1_acc = []\n",
    "running_top3_acc = []\n",
    "running_top5_acc = []\n",
    "running_top7_acc = []\n",
    "running_top9_acc = []\n",
    "running_top11_acc = []\n",
    "running_top13_acc = []\n",
    "running_top15_acc = []\n",
    "total_count = 0\n",
    "\n",
    "gt_beam = []\n",
    "\n",
    "for val_count, (pos_data, beam_val) in enumerate(val_loader):\n",
    "    net.eval()\n",
    "    data = [pos_data[0], pos_data[1].type(torch.Tensor)]\n",
    "    label = beam_val.type(torch.LongTensor)\n",
    "    x = [data[0].cuda(), data[1].cuda()]\n",
    "    #print(\"x size\", x.size())\n",
    "    opt.zero_grad()\n",
    "    labels = label.cuda()\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval.csv'\n",
    "\n",
    "indx = np.arange(1, len(top1_pred_out)+1, 1)\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = indx\n",
    "df2['link_status'] = gt_beam  # Add the link_status column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea80802-addc-4154-b84b-966492408ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6efdc268-9f70-4be0-98ac-739397c2f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model checkpoint\n",
    "image_test_dir = './scenario31_64_img_beam_test.csv'\n",
    "pos_test_dir = './scenario31_64_pos_beam_test.csv'\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(image_test_dir)\n",
    "\n",
    "# Extract the 'unit1_pwr1_best-beam' data and convert it to a list\n",
    "link_status_data = test_data['original_unit1_pwr_best-beam'].tolist()\n",
    "org = test_data['original_index'].tolist()\n",
    "pwr_60ghz = test_data['original_unit1_pwr'].tolist()\n",
    "\n",
    "checkpoint_path = f'{checkpoint_directory}/fused_transfomer_beam_pred'\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "net = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e66dd8-1e93-4666-95f0-89c7bf0c2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(DataFeed(image_test_dir, pos_test_dir, image_transform=image_proc_pipe),\n",
    "                            batch_size=val_batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51716e8b-4373-42cf-bbf9-093ff9385f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n",
      "total test examples are 702\n",
      "Training_size 1--No. of skipped batchess 0\n",
      "Average Top-1 accuracy 0.45014245014245013\n",
      "Average Top-3 accuracy 0.8190883190883191\n",
      "Average Top-5 accuracy 0.9131054131054132\n",
      "Average Top-7 accuracy 0.9387464387464387\n",
      "Average Top-9 accuracy 0.9515669515669516\n",
      "Average Top-11 accuracy 0.9601139601139601\n",
      "Average Top-13 accuracy 0.9629629629629629\n",
      "Average Top-15 accuracy 0.9672364672364673\n",
      "Saving the predicted value in a csv file\n"
     ]
    }
   ],
   "source": [
    "print('Start Testing')\n",
    "ave_top1_acc = 0\n",
    "ave_top3_acc = 0\n",
    "ave_top5_acc = 0\n",
    "ave_top7_acc = 0\n",
    "ave_top9_acc = 0\n",
    "ave_top11_acc = 0\n",
    "ave_top13_acc = 0\n",
    "ave_top15_acc = 0\n",
    "ind_ten = t.as_tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], device='cuda:0')\n",
    "top1_pred_out = []\n",
    "top3_pred_out = []\n",
    "top5_pred_out = []\n",
    "top7_pred_out = []\n",
    "top9_pred_out = []\n",
    "top11_pred_out = []\n",
    "top13_pred_out = []\n",
    "top15_pred_out = []\n",
    "running_top1_acc = []\n",
    "running_top3_acc = []\n",
    "running_top5_acc = []\n",
    "running_top7_acc = []\n",
    "running_top9_acc = []\n",
    "running_top11_acc = []\n",
    "running_top13_acc = []\n",
    "running_top15_acc = []\n",
    "total_count = 0\n",
    "\n",
    "gt_beam = []\n",
    "\n",
    "for val_count, (pos_data, beam_val) in enumerate(test_loader):\n",
    "    net.eval()\n",
    "    data = [pos_data[0], pos_data[1].type(torch.Tensor)]\n",
    "    label = beam_val.type(torch.LongTensor)\n",
    "    x = [data[0].cuda(), data[1].cuda()]\n",
    "    #print(\"x size\", x.size())\n",
    "    opt.zero_grad()\n",
    "    labels = label.cuda()\n",
    "\n",
    "    gt_beam.append(labels.detach().cpu().numpy()[0].tolist())\n",
    "    total_count += labels.size(0)\n",
    "    out = net.forward(x)\n",
    "    _, top_1_pred = t.max(out, dim=1)\n",
    "    top1_pred_out.append(top_1_pred.detach().cpu().numpy()[0].tolist())\n",
    "    sorted_out = t.argsort(out, dim=1, descending=True)\n",
    "\n",
    "    top_3_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:3])\n",
    "    top3_pred_out.append(top_3_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_5_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:5])\n",
    "    top5_pred_out.append(top_5_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_7_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:7])\n",
    "    top7_pred_out.append(top_7_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_9_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:9])\n",
    "    top9_pred_out.append(top_9_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_11_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:11])\n",
    "    top11_pred_out.append(top_11_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_13_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:13])\n",
    "    top13_pred_out.append(top_13_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    top_15_pred = t.index_select(sorted_out, dim=1, index=ind_ten[0:15])\n",
    "    top15_pred_out.append(top_15_pred.detach().cpu().numpy()[0].tolist())\n",
    "\n",
    "    reshaped_labels = labels.reshape((labels.shape[0], 1))\n",
    "    tiled_3_labels = reshaped_labels.repeat(1, 3)\n",
    "    tiled_5_labels = reshaped_labels.repeat(1, 5)\n",
    "    tiled_7_labels = reshaped_labels.repeat(1, 7)\n",
    "    tiled_9_labels = reshaped_labels.repeat(1, 9)\n",
    "    tiled_11_labels = reshaped_labels.repeat(1, 11)\n",
    "    tiled_13_labels = reshaped_labels.repeat(1, 13)\n",
    "    tiled_15_labels = reshaped_labels.repeat(1, 15)\n",
    "\n",
    "    batch_top1_acc = t.sum(top_1_pred == labels, dtype=t.float32)\n",
    "    batch_top3_acc = t.sum(top_3_pred == tiled_3_labels, dtype=t.float32)\n",
    "    batch_top5_acc = t.sum(top_5_pred == tiled_5_labels, dtype=t.float32)\n",
    "    batch_top7_acc = t.sum(top_7_pred == tiled_7_labels, dtype=t.float32)\n",
    "    batch_top9_acc = t.sum(top_9_pred == tiled_9_labels, dtype=t.float32)\n",
    "    batch_top11_acc = t.sum(top_11_pred == tiled_11_labels, dtype=t.float32)\n",
    "    batch_top13_acc = t.sum(top_13_pred == tiled_13_labels, dtype=t.float32)\n",
    "    batch_top15_acc = t.sum(top_15_pred == tiled_15_labels, dtype=t.float32)\n",
    "\n",
    "    ave_top1_acc += batch_top1_acc.item()\n",
    "    ave_top3_acc += batch_top3_acc.item()\n",
    "    ave_top5_acc += batch_top5_acc.item()\n",
    "    ave_top7_acc += batch_top7_acc.item()\n",
    "    ave_top9_acc += batch_top9_acc.item()\n",
    "    ave_top11_acc += batch_top11_acc.item()\n",
    "    ave_top13_acc += batch_top13_acc.item()\n",
    "    ave_top15_acc += batch_top15_acc.item()\n",
    "\n",
    "print(\"total test examples are\", total_count)\n",
    "running_top1_acc.append(ave_top1_acc / total_count)  # (batch_size * (count_2 + 1)) )\n",
    "running_top3_acc.append(ave_top3_acc / total_count)\n",
    "running_top5_acc.append(ave_top5_acc / total_count)\n",
    "running_top7_acc.append(ave_top7_acc / total_count)\n",
    "running_top9_acc.append(ave_top9_acc / total_count)\n",
    "running_top11_acc.append(ave_top11_acc / total_count)\n",
    "running_top13_acc.append(ave_top13_acc / total_count)\n",
    "running_top15_acc.append(ave_top15_acc / total_count)\n",
    "\n",
    "print('Training_size {}--No. of skipped batchess {}'.format(n,skipped_batches))\n",
    "print('Average Top-1 accuracy {}'.format( running_top1_acc[-1]))\n",
    "print('Average Top-3 accuracy {}'.format( running_top3_acc[-1]))\n",
    "print('Average Top-5 accuracy {}'.format( running_top5_acc[-1]))\n",
    "print('Average Top-7 accuracy {}'.format( running_top7_acc[-1]))\n",
    "print('Average Top-9 accuracy {}'.format( running_top9_acc[-1]))\n",
    "print('Average Top-11 accuracy {}'.format( running_top11_acc[-1]))\n",
    "print('Average Top-13 accuracy {}'.format( running_top13_acc[-1]))\n",
    "print('Average Top-15 accuracy {}'.format( running_top15_acc[-1]))\n",
    "\n",
    "print(\"Saving the predicted value in a csv file\")\n",
    "file_to_save = f'{save_directory}//best_epoch_eval_Test.csv'\n",
    "\n",
    "indx = test_data.index + 1\n",
    "df2 = pd.DataFrame()\n",
    "df2['index'] = org\n",
    "df2['link_status'] = link_status_data  # Add the link_status column\n",
    "df2['original_unit1_pwr'] = pwr_60ghz # Add the original_unit1_pwr_60ghz column\n",
    "\n",
    "df2['top1_pred'] = top1_pred_out\n",
    "df2['top3_pred'] = top3_pred_out\n",
    "df2['top5_pred'] = top5_pred_out\n",
    "df2['top7_pred'] = top7_pred_out\n",
    "df2['top9_pred'] = top9_pred_out\n",
    "df2['top11_pred'] = top11_pred_out\n",
    "df2['top13_pred'] = top13_pred_out\n",
    "df2['top15_pred'] = top15_pred_out\n",
    "df2.to_csv(file_to_save, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
